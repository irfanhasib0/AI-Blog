ul.lst-kix\_qptiuu6mlbhu-0{list-style-type:none}ul.lst-kix\_qptiuu6mlbhu-1{list-style-type:none}.lst-kix\_b2x7vhnewr2g-0>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-0}ul.lst-kix\_qptiuu6mlbhu-2{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-1.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-1 0}.lst-kix\_b2x7vhnewr2g-5>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-5}ul.lst-kix\_qptiuu6mlbhu-7{list-style-type:none}ul.lst-kix\_qptiuu6mlbhu-8{list-style-type:none}ul.lst-kix\_qptiuu6mlbhu-3{list-style-type:none}ul.lst-kix\_qptiuu6mlbhu-4{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-4.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-4 0}ul.lst-kix\_qptiuu6mlbhu-5{list-style-type:none}ul.lst-kix\_qptiuu6mlbhu-6{list-style-type:none}.lst-kix\_b2x7vhnewr2g-1>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-1}.lst-kix\_b2x7vhnewr2g-7>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-7,lower-latin) ". "}.lst-kix\_qptiuu6mlbhu-7>li:before{content:"- "}.lst-kix\_qptiuu6mlbhu-8>li:before{content:"- "}ol.lst-kix\_b2x7vhnewr2g-5.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-5 0}.lst-kix\_b2x7vhnewr2g-5>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-5,lower-roman) ". "}.lst-kix\_b2x7vhnewr2g-6>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-6,decimal) ". "}.lst-kix\_b2x7vhnewr2g-4>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-4,lower-latin) ". "}.lst-kix\_b2x7vhnewr2g-1>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-1,lower-latin) ". "}.lst-kix\_b2x7vhnewr2g-2>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-2,lower-roman) ". "}.lst-kix\_b2x7vhnewr2g-6>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-6}.lst-kix\_b2x7vhnewr2g-3>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-3,decimal) ". "}.lst-kix\_b2x7vhnewr2g-3>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-3}ol.lst-kix\_b2x7vhnewr2g-8.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-8 0}.lst-kix\_b2x7vhnewr2g-0>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-0,decimal) ". "}ol.lst-kix\_b2x7vhnewr2g-6.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-6 0}.lst-kix\_b2x7vhnewr2g-8>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-8}.lst-kix\_b2x7vhnewr2g-2>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-2}ol.lst-kix\_b2x7vhnewr2g-1{list-style-type:none}.lst-kix\_qptiuu6mlbhu-2>li:before{content:"- "}ol.lst-kix\_b2x7vhnewr2g-2{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-0{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-5{list-style-type:none}.lst-kix\_qptiuu6mlbhu-3>li:before{content:"- "}.lst-kix\_qptiuu6mlbhu-4>li:before{content:"- "}ol.lst-kix\_b2x7vhnewr2g-6{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-3{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-2.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-2 0}ol.lst-kix\_b2x7vhnewr2g-4{list-style-type:none}.lst-kix\_qptiuu6mlbhu-5>li:before{content:"- "}.lst-kix\_qptiuu6mlbhu-6>li:before{content:"- "}ol.lst-kix\_b2x7vhnewr2g-7{list-style-type:none}ol.lst-kix\_b2x7vhnewr2g-8{list-style-type:none}.lst-kix\_b2x7vhnewr2g-8>li:before{content:"" counter(lst-ctn-kix\_b2x7vhnewr2g-8,lower-roman) ". "}.lst-kix\_b2x7vhnewr2g-4>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-4}ol.lst-kix\_b2x7vhnewr2g-7.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-7 0}ol.lst-kix\_b2x7vhnewr2g-0.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-0 0}.lst-kix\_b2x7vhnewr2g-7>li{counter-increment:lst-ctn-kix\_b2x7vhnewr2g-7}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix\_qptiuu6mlbhu-1>li:before{content:"- "}.lst-kix\_qptiuu6mlbhu-0>li:before{content:"- "}ol.lst-kix\_b2x7vhnewr2g-3.start{counter-reset:lst-ctn-kix\_b2x7vhnewr2g-3 0}ol{margin:0;padding:0}table td,table th{padding:0}.c7{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman"}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c11{color:#67ab9f;text-decoration:none;vertical-align:baseline;font-style:normal}.c3{vertical-align:sub;font-size:12pt;font-family:"Times New Roman";font-weight:400}.c22{font-weight:700;vertical-align:baseline;font-size:15pt;font-family:"Times New Roman"}.c15{color:#666666;text-decoration:none;vertical-align:baseline;font-style:italic}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-style:italic}.c20{font-weight:700;font-size:14pt;font-family:"Times New Roman"}.c4{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c8{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c27{font-weight:700;font-size:16pt;font-family:"Times New Roman"}.c5{color:#000000;text-decoration:none;font-style:normal}.c10{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c25{color:#333333;text-decoration:none}.c9{color:inherit;text-decoration:inherit}.c17{background-color:#ffffff;color:#202124}.c29{padding:0;margin:0}.c16{margin-left:36pt;padding-left:0pt}.c19{color:#67ab9f}.c6{height:11pt}.c26{background-color:#f8f9fa}.c28{font-style:normal}.c18{color:#202124}.c12{vertical-align:super}.c21{font-style:italic}.c24{vertical-align:baseline}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}                        Object Detection : The RCNN Family



---

 

Object Detection is the combination of two sub tasks (i) Object Localization (ii) Object Classification. These two steps comprise the simplest intuitive way, firstly we localize the object in the image and then classify it to an object category. When it comes to object detection with deep learning, we need to consider an additional step ‘Feature Extraction’. It breaks down the raw data into a high dimensional feature space, which is more convenient for the computer to work with.

[A] Object localization : This step takes the rgb image and extracts the N set of (x,y,w,h) bounding box coordinates for potential objects present in it. It can be done using some specific image processing based algorithm e.g Selective search or a convolutional neural network e.g Region Proposal Network used in Faster RCNN algorithm.

[B] Feature map Extraction : It takes an image array and converts it to a high dimensional feature map array using a convolutional layer based neural network e.g vgg16, resnet 50 etc.

[C] Classification : This step takes each of the extracted feature maps and classifies it to an object category using dense layers or some machine learning  based classifier algorithm e.g SVM (support vector machine). Detailed explanation of step [A],[B] and [C] can be found on the right section.

RCNN



---

The RCNN method uses the following modules to perform object detection :

Selective Search  : It is an algorithm for generating region proposal suggestions. It suggests potential bboxes (bounding boxes) that may contain an object. 'Objectness' is another similar algorithm that can be used for region proposal generation. SOTA works now use a simple CNN based architecture (region proposal network) for this purpose. See the right section for more about selective search. 

Feature Extractor (FE) :  It can be any deep convolutional neural network. The output of the network is truncated from the last dense layer (if a 1d feature vector is necessary) or any of the intermediate convolutional layers (if a 2 dimensional feature map is required). The 2d featuremap also preserves the spatial relationship with the input image due to the sliding nature of convolution operation.  For RCNN they don't require the spatial relation, so use the output of a dense layer as a 1d feature vector. For fast RCNN it will need a 2d feature map instead.

Binary SVM : Support vector machine algorithm is used for binary (yes/no) classification every single class. Binary SVM is specially suited for single class classification since it tries to select the hyperplane that separates the two classes with highest margin.

![](https://lh3.googleusercontent.com/dPY4s3K5ngFRg21q5IokQxoVGwjGlcm_pJM-iPC3lt5nSCa7JinN55aM5qati6a2UGhG7HcS_80p1ov7ottwYFz54enok5a2mzWk9c6-wQ2tL1iC6FlixDLwJWihkTDbM_4b2klt-YSA1lrQ-B_3i7gOzQw-GMQZ8LMu8ADZKFIaVsxSpFw)

                                                                   Figure : RCNN (top) and Fast RCNN (bottom)

BBOX Regression : 

This approach was Inspired from the Deformable part model by Girshick at el. 2010. Let’s assume the following -

* The initial bbox predictions are P =  [Px,Py,Pw,Ph]
* The initial bbox ground truths are G = [Gx,Gy,Gw,Gh]
* dx(P) , dy(P), dw(P), dh(P) is the output of regression i.e the errors/offsets to be corrected on P for matching with G more accurately.

For x and y, a scale invariant transformation dx(P) dy(P) is learnt. It is an offset from an initial less accurate prediction by selective search, which needs to be added to the predictions. Since dx,dy are scale invariant offset, they are multiplied with the size (Pw , Ph) of the box before adding to the prediction box i.e  Gx = Pw \* Dx(P) + Px. For w and h a log scale transformation is learnt. It makes it capable of learning a wide range of change in w and h direction. Unlike x,y offsets, here it is just the difference of their logarithmic value instead of their actual value (eq [5] below).

 Most of the SOTA methods use an updated version of the bbox regression method introduced in RCNN. For example yolo-v3 uses  Gx = Sigmoid(P) +Px . They wrap the dx  with a sigmoid function to restrict its output between 0-1. Some methods use a fixed grid system with a potential bbox at every location of the featuremap as an initial bbox then corrects it with regression. We will explain about these in detail in respective sections.

Hard Negative Mining :  It is an approach that oversamples the difficult or hard to learn images more and makes the learning faster. It was possible to achieve good accuracy after just one pass over the data using hard negative mining for the SVM models.

![](https://lh3.googleusercontent.com/rfGPIqEbUxNBK4pOGyMW4btiAMtRxHBVHWgTbXJxPnt80sqUHmx_50m9Ysg8x8wqHr7zjuzHsRQgHsQVwhekzimoKQntcSOfVM9NAdBrwGkuAsNIiPZO4Z6MNCcJ_KtVZLQNXStET7tjJW2D2PvVfWnLQ06mlb4eHP3S8Dq0dLZe59rFuDQ)

RCNN Algorithm :

1. [A] Selective Search algorithm is used for generating ~ 2000 sets of bounding box coordinate (x,y,w,h) proposals per image. Using the bbox coordinates found from selective search, image RoIs (Region of Interest) of different sizes are cropped.

2. [A] The SVM classifier can only take fixed sized inputs. So the outputs of the feature extractor should have constant size. It is possible if the input image RoIs to the feature extractor have fixed size. For this reason each RoI is 'wrapped' to a fixed size [resizing with bilinear/nearest sampling] before inputting them to the feature extractor model. 

3. [B] A deep convolutional neural network is used for extracting feature maps from each of the image RoI one by one. The model output is truncated from the bottom and output from a dense layer with 4096 nodes is extracted as a feature map.

         -> Pre training with ICSLV-2012 images with 200 classes lr 0.01 on CNN architecture by krizhevsky at el. 2012.

         -> Oversampling positive classes 32 positive sample + 96 neg sample per mini-batch

         -> Fine tuning with lr 0.001 with wrapped region proposal with N + 1 class. N is no of object class 1 is background

         -> During fine tuning a region proposal with more than 0.50 IoU with a positive sample was taken as positive.

         -> During Feature Extraction the last layer is removed and features from an internal dense layer are collected. i.e 1024 or 2048 features

4. [C]  Separate binary classifiers for each of the N (N=no of categories) are trained to categorize the fmaps to respective object categories.

         -> One binary SVM classifier is used for each of the classes.

         -> Hard negative mining [4] is found to be useful for fast convergence. For detail see right section.

5. [A\*] Fine tuning of the localization boxes are done by bbox regression using Ridge regression with lambda = 1000. The regressor only learns the x,y offset from the corner of the bbox predicted by selective search. For w,h it is log offset instead. We have already explained it above.

As per our today's practice on SOTA object detection algorithms several things might seem to be redundant -

[i] Extracting feature maps for every RoI one by one. [It is solved in Fast RCNN eventually]

[ii] Using a separate SVM for each class instead of using the softmax class outputs of the feature extractor network. [Improved in Fast RCNN]

[iii] The word "wrapped" image is used in the paper for resizing each Fmap RoI to a fixed size. Nowadays a good practice is using bilinear sampling. Note that - rcnn first resizes (wrapped) each of the image RoI then inputs them to Feature extractor but Fast RCNN will resize the feature maps using RoI pooling layer.

 Using additional SVM classifier instead of the dense layer output from the feature extractor network :

Quote from the paper's appendix B : "We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classi- fier was trained on randomly sampled negative examples rather than on the subset of “hard negatives” used for SVM training."

Anyway, in Fast RCNN the authors were finally able to do it using a small dense network after the feature extractor, without any SVM classifier.

Fast RCNN



---

Improvements in Fast RCNN:

- Fast RCNN eliminated one by one RoI input into the feature extractor. There are lots of common regions between RoI's specially when nearly 2000 RoI generated from one image. This method applies a feature extractor for the whole image just once and then it crops the corresponding RoI for each of the ~ 2000 corresponding regions from the feature map. 

- Replacing image wrapping with RoI Pooling.

- Getting rid of per class svm by using the softmax outputs from dense layers after feature extractor.

 Fast RCNN Algorithm :

1.[A] Selective Search generates ~ 2k RoI proposals per image. 

2.[B] A CNN based network i.e VGG16 is used for feature extraction.

3.[B] The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps RoIs so that it can fit into the dense layer afterwards. (H=7, W=7 for VGG 16) 

4. [C] The last fully connected layer is replaced with 2 sibling dense layers (i) one  for classification (ii) another for bbox regression. For regression it adopted the same fine tuning mechanism (optimizing tx,tx,tx,tx) like RCNN. 

RoI Pooling Layer

RoI Pooling Layer : The  operator [\*] represents the flooring operation here. e.g [1.13] = 1.00

The functionality of RoI Pooling layer can be divided into two parts -

[1] Scaling the image RoIs to fmap dimension : Due to multiple polling operations in the consecutive convolutional layers, the feature map array has smaller (e.g 16~32 times smaller) width and height compared to the original image. In the example below the feat map is 16 times smaller than the original image in terms of width and height. The RoI Pooling layer scales down each of the RoI coordinates (generated at the image scale) so that it fits on top of the feature map in the corresponding feature location for that image RoI. Each RoI coordinate needs to be divided by the scale factor (here it is 16) for fitting on the feature map. In this way the 188x188 RoI window coordinates are converted to 11x11 below. The center coordinates (x,y) of the RoI is also divided by the factor like (w,h) which is not shown in the figure for brevity.

[2] Resizing the Fmap RoI to fit the fixed input of dense layers : The dense layer has fixed input size i.e 25 dense nodes. If a 5x5x1 feature map tensor is flattened (5x5x1=25) only then it will fit the dense input. For 5x5x10 the dense should have 250 input nodes. For each of the RoI Feature maps do a maxpool operation with kernel size ([h/h''], [w/w'']) for below example it will be ([11/5],[11/5]) = (2,2). Now an arbitrary feature map of size 11x11 is operated with a maxpool of kernel size 2 for fitting fixed 5x5 input size of the dense layers afterward. Another example can be like - an orbitary Fmap RoI of size 20x20 would require a kernel of 4x4 for fitting it into 5x5 input size

![](https://lh6.googleusercontent.com/mTOPEyO4BJAqqOvjdJBAHkSWPQaCDN-LXne9ax6H1ZCYbVHMf8RK2iXj8Y0qk5_1mt2xL9FdLCZIxzsx4bmUvpyw-XY7kXT0_YJSTMU2L2xMRiWrP9MfJxumRO0JWk15xd75pvJR8NefvQBqE_YXXUf9cynoW_iikJzlrjCUb1qxnQCLYto)

 Figure : RoI Pooling Layer top 2d view with numerical examples bottom 3d view of the same thing

 Faster RCNN



---

Improvements :

- The selective search is replaced with a Region Proposal Network that mostly shares weights with the Feature Extractor Network.

- The region proposals are further broken down to 3 different scales with 3 different aspect ratios called anchor boxes.

The Algorithm:

[A] RPN (Region Proposal Network): 

 - Region Proposal Network proposes possible box coordinates and objectness confidence at each pixel location of the feature map. This is a class agnostic box predictor so it has nothing to do with the category of the object. The output of the RPN module is converted to bbox coordinates at every feature map location and trained with the bbox regressor. 

  - The output of the last shared convolutional layer is passed through a nxn convolution layer (e,g 3x3) then forwarded to - 2 sibling 1x1 convolution layers for bbox and confidence prediction at every fmap location. The 1x1 convolution layer can also be interpreted as a dense layer applied at every pixel locations and results in an array with same height and width while having depth of 4k and 2k respectively, k= number of anchor box per location.  

 - At each pixel location we can get k=9 boxes and respective confidence scores. It will make it possible to detect two objects centered at the same image location (having different aspect ratio or scale). If the output of the RPN has width m and height n the depth would be 2k and 4k for confidence and bbox outputs. So a total of m\*n\*k set of (x,y,w,h) can be extracted from the depthwise 4k data points at each of the m\*n locations. Similarly m\*n\*k set of (object conf,background conf) can be found from 2k data points depthwise. For more on it see 'Anchor box' section below.

 - The RPN is trained with binary classification labels. Among the potential k boxes at every feat. map location, boxes having IoU more than 0.7 (if multiple is found then the highest one is taken only) it is taken as a positive sample. Boxes with less than 0.3 IoU with all the GT (Ground Truth) boxes are taken as negative samples (background), others are ignored. 

[B] Feature Extractor :

 - It used the CNN (Convolutional Neural Network) network feature extractor which shares its weights with the RPN (until the RPN specific layers). It used the VGG16 and also ZF [zeiler at el.] pre-trained on ImageNet dataset for demonstrating their algorithm performance.

 - The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps for every Fmap RoI so that it can fit into the first dense layer (H=7, W=7 for VGG 16) 

 - RoI pooling layer projects the RoI coordinates on top of the feature map array in a max pooling manner, the difference here is it adjusts the pooling window size for keeping the output size fixed for all the fmap. RoIs.

[C] Classifier : Each of the output (fmap RoIs) from the RoI Pooling layer is sent to a small dense network ending with 2 sibling dense layers (i) one for classification (ii) another for bbox regression. The regressor here 

learns the x,y offset and log offset for w and h from the prior boxes predicted by RPN initially.

![](https://lh6.googleusercontent.com/mRUoGmS2zzG0frHjCPTmMNmU6Vy_U1VsJKp3IYmeT8GF7ueMfrjoPTi1kbzXyeYFAm239o1r8luRVjw92cyRURq4Lllg-VZ5he-9JIHZt-yuIThETdcec3SA1fp8B8u1NwkJOsdTXjvB1QaxcC1F-SC4-uOjGZtn7qoN-bT523mjXys1ses)

                                                                  Figure : Faster RCNN

Anchor Box / Multi Box / Default Box

Gw = Pw exp(width offset from RPN output) --- [1]       

- bbox width from RPN output = log(Gw/Pw) = log(Gw) -log( Pw)

Gh  = Ph exp(height offset from RPN output)  ---- [2]

- bbox width from RPN output = log(Gw/Pw) = log(Gw) -log( Pw)

Anchor boxes are prior box estimates at every feature map location. They are determined before the training and fed into the network beforehand.  The prior boxes (Pw , Ph) at every image location are further fine tuned by multiplying with the exponential value of the corresponding RPN outputs (see eq [1] and [2] above). For width and height a log offset is learnt between the prior box and GT box like Fast-RCNN. It is already explained in the bbox regression section above.  In short, the model basically learns how to fit each of the anchor boxes to the object present at that location by multiplying it with a factor i.e exp(RPN output). At the same time it learns two confidence scores for object and background for each anchor box. That's why the 2k scores are associated with each anchor box.

Anchor boxes were first used by erhan at el. , szegedy at el in their respective works. It is referred to as the multibox approach or default box in other works.  Initial anchor box methods used different anchors (prior boxes) at different locations of the image resulting in a lot of unique anchors. For example, the multibox method by szegedy at el. was not translation invariant. It used 800 anchor boxes at different locations of the image. They found this 800 anchors by k means clustering of the [x,y,w,h] of all the Ground truth bboxes. In contrast, faster rcnn does k means clustering for h,w only irrespective of their x,y location in image. These 9 boxes can just fit any arbitrary x,y location in the image instead of 800 different boxes for various x,y locations. They conducted some experiments with different values of k and found k=9 anchor boxes are optimal for achieving the best mAP.  This 9 boxes are now capable of producing w\*h\*9 ~ 2400 x 9 RoI per image at maximum.

 -> For Multibox the shape of tensor for RoI prediction was 4+1\* 800 dimensional  fully connected layer

 ->  For Faster RCNN it reduced to 4+2\*9 convolutional output layer.

![](https://lh4.googleusercontent.com/SW3mYwD1RiZRfuh25EOAGj4KQrA7tXMCjFkuzkMVQpDldwEtsQiLpbk8KnnxFh2wRsxNAjgELeLk-YkIEbxYC6Gc44s7jKR57RqlB1IVG4fU3wwnh_4apu4r6_DLxjmVjN3NKXUDzLe5d1Jl91lob-wTewmtLd_1U93ljftitOjjmijEe60)

                             Image from Faster RCNN paper Ren et al 2016

![](https://lh3.googleusercontent.com/LfgyaAPXcobtEdFgQg1bT8Gt8Bif9l_2peR2NBddIcjt7PvCyxAQCCO_pWQ450V7t5JeNvle9fgWibwp_w9UVetaAPTHJz0CPEI2mcv0FMM0Rp0djhrVHgH3a4OQ1ccjX86y6AxNNzftEK38CgUsU5Gfd36L7I5QjP_rGD-4Vbj9p9WXW_c)

                       Figure :  Total 9 anchor boxes at one pixel locations

RPN Loss :

x, xa, x\* are prediction , anchor box, ground truth.

ti  : x,y,w,h of  ith bbox , pi and pi\* are prediction and GT confidence.

pobj,pbg in for object and background confidence.

Lreg(tx , tx\* )  = (x-x\*)/w             ;          Lreg(ty , ty\* ) = (y - y\*)/ha ;

Lreg(tw , tw\* ) = log(w / w\*)/wa   ;          Lreg(th , th\* )  = log(h / h\*)/ha

 

Li =  Lcls + Lbbox

Lcls  = 1/Ncls x ΣNi  [ΣMj -pobji\* log(pobji) - pbgi\*log(pbgi) ]

Lreg = λ x 1/Nreg  x ΣNi pi\* x Lreg(ti, ti\*  ) + Robust L1 

here Ncls = 256 ; Nreg = 2400 ;  Lambda = 10 

Fast-RCNN\*   Loss :

Here the regression loss is mostly the same except for the fact that it represents the RPN predicted coordinates here (instead of the GT). So here the loss function learns the offset from RPN predicted coordinates.

Loss = Lcls + Lbbox

Lbbox = 1/N x ΣNi Pi\* x (t i\*- ti ) ^2 ; ti  : x,y,w,h of  ith bbox + Robust L1

Lcls = 1/N x ΣNi [ΣMj -pji\*log(pji) ]

i : ith data sample in mini batch

j : jth object category

Back propagation :

(1) Alternate training : (Used by the authors)

RPN and Fast-RCNN share the same network (the "Shared CNN Layers" part in above figure). Rest of the layers are task specific i.e RPN Layers and Dense Layers at the end. Both of the networks are initialized with imagenet pre-trained weights for either VGG-16 (first 13 CNN layers) or ZF network (first 5 CNN layers)  [zeiler at el.]. \*Fast-RCNN is a special case of Fast-RCNN,  that will be trained with region proposals from RPN predictions instead of selective search. 

- First the RPN is trained. 

- Then using the region proposal results of RPN is used as input for training the \*Fast-RCNN.

- Next RPN is initialized with the Fast-RCNN weights (only the shared part) and trained again for the RPN specific layers at the bottom are trained only keeping the shared layers fixed.

- Finally, both the \*Fast-RCNN is trained again keeping the common layers fixed.

(2) Approximate Joint training : Using the same network for RPN and Fast-RCNN\* from the beginning. Training them together. For Fast-RCNN the outputs from RPN during forward pass are taken as input assuming them fixed predefined values. Since the RoI pooling layer is not differentiable that's why they needed to consider the outputs of RPN fixed values.

(3) Non Approximate Joint training : In this method they proposed using a differentiable RoI Pooling layer where so that the outputs taken from RoI pooling and fed back to the Fast-RCNN would be a part of the back propagation chain

Authors showed their results with method (1) in the paper.

Mask RCNN



---

Improvements in Mask RCNN :

- RoI Pooling layer is replaced with RoI align layers.

- Additional semantic segmentation branch is added for predicting both bbox and segmentation at a time. I will refer it as [D]

The Algorithm Mask RCNN:

The algorithm is very similar to faster rcnn. The major difference here is that they used a more accurate version of the RoI Pooling layer. They also added a segmentation branch to the original network and was able to produce accurate semantic segmentation along with bounding box prediction.

 

Mask RCNN Loss

Loss = Lcls + Lbbox + Lseg

Lcls and Lreg are similar to Faster RCNN.

Lseg = 1/N x ΣNi [Σjk -yjki\*log(yjki) ] , j and k are pixel locations in horizontal and vertical axis.

![](https://lh6.googleusercontent.com/4Gv_S83WercZPykwewXtBRR-y30anlHUrVIDQuMdTt22jhCn49qeM1PgpB7wD5ZtXsTPJlGywrihUZ_jjEv02HTLKXkPyd0Ml_xBKMuQ2SYpjduH845aAjPc9j9q9T5kwS_WS9QtCP4IRFBMvQvJXk72qoOVSUdFiRuEzSABpPdQ2KiGSK4)

                                               Figure : Mask RCNN Algorithm

![](https://lh4.googleusercontent.com/h9J8XHZZ0dO67vCS8KQzQX7qfbrqt0yNx5S0MCh8Ape6kR7SmlrRDfnOW5z37dK7oDTgQ1Alg312Prh-NYSarxbZKzCd8ulGdgid_9h02WW8ph1vfAasWzkPShqSez6_t1PO7FZ9h1cIGTsrQKWS_rZXikEYOVUT9oGDoIaVcAKkgOVRi7Q)

                                                      Figure : RoI Align Layer

RoI Align layer does the same job of RoI pooling but little accurately using a different approach.

[1] RoI Scaling : It does not round up the floating point number while scaling down the RoI sizes to match the feature map dimension/

[2] Resizing : It uses bilinear interpolation for converting the floating window size to a fixed window input size i.e 5x5 here.

Summary



---

RCNN 

 [A] Selective Search  

 [B] 1 by 1 FE 

 [C] 1 by 1 classification by separate binary SVM for each class

Fast RCNN 

 [A] Selective Search

 [B] Single pass Feature Extractor + 1x1 RoI Pooling

 [C] 1x1 fmap through dense layer and softmax 

Faster RCNN

 [A] RPN + Anchor box

 [B] Single pass Feature Extractor + 1x1 RoI Pooling

 [C] 1x1 fmap through dense layer and softmax 

Mask RCNN 

 [A] RPN + Anchor Box

 [B] Single pass Feature Extractor + 1x1 RoI Align

 [C] 1x1 fmap through dense layer and softmax 

 [D] Segmentation Branch

Symbols and Short forms



---

![](https://lh5.googleusercontent.com/FT_zQg5IJptXBMpbrjEcccj3bJTI_ilZtMenVcVOOYvpUxp-8TTPXR6XZGpJeZjYM7s18T7x-ZUiDvx_tD4Z4M4wBNXAhadVzWftFHXSfAWgg-9uQWQF-VFfu1H_sM38DkBh-y-YMYMeqeYfih3KKtxRu66ZshPpwmyxyFGl_FDJ2w0Yyko)

Below are some short forms that will be used in the later sections of this chapter : 

RoI    : Region of Interest. Small cropped section of an image.

Fmap /feat map : Feature Map.  Truncated 3-dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.

FE     : Feature Extractor. It will represent a deep learning model that extracts features from input data. The term ‘FE’ will be used synonymously with any deep neural network architecture. e.g VGG16 which exactly does the same.

FC     : Fully Connected Dense Layer/Layers.

bbox  : Bounding box. [i.e box coordinates x,y,w,h]

GT    : Ground Truth

CNN : Convolutional Neural Network. ; Conv. : Convolutional.

For explainability of the figures I have used consistent symbols while drawing part of the model.  The symbols and references are defined at the end.

Appendix :



---

Selective search  

Quote from Uijlings et al 2013 - "Our grouping procedure now works as follows. We first

use (Felzenszwalb and Huttenlocher 2004) to create initial regions. Then we use a greedy algorithm to iteratively group regions together: First the similarities between all neighbour- ing regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours. The process ofgroup- ing the most similar regions is repeated until the whole image becomes a single region"

![](https://lh6.googleusercontent.com/XIke7D_HDqUpYn5kTMC-wUSJ9aWgNaDfsBTYORatEy-zpAu2Gsv0_COzFmhdT5koUXvUPzvjtBhDXXIw8QZa_SdOasKtxgwlt539ntZJR-d4gQYml9CUX5KWhR7vqYAqMrJyS_TXgPQ4BtZX4m-rDLTa-51N0tuKMUuZqHmSEQuVjBT-Iks)

                   Image from the selective search paper by  Uijlings et al 2013

Relevance between the above explanation and the authors figure in the respective papers. The steps [A],[B], [C] and [D] are shown below for better correlating with the above content, while reading from the paper.

![](https://lh3.googleusercontent.com/IWTkiS-q4dvFnnIOeHQUNSs5YLRKewDVSqWPsKFgX7dKkU_zcZgriRjI9SoGN66wy8saQL26zkPyZDB9LUto27yWLiThchuBqQVGl3VwcuFta2g5r0ToMA93dhbX3tA4t_Xur5rDq-kifnJNviAkBZXLS6fxdLKfHz3W12l49vSeF1jwghA)

(i) Top : Image from RCNN paper by Girshick et al. 2014. (ii) Bottom : Image from Fast RCNN paper by Girshick et al. 2015

![](https://lh3.googleusercontent.com/HjhoSHPom7z883lAN3RIM-ae0FtLUgN2s_iU-k-mpEP8T4gmGTd2_5SwOY5mRaDkXVdQNGtsOqhf13A7G2IcVaXQMmsj_N862FClTNv4k24xvc5qiGjFT72Ljdq3TWTIgkj0b8JTUEjXHj_xRAQ8F9wrHgjxXth5A8-IttWsmlJlIDOy5vU)![](https://lh6.googleusercontent.com/Wn7x2QWy934rbWsH6rpYb3Xc_ukZ-d_nflGc9nb2RkO9X73gHH1c6rqVONV3aee-8nl2Yw_jUA8xDm96fP101GWjmgwg4vzED3Ww9FxHSweVtaR_o08vIajdA_tJd58HmvIgXHIIO-iGcnN8QnVdQWacZwSzdUWSjDLpIJedzLJ_vyT6Xp0)

 Left : Image from Faster RCNN paper Ren et al 2016 , Right : Image from Mask RCNN from He et al 

References

Models :

1. ImageNet Classification with Deep Convolutional

Neural Networks : [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://www.google.com/url?q=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&sa=D&source=editors&ust=1666672297813222&usg=AOvVaw0Mor71gQNJQjL60-DntyKi)   

2. Very Deep Convolutional Networks For Large-Scale Image Recognition: [https://arxiv.org/pdf/1409.1556.pdf](https://www.google.com/url?q=https://arxiv.org/pdf/1409.1556.pdf&sa=D&source=editors&ust=1666672297813820&usg=AOvVaw03MLKw9Z86783jUk75yHTI)

3. Visualizing and understanding convolutional neural networks :[https://arxiv.org/pdf/1311.2901.pdf](https://www.google.com/url?q=https://arxiv.org/pdf/1311.2901.pdf&sa=D&source=editors&ust=1666672297814271&usg=AOvVaw1gAGw02WTP1MAdvnfqHPny)

Region Proposal Generation 

4. Selective search : [https://link.springer.com/article/10.1007/s11263-013-0620-5](https://www.google.com/url?q=https://link.springer.com/article/10.1007/s11263-013-0620-5&sa=D&source=editors&ust=1666672297814821&usg=AOvVaw30WUsYMeVejsr94Y14Bi22)

5. Objectness         : [https://arxiv.org/abs/2004.02945](https://www.google.com/url?q=https://arxiv.org/abs/2004.02945&sa=D&source=editors&ust=1666672297815139&usg=AOvVaw3RhSpoT_Y7c46Iez6CSeN_)

Object Detection

6. SPP-Net: Deep Absolute Pose Regression with Synthetic Views : [https://arxiv.org/abs/1406.4729](https://www.google.com/url?q=https://arxiv.org/abs/1406.4729&sa=D&source=editors&ust=1666672297815588&usg=AOvVaw0TxVoYPIsCpRuN2CN5we3Q)

7. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks : [https://arxiv.org/abs/1312.6229](https://www.google.com/url?q=https://arxiv.org/abs/1312.6229&sa=D&source=editors&ust=1666672297815897&usg=AOvVaw1vDPepnFfOOTmsWMNBmWkI)

8. Deformable Part Models are Convolutional Neural Networks : [https://arxiv.org/pdf/1409.5403](https://www.google.com/url?q=https://arxiv.org/pdf/1409.5403&sa=D&source=editors&ust=1666672297816303&usg=AOvVaw1_81AINGDAdwcerFq_ATWU)

9. R-CNN: [https://arxiv.org/abs/1311.2524](https://www.google.com/url?q=https://arxiv.org/abs/1311.2524&sa=D&source=editors&ust=1666672297816815&usg=AOvVaw1L2HBvdvZhynRItjTXlgdL)

10. Fast R-CNN: [https://arxiv.org/abs/1504.08083](https://www.google.com/url?q=https://arxiv.org/abs/1504.08083&sa=D&source=editors&ust=1666672297817274&usg=AOvVaw08rq_mQBOoqIYslS0w9mRE)

11 Faster R-CNN: [https://arxiv.org/abs/1506.01497](https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&sa=D&source=editors&ust=1666672297817635&usg=AOvVaw2WwCt-3CXRs1jOXipgMeX8)

12. Mask R-CNN: [https://arxiv.org/abs/1703.06870](https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&sa=D&source=editors&ust=1666672297817912&usg=AOvVaw2H5C985qosHCOEhjGCTcnM)

Multibox Approaches 

13. Scalable object detection using deep neural networks : [https://arxiv.org/abs/1312.2249](https://www.google.com/url?q=https://arxiv.org/abs/1312.2249&sa=D&source=editors&ust=1666672297818290&usg=AOvVaw3zi6ulIWx1TD8hthWZujBv)

14. Scalable, high-quality object detection  : [https://arxiv.org/abs/1412.1441](https://www.google.com/url?q=https://arxiv.org/abs/1412.1441&sa=D&source=editors&ust=1666672297818546&usg=AOvVaw3Cm1-myeNhVXtK_KrSlSR6)

Blogs 

Special thanks to for easy such a nice explanation of important things -

1. [https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/](https://www.google.com/url?q=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/&sa=D&source=editors&ust=1666672297819010&usg=AOvVaw2VWvKHU8BMOOc_jqFzZgmi)

2. [https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4](https://www.google.com/url?q=https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4&sa=D&source=editors&ust=1666672297819397&usg=AOvVaw2sY4sfeMqGvb07YRngj8Iw)

3. [https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272](https://www.google.com/url?q=https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272&sa=D&source=editors&ust=1666672297819763&usg=AOvVaw37TJMiajIvzIpn8ggmWeo6)

