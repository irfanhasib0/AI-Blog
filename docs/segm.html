<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_vglxt0gyulny-1.start{counter-reset:lst-ctn-kix_vglxt0gyulny-1 0}.lst-kix_ja5vb6dsrozx-8>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-8}ol.lst-kix_vglxt0gyulny-4.start{counter-reset:lst-ctn-kix_vglxt0gyulny-4 0}.lst-kix_vglxt0gyulny-2>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-2,lower-roman) ". "}.lst-kix_vglxt0gyulny-3>li{counter-increment:lst-ctn-kix_vglxt0gyulny-3}ol.lst-kix_ja5vb6dsrozx-6.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-6 0}.lst-kix_vglxt0gyulny-1>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-1,lower-latin) ". "}.lst-kix_vglxt0gyulny-0>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-0,decimal) ". "}.lst-kix_uoz4d3ortl5p-3>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-3}.lst-kix_vglxt0gyulny-5>li{counter-increment:lst-ctn-kix_vglxt0gyulny-5}ol.lst-kix_uoz4d3ortl5p-1.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-1 0}.lst-kix_vglxt0gyulny-2>li{counter-increment:lst-ctn-kix_vglxt0gyulny-2}ol.lst-kix_vglxt0gyulny-7.start{counter-reset:lst-ctn-kix_vglxt0gyulny-7 0}ol.lst-kix_ja5vb6dsrozx-3.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-3 0}.lst-kix_uoz4d3ortl5p-5>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-5}.lst-kix_uoz4d3ortl5p-2>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-2}ol.lst-kix_ja5vb6dsrozx-0.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-0 0}.lst-kix_ja5vb6dsrozx-2>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-2,lower-roman) ". "}ol.lst-kix_ja5vb6dsrozx-4.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-4 0}.lst-kix_ja5vb6dsrozx-0>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-0}.lst-kix_ja5vb6dsrozx-1>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-1,lower-latin) ". "}.lst-kix_ja5vb6dsrozx-0>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-0,decimal) ". "}.lst-kix_ja5vb6dsrozx-5>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-5}.lst-kix_ja5vb6dsrozx-7>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-7,lower-latin) ". "}ol.lst-kix_uoz4d3ortl5p-1{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-4.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-4 0}ol.lst-kix_uoz4d3ortl5p-0{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-3{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-2{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-5{list-style-type:none}.lst-kix_ja5vb6dsrozx-6>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-6,decimal) ". "}ol.lst-kix_uoz4d3ortl5p-4{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-7{list-style-type:none}.lst-kix_ja5vb6dsrozx-3>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-3,decimal) ". "}ol.lst-kix_uoz4d3ortl5p-6{list-style-type:none}ol.lst-kix_vglxt0gyulny-2.start{counter-reset:lst-ctn-kix_vglxt0gyulny-2 0}ol.lst-kix_uoz4d3ortl5p-8{list-style-type:none}.lst-kix_ja5vb6dsrozx-6>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-6}.lst-kix_ja5vb6dsrozx-5>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-5,lower-roman) ". "}.lst-kix_ja5vb6dsrozx-4>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-4,lower-latin) ". "}.lst-kix_uoz4d3ortl5p-1>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-1}.lst-kix_uoz4d3ortl5p-4>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-4}.lst-kix_uoz4d3ortl5p-7>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-7}.lst-kix_vglxt0gyulny-7>li{counter-increment:lst-ctn-kix_vglxt0gyulny-7}ol.lst-kix_vglxt0gyulny-3.start{counter-reset:lst-ctn-kix_vglxt0gyulny-3 0}.lst-kix_vglxt0gyulny-4>li{counter-increment:lst-ctn-kix_vglxt0gyulny-4}ol.lst-kix_uoz4d3ortl5p-3.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-3 0}ol.lst-kix_ja5vb6dsrozx-5.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-5 0}.lst-kix_vglxt0gyulny-1>li{counter-increment:lst-ctn-kix_vglxt0gyulny-1}.lst-kix_uoz4d3ortl5p-0>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-0,decimal) ". "}ol.lst-kix_uoz4d3ortl5p-2.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-2 0}ol.lst-kix_ja5vb6dsrozx-2.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-2 0}.lst-kix_uoz4d3ortl5p-1>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-1,lower-latin) ". "}.lst-kix_ja5vb6dsrozx-3>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-3}ol.lst-kix_vglxt0gyulny-8.start{counter-reset:lst-ctn-kix_vglxt0gyulny-8 0}.lst-kix_uoz4d3ortl5p-7>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-7,lower-latin) ". "}.lst-kix_ja5vb6dsrozx-2>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-2}.lst-kix_uoz4d3ortl5p-6>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-6,decimal) ". "}.lst-kix_uoz4d3ortl5p-5>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-5,lower-roman) ". "}.lst-kix_uoz4d3ortl5p-2>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-2,lower-roman) ". "}.lst-kix_uoz4d3ortl5p-4>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-4,lower-latin) ". "}.lst-kix_uoz4d3ortl5p-3>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-3,decimal) ". "}ol.lst-kix_vglxt0gyulny-4{list-style-type:none}ol.lst-kix_vglxt0gyulny-5{list-style-type:none}ol.lst-kix_vglxt0gyulny-2{list-style-type:none}.lst-kix_vglxt0gyulny-8>li{counter-increment:lst-ctn-kix_vglxt0gyulny-8}ol.lst-kix_vglxt0gyulny-3{list-style-type:none}ol.lst-kix_vglxt0gyulny-8{list-style-type:none}ol.lst-kix_vglxt0gyulny-6{list-style-type:none}ol.lst-kix_vglxt0gyulny-5.start{counter-reset:lst-ctn-kix_vglxt0gyulny-5 0}ol.lst-kix_vglxt0gyulny-7{list-style-type:none}ol.lst-kix_vglxt0gyulny-0{list-style-type:none}ol.lst-kix_vglxt0gyulny-1{list-style-type:none}.lst-kix_ja5vb6dsrozx-8>li:before{content:"" counter(lst-ctn-kix_ja5vb6dsrozx-8,lower-roman) ". "}.lst-kix_uoz4d3ortl5p-8>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-8}.lst-kix_uoz4d3ortl5p-8>li:before{content:"" counter(lst-ctn-kix_uoz4d3ortl5p-8,lower-roman) ". "}ol.lst-kix_uoz4d3ortl5p-5.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-5 0}ol.lst-kix_uoz4d3ortl5p-8.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-8 0}ol.lst-kix_ja5vb6dsrozx-8{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-7{list-style-type:none}.lst-kix_uoz4d3ortl5p-0>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-0}ol.lst-kix_ja5vb6dsrozx-6{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-5{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-4{list-style-type:none}.lst-kix_uoz4d3ortl5p-6>li{counter-increment:lst-ctn-kix_uoz4d3ortl5p-6}ol.lst-kix_ja5vb6dsrozx-3{list-style-type:none}ol.lst-kix_uoz4d3ortl5p-7.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-7 0}ol.lst-kix_ja5vb6dsrozx-2{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-1{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-7.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-7 0}.lst-kix_vglxt0gyulny-0>li{counter-increment:lst-ctn-kix_vglxt0gyulny-0}ol.lst-kix_ja5vb6dsrozx-0{list-style-type:none}ol.lst-kix_ja5vb6dsrozx-1.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-1 0}.lst-kix_vglxt0gyulny-6>li{counter-increment:lst-ctn-kix_vglxt0gyulny-6}.lst-kix_ja5vb6dsrozx-4>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-4}ol.lst-kix_vglxt0gyulny-0.start{counter-reset:lst-ctn-kix_vglxt0gyulny-0 0}ol.lst-kix_ja5vb6dsrozx-8.start{counter-reset:lst-ctn-kix_ja5vb6dsrozx-8 0}.lst-kix_ja5vb6dsrozx-7>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-7}ol.lst-kix_uoz4d3ortl5p-6.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-6 0}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_vglxt0gyulny-3>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-3,decimal) ". "}.lst-kix_vglxt0gyulny-4>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-4,lower-latin) ". "}.lst-kix_vglxt0gyulny-6>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-6,decimal) ". "}.lst-kix_vglxt0gyulny-5>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-5,lower-roman) ". "}.lst-kix_vglxt0gyulny-8>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-8,lower-roman) ". "}ol.lst-kix_uoz4d3ortl5p-0.start{counter-reset:lst-ctn-kix_uoz4d3ortl5p-0 0}.lst-kix_ja5vb6dsrozx-1>li{counter-increment:lst-ctn-kix_ja5vb6dsrozx-1}ol.lst-kix_vglxt0gyulny-6.start{counter-reset:lst-ctn-kix_vglxt0gyulny-6 0}.lst-kix_vglxt0gyulny-7>li:before{content:"" counter(lst-ctn-kix_vglxt0gyulny-7,lower-latin) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c3{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:italic}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial"}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c4{text-decoration-skip-ink:none;font-size:14pt;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c17{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c8{color:inherit;text-decoration:inherit}.c13{padding:0;margin:0}.c14{font-weight:700}.c9{font-size:14pt}.c11{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c17 doc-content"><p class="c5"><span class="c12 c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image Segmentation (Under Progress)</span></p><hr><p class="c0"><span class="c11 c12"></span></p><p class="c5"><span class="c1">Image segmentation classifies each of the image pixels to a specific class. Each class can represent one instance of a particular category or all the instances of a category can represent one single class. Based on this criterion segmentation is divided into three types.</span></p><p class="c5"><span class="c1">[i]. Semantic Segmentation: Semantic segmentation labels one class of objects all together with one label. It is often mentioned as a &ldquo;staff&rdquo; class. All the sheep are considered as one class in the left image below.</span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 394.00px; height: 123.00px;"><img alt="" src="files/segm/image3.png" style="width: 394.00px; height: 123.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from COCO Dataset paper by Lin at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">[ii]. Instance Segmentation: It differentiates between each of the objects even if they are of the same category. Like each of the sheep is separated by colors and considered as different items in the right image which is an example of instance segmentation.</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">[iii]. Pantopic Segmentation: It is a combination of semantic and instance segmentation. So it can result in both kinds of segmentation outputs.<br></span></p><p class="c5"><span class="c14">Table of contents :</span></p><ol class="c13 lst-kix_ja5vb6dsrozx-0 start" start="1"><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.9u0bwbtvrtu0">Spatial Pyramid Pooling Module</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.jko2kubkm30a">Fully Convolutional Network</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.a1wxvkgii8h8">UNet</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.x5u01mdm4ok">PSPNet</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.rb4xs7vp2epa">Feature Pyramid Network</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.3unk15mw2mil">DeepLab V1 </a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.kzmo884wbiqc">DeepLab V2 </a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.3uzojslo4aec">UPSNet</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.x605mzn41se0">Pantopic FPN</a></span></li><li class="c3 li-bullet-0"><span class="c4"><a class="c8" href="#id.4vgavbjt74rk">DeepLab V3</a></span></li></ol><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c2"></span></p><a id="id.9u0bwbtvrtu0"></a><p class="c5"><span class="c2">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c5"><span>Although SPP is used with many computer vision tasks it has a significant impact in SOTA segmentation models.</span><span class="c1">&nbsp;CNN based (classifier) networks are often designed with some CNN blocks and a few dense layers at the bottom. The output dense layer should have a fixed size for predicting the probabilities of predefined classes. Now if input image size changes, the output from the last conv layer also changes. Consequently, flattening will produce a variable number of input nodes for the fixed sized dense layer. To resolve this, input images are often cropped or warped. Cropping is prone to losing a part of the object, warping causes unwanted distortion. Obviously, none of these is the best approach. This work introduces an SPP layer at the bottom that can take the varying output from the last conv layer while maintaining a fixed sized output vector.</span></p><p class="c5"><span class="c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 490.41px; height: 202.98px;"><img alt="" src="files/segm/image5.png" style="width: 490.41px; height: 202.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c2"></span></p><p class="c5"><span class="c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c15">Image taken from the SPP paper by He at el. 2015</span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 477.50px; height: 332.36px;"><img alt="" src="files/segm/image1.png" style="width: 477.50px; height: 332.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c15">Image taken from the SPP paper by He at el. 2015</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">The SPP module removes the fully connected layer from the bottom of a regular CNN based model (e.g VGG19 , GoogleNet etc) and replaces it with the SPP layer.</span></p><p class="c5"><span class="c1">It is composed of parallel pooling (max/average) layers having bin size proportional to input size while keeping the number of bin fixed for each of these parallel heads. For example a feature map of size 32x32 will require a bin (or kernel) size of 8x8 with stride 8 for resulting in a 4x4 output. A 64x64 will require a kernel size i.e 16x16 , stride = 16 for producing an output of the same size. &nbsp;The trick is dynamically changing the kernel size and stride of the pooling layers based on input size.</span></p><p class="c5"><span>The way they calculate it is as follows &nbsp;: </span><span class="c14 c11 c16">kernel_size = ceil(input/output) , stride = floor(input/output).</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">Apart from making the model capable of taking variable input size it comes with another major improvement with the multiple heads with different output size or different receptive fields. Each of the 4x4, 2x2 , 1x1 polling layers looks at objects at different scales. Additionally, 1x1 filter is a global pooling operation which can be considered like BoW. </span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.jko2kubkm30a"></a><p class="c5"><span class="c9 c11">FCN :</span><span class="c9">&nbsp;</span><span class="c6">Fully Convolutional Networks for Semantic Segmentation</span></p><hr><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 702.00px; height: 381.00px;"><img alt="" src="files/segm/image10.png" style="width: 702.00px; height: 381.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Long at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">FPN replaced the FC layer with a convolution layer that&rsquo;s how it is a Fully Convolutional NN. It can take arbitrary input images since it has no dense layer at the end.</span></p><p class="c5"><span class="c1">It used bilinear interpolation for upsampling at the end for generating highly coarse to dense output. Each channel in the output represents one category which was optimized with pixel wise cross entropy loss. Eventually the last layer has a softmax activation for assigning each spatial pixel to one class.</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">Shift and stitching : Applying maxpool with stride 2 to an image store the output. Apply maxpool stride 2 on the same image with one pixel left and down shifted. The second maxpool will apply pooling on the locations where the first one skipped due to stride=2. Now create final output by taking one row from output-1 and the one from output-2 starting from the left for each case. Do the same for columns starting from top for each output. It will reconstruct the output image with the same size of the input resolution.</span></p><p class="c5"><span class="c1">Upsampling : It is like 1/f strided pooling.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.a1wxvkgii8h8"></a><p class="c5"><span class="c6">U-Net: Convolutional Networks for Biomedical Image Segmentation</span></p><hr><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 406.67px;"><img alt="" src="files/segm/image9.png" style="width: 624.00px; height: 406.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Ronneberger at el</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.x5u01mdm4ok"></a><p class="c5"><span class="c9">Pyramid Scene Parsing Network </span></p><hr><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1058.92px; height: 290.90px;"><img alt="" src="files/segm/image13.png" style="width: 1058.92px; height: 290.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from Zhao at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.rb4xs7vp2epa"></a><p class="c5"><span class="c9">Feature Pyramid Network for Object Detection</span></p><hr><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 740.00px; height: 245.00px;"><img alt="" src="files/segm/image6.png" style="width: 740.00px; height: 245.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Lin at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.3unk15mw2mil"></a><p class="c5"><span class="c2">DeepLab V1 &nbsp;: SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 696.00px; height: 326.00px;"><img alt="" src="files/segm/image12.png" style="width: 696.00px; height: 326.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from Chen at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.kzmo884wbiqc"></a><p class="c5"><span class="c2">DeepLab V2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c1">Atrous Convolution : </span></p><p class="c5"><span class="c1">Receptive field of a convolutional filter is the pixel region the kernel covers and calculates the results on. A 3x3 convolution has a 3x3 receptive field.</span></p><p class="c5"><span class="c1">Intuitively we can understand the larger the receptive the larger objects/features can be covered. In other words it will cover more coarse features. The smaller it gets the more detailed or dense information can be analyzed. Both are necessary for perceiving the visual objects within an image.</span></p><p class="c5"><span class="c1">In typical CNN architecture a conv layer is often followed by a max pool which reduces the image resolution to half. Thus it makes the receptive field twice large for the next conv. Layer. In this way of downsizing we are losing valuable dense information within the image which is necessary for semantic segmentation to work well.</span></p><p class="c5"><span class="c1">Another way for increasing the receptive field without max pool is using larger kernel sizes like 7x7 or 11x11. It will require lots of model parameters and the model will be too heavy to train.</span></p><p class="c5"><span class="c1">Atrous convolution is like regular convolution just having gaps between the kernel weights. A 3x3 conv kernel can be arranged in a 5x5 grid with one pixel gap. For ease of understanding it can be assured zero weights at the gap position. Putting a zero in the kernel will require the same memory of a 5x5 kernel since &lsquo;0&rsquo; is also a number. So the memory efficient implementation is a little tricky. Good thing is regular convolution in pytorch or tensorflow supports dilation / rate parameter which can be used as atrous convolution efficiently. Using padding = rate will result in equal sized output from an input image just like the figure below shows.</span></p><p class="c5"><span class="c1">For looking at different</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.00px; height: 280.00px;"><img alt="" src="files/segm/image11.png" style="width: 563.00px; height: 280.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Artrous conv. Image from chen at. El. 2017</span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 750.81px; height: 340.67px;"><img alt="" src="files/segm/image14.png" style="width: 750.81px; height: 340.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Chen at el.</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="id.3uzojslo4aec"></a><p class="c5"><span class="c2">UPSNet</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1085.15px; height: 303.87px;"><img alt="" src="files/segm/image2.png" style="width: 1085.15px; height: 303.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Xiong at el.</span></p><p class="c0"><span class="c1"></span></p><a id="id.x605mzn41se0"></a><p class="c5"><span class="c2">Panoptic Feature Pyramid Networks</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 540.00px; height: 407.00px;"><img alt="" src="files/segm/image8.png" style="width: 540.00px; height: 407.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c5"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c9">DeepLab V3 : </span><a id="id.4vgavbjt74rk"></a><span class="c2">Rethinking Atrous Convolution for Semantic Image Segmentation</span></p><hr><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1017.41px; height: 324.05px;"><img alt="" src="files/segm/image4.png" style="width: 1017.41px; height: 324.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 997.00px; height: 308.00px;"><img alt="" src="files/segm/image7.png" style="width: 997.00px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1405.0312.pdf&amp;sa=D&amp;source=editors&amp;ust=1679496757802280&amp;usg=AOvVaw2hmdRLQRBrmyKdwtUUOlTz">https://arxiv.org/pdf/1405.0312.pdf</a></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&amp;sa=D&amp;source=editors&amp;ust=1679496757802774&amp;usg=AOvVaw0W6dJ5C0dc-F8a7UCY2n4e">https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</a></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://learnopencv.com/deeplabv3-ultimate-guide/&amp;sa=D&amp;source=editors&amp;ust=1679496757803023&amp;usg=AOvVaw1yjanN-Qrv7cFdPVYi6YTo">https://learnopencv.com/deeplabv3-ultimate-guide/</a></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://sthalles.github.io/deep_segmentation_network/&amp;sa=D&amp;source=editors&amp;ust=1679496757803253&amp;usg=AOvVaw39_SvD0Nw0Jtp3nthjOUor">https://sthalles.github.io/deep_segmentation_network/</a></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://towardsdatascience.com/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5&amp;sa=D&amp;source=editors&amp;ust=1679496757803527&amp;usg=AOvVaw3bczHOfGSBl93HP-8ea0Or">https://towardsdatascience.com/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5</a></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c&amp;sa=D&amp;source=editors&amp;ust=1679496757803866&amp;usg=AOvVaw3hnlaWJVvFWcCNOnVEybTW">https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c</a></span></p><p class="c5"><span class="c7"><a class="c8" href="https://www.google.com/url?q=https://zhuanlan.zhihu.com/p/56035377&amp;sa=D&amp;source=editors&amp;ust=1679496757804245&amp;usg=AOvVaw1X-A1VSHAU5KPTSA-lGRIS">https://zhuanlan.zhihu.com/p/56035377</a></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p></body></html>