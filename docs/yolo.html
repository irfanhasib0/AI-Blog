<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_8s7n3kf664eg-8{list-style-type:none}ul.lst-kix_8s7n3kf664eg-7{list-style-type:none}.lst-kix_4gvvutwnlpi-1>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-0>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-5>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-4>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-6>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-2>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-3>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-8>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-7>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-8>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-7>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-5>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-6>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-8>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-6>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-5>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-7>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-0>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-2>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-1>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-4>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-3>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-2{list-style-type:none}.lst-kix_y89d79xx04k4-0>li:before{content:"-  "}.lst-kix_y89d79xx04k4-1>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-1{list-style-type:none}ul.lst-kix_4gvvutwnlpi-4{list-style-type:none}ul.lst-kix_4gvvutwnlpi-3{list-style-type:none}.lst-kix_y89d79xx04k4-2>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-0{list-style-type:none}ul.lst-kix_4gvvutwnlpi-6{list-style-type:none}.lst-kix_y89d79xx04k4-7>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-5{list-style-type:none}ul.lst-kix_4gvvutwnlpi-8{list-style-type:none}ul.lst-kix_4gvvutwnlpi-7{list-style-type:none}.lst-kix_y89d79xx04k4-6>li:before{content:"-  "}.lst-kix_y89d79xx04k4-4>li:before{content:"-  "}.lst-kix_y89d79xx04k4-5>li:before{content:"-  "}.lst-kix_y89d79xx04k4-3>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-3>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-8{list-style-type:none}ul.lst-kix_10bg29dgyeg1-7{list-style-type:none}.lst-kix_p7e7tkpe56mn-0>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-4>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-6{list-style-type:none}ul.lst-kix_10bg29dgyeg1-5{list-style-type:none}ul.lst-kix_10bg29dgyeg1-4{list-style-type:none}ul.lst-kix_10bg29dgyeg1-3{list-style-type:none}ul.lst-kix_10bg29dgyeg1-2{list-style-type:none}ul.lst-kix_10bg29dgyeg1-1{list-style-type:none}.lst-kix_p7e7tkpe56mn-1>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-0{list-style-type:none}.lst-kix_p7e7tkpe56mn-2>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-0>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-2>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-1>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-8>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-4>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-3>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-5>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-2>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-6>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-5>li:before{content:"-  "}ul.lst-kix_y89d79xx04k4-5{list-style-type:none}ul.lst-kix_y89d79xx04k4-4{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-8{list-style-type:none}ul.lst-kix_y89d79xx04k4-7{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-7{list-style-type:none}ul.lst-kix_y89d79xx04k4-6{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-6{list-style-type:none}.lst-kix_8s7n3kf664eg-4>li:before{content:"-  "}ul.lst-kix_y89d79xx04k4-1{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-5{list-style-type:none}ul.lst-kix_y89d79xx04k4-0{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-4{list-style-type:none}ul.lst-kix_y89d79xx04k4-3{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-3{list-style-type:none}ul.lst-kix_y89d79xx04k4-2{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-2{list-style-type:none}.lst-kix_8s7n3kf664eg-3>li:before{content:"-  "}ul.lst-kix_mnrrbqvs6ca-1{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-0{list-style-type:none}.lst-kix_10bg29dgyeg1-0>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-1>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-6>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-7>li:before{content:"-  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_y89d79xx04k4-8>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-8{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-7{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-6{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-5{list-style-type:none}ul.lst-kix_y89d79xx04k4-8{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-4{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-3{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-2{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-1{list-style-type:none}ul.lst-kix_8s7n3kf664eg-4{list-style-type:none}.lst-kix_10bg29dgyeg1-8>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-0{list-style-type:none}ul.lst-kix_8s7n3kf664eg-3{list-style-type:none}ul.lst-kix_8s7n3kf664eg-6{list-style-type:none}.lst-kix_10bg29dgyeg1-7>li:before{content:"-  "}ul.lst-kix_8s7n3kf664eg-5{list-style-type:none}ul.lst-kix_8s7n3kf664eg-0{list-style-type:none}ul.lst-kix_8s7n3kf664eg-2{list-style-type:none}ul.lst-kix_8s7n3kf664eg-1{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c33{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:110.2pt;border-top-color:#000000;border-bottom-style:solid}.c35{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:118.5pt;border-top-color:#000000;border-bottom-style:solid}.c4{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:463.5pt;border-top-color:#000000;border-bottom-style:solid}.c24{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117.8pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:120.8pt;border-top-color:#000000;border-bottom-style:solid}.c28{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:138pt;border-top-color:#000000;border-bottom-style:solid}.c40{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:114.8pt;border-top-color:#000000;border-bottom-style:solid}.c1{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c14{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c18{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c8{color:#ff9900;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c13{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:italic}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c16{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:italic}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c17{color:#000000;text-decoration:none;font-size:11pt;font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c27{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c31{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c34{color:#666666;text-decoration:none;vertical-align:baseline;font-size:14pt}.c37{color:#000000;text-decoration:none;font-size:11pt}.c3{vertical-align:sub;font-family:"Times New Roman";font-weight:400}.c41{color:#000000;text-decoration:none;font-style:normal}.c23{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c22{color:inherit;text-decoration:inherit}.c26{font-family:"Times New Roman";font-weight:700}.c38{padding:0;margin:0}.c25{margin-left:36pt;padding-left:0pt}.c10{font-weight:400;font-family:"Times New Roman"}.c39{font-size:14pt}.c15{font-size:12pt}.c21{height:0pt}.c36{vertical-align:baseline}.c30{font-style:italic}.c29{font-size:10pt}.c11{height:11pt}.c32{vertical-align:super}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c23 doc-content"><p class="c20"><span class="c26 c30 c39">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13">Object Detection : Single Shot Detectors (YOLO V1-V3 , SSD)</span></p><hr><p class="c2 c11"><span class="c16"></span></p><p class="c2 c11"><span class="c26 c36 c30 c37"></span></p><p class="c2"><span class="c16">Faster RCNN Recap :</span></p><hr><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c10 c15">Among the RCNN Family of algorithms discussed in the </span><span class="c31 c10 c15"><a class="c22" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667672801170881&amp;usg=AOvVaw13nBV-trFwkTbTaZdrQP_q">previous article</a></span><span class="c10 c15">, Faster RCNN (2015) and Mask RCNN (2017) are the fastest and most accurate. After Faster RCNN &nbsp;YOLO v1 and then SSD was published and started to get popular in the object detection community. Both of them are single shot detectors; it means they take one RGB image as input and predict the bboxes and class (bounding boxes) by passing the image data only once through different sections of the model. These three algorithms are nearly contemporary and share many relevant upgrades. &nbsp;I will start this section with YOLO-v1 and SSD, then will cover YOLO-v2 and YOLO-v3 as well. For knowing details about Mask RCNN or Faster RCNN please refer to the blog on </span><span class="c31 c10 c15"><a class="c22" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667672801171264&amp;usg=AOvVaw3FRsKMW7LAoxxwAtvQvuyK">RCNN &nbsp;Family</a></span><span class="c5">. </span></p><p class="c2"><span class="c5">&nbsp; &nbsp; Before we start, let&rsquo;s remember a little about Faster RCNN. &nbsp;One major drawback of Faster RCNN is that it is still quite slow(~ 7 FPS), consequently fails to support many of the real time applications which require faster prediction. It was able to achieve the region proposals and Fmap RoIs by one pass through a network by sharing most of the convolutional layers. The major bottleneck &nbsp;of Faster RCNN is that it still requires one by one passing of the feature maps through the classifier, possibly it is one of the key factors that helps the algorithm to reach its high level of &nbsp;accuracy. YOLO tried to make a tradeoff between speed and accuracy. It replaced the one by one Fmap processing with an end to end manner. The bbox regressor is a tensor having the width and height of the feature map and a depth of 4k + 2k , k = number of anchor bbox ; 4 -&gt; {x,y,w,z} ; 2-&gt; {object , no object confidence}. So at every feature map location it is able to predict k no of box coordinates and confidence. Unlike Faster RCNN it only learns objectness confidence not background confidence.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c10 c15">&nbsp; &nbsp; &nbsp; </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 746.50px; height: 398.37px;"><img alt="" src="files/yolo/image7.png" style="width: 746.50px; height: 398.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c18">Figure : Faster RCNN </span></p><p class="c2 c11"><span class="c18"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c16">YOLO Version 1 </span></p><hr><p class="c2 c11"><span class="c16"></span></p><p class="c2"><span class="c10 c15">YOLO v1 stacked the two different branches of RPN together, which were for 4k bbox coordinates and 2k confidence respectively. So across the depth it has one additional channel for class agnostic box confidence. It also stacked extra N channels at the end of this output tensor dedicated for each class. They can predict the probability of each of the object classes per feature map location. It eliminates the necessity of additional &nbsp;classifiers layers and effectively makes it a single shot detector.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c7">Improvements :</span></p><p class="c2"><span class="c26 c15">1. Joining the Region Proposal Generator and Classifier together &nbsp;:</span><span class="c5">&nbsp;For YOLO the RPN tensor , confidence tensor (objectnes confidence only), the classifier tensors are all stacked together. Consequently, the outputs of the RPN are required to be dealt separately for projecting on top of the feature map. </span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c26 c15">2. Eliminating RoI Align :</span><span class="c5">&nbsp;</span></p><p class="c2"><span class="c5">[i] Faster RCNN predicts the Region Proposal bbox wrt the original image size so they need to be scaled down for projecting on the featuremap. YOLO directly predicts the region proposals on top of the feature map so no need to scale down and projection anymore. </span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c5">[ii] Faster RCNN requires the Fmap RoI to be of fixed size so that they can be fed to the classifier layers but here the classification is done parallely for every region proposal at every feature map location. No need to input the feature maps to the classifier layers again.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c26 c15">3. Eliminated One by One Fmap RoI classification :</span><span class="c5">&nbsp;It is done more efficiently by jointly predicting the bboxes and class probabilities (for every object) at every featuremap location.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c7">Draw backs :</span></p><p class="c2"><span class="c5">It does not use an anchor box in v1. Eventually v2 adopted it. </span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c7">How it works : </span></p><p class="c2"><span class="c26 c15">&nbsp; &nbsp; &nbsp; </span><span class="c5">In the context of deep convolutional neural networks the output from every CNN stage is a 3D array which we are calling feature maps. It represents distinct high level information across the channels for each of the spatial pixel locations. &nbsp;Usually, the first two dimensions are width and height gradually shrinks down from the original image while the depth gets increased in parallel. It enables the deeper layers to represent more complex image features by each of its depth channels depthwise.</span></p><p class="c2"><span class="c26 c15">&nbsp; &nbsp; &nbsp; </span><span class="c5">The YOLO model takes an image and outputs an array of shape 7x7xN. Each of the channels of the output tensor represent - &ldquo;x,y,w,h,objectness confidence, class 1 confidence, class 2 confidence, ........., class n confidence&rdquo; respectively at every 7x7 grid location. The following example will make it more clear. The red circle on the grid (figure below) represents the 1x2 location in the featuremap. So the numeric value at (grid_x,grid_y,chanel) will represent the following information -</span></p><p class="c2"><span class="c5">&nbsp; &nbsp; &nbsp; </span></p><a id="t.8d740092d50839d677c0c827cdd6281d9c117a30"></a><a id="t.0"></a><table class="c27"><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c6"><span class="c5">(Grid X, Grid Y, Channel)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c5">Information</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x0)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">x ; If there is a object at this location, &nbsp;&#39;x&#39; would be the offset of its location along the height</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x1)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">y; &nbsp;similarly &#39;y&#39; would be its offset along width.</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x2)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">w; &nbsp;&#39;w&#39; is the width of the object bbox</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x3)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">h; &nbsp; &#39;h&#39; is the height of the bbox.</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x4)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">probability of presence of any class object at that location.</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x5)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">probability of the presence of a &#39;class 1&#39; object at that location</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">&nbsp;&hellip;</span></p><p class="c2"><span class="c5">&nbsp;&hellip;</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c6"><span class="c5">&hellip;</span></p><p class="c6"><span class="c5">&hellip;</span></p></td></tr><tr class="c21"><td class="c28" colspan="1" rowspan="1"><p class="c2"><span class="c5">(1x2x n+5)</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c2"><span class="c5">probability of the presence of a &#39;class n&#39; object at that location </span></p></td></tr></table><p class="c2 c11"><span class="c5"></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c5">They designed the loss function in such a way that the model only learns the x,y offsets from the grid corner, not the absolute pixel coordinate of the object in the image (See x,y values in the enlarged figure in the bottom-right side). The absolute coordinate is derived from the grid location of the feature map. Its value is restricted from 0-1 by applying sigmoid activation function, so that it does not cross the current cell boundary. If it needed to learn the absolute coordinate the value of x,y needed to cover the entire image width and height. Learning an object&#39;s location in the entire image precisely is a harder task then just learning its location within the grid.</span></p><p class="c2"><span class="c5">Now x,y covers a small portion of the region but for w, h there is no restriction for staying within the grid. So w,h loss can get very high sometimes for big boxes. It can make the training unstable for all the losses since they are added together for gradient calculation. For handling this issue they use the difference between the square root of w and h instead of their actual value (Just to remind Faster RCNN was using log scale here). See the loss function below for better understanding.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 860.42px; height: 473.90px;"><img alt="" src="files/yolo/image9.png" style="width: 860.42px; height: 473.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c10 c15">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c14">Figure : Yolo V1</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c26 c15 c30">The Algorithm </span></p><p class="c2"><span class="c5">&nbsp;One thing is avoided intentionally for simplicity. Yolo v1 uses two boxes at each pixel location. So there would be two sets of x,y,w,h like -&gt; x1,y1,w1,h1 , x2,y2,w2,h2. &nbsp;Resulting the depth as follows - 2 (two boxes) * 4 + 1+ no of class. &nbsp;For VOC dataset they used if no of box = 2 and class = 20 having a depth of 2*4 + 1+ 20 = 30 resulting in the output tensor with shape 7x7x30.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c7">Training Steps </span></p><p class="c2"><span class="c5">1. Preparing GT tensor : It is a good practice to prepare the mini-batch generator functions for training to output the same data format of the YOLO v1 model output. It should be of size (7x7xN) N = &nbsp;4 + 1 + no of classes. Now the loss can be calculated quite easily by comparing with the model output and train data since they have the same shape.</span></p><p class="c2"><span class="c5">2. Forward pass : Feed the RGB image to the model and extract the predicted tensor with shape 7x7xN</span></p><p class="c2"><span class="c5">3. xy_loss : slice the first 2 channels of the ground truth and prediction tensors both having shape 7x7x2 and calculate the squared difference.</span></p><p class="c2"><span class="c5">In numpy syntax it would be like this -<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &gt;&gt; xy_gt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = gt_tensor[ : , : , :2 ] </span></p><p class="c2"><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&gt;&gt; xy_pred &nbsp; &nbsp; &nbsp; &nbsp; = predicted_tensor[: , : , :2]</span></p><p class="c2"><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&gt;&gt; xy_loss &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= numpy.mean (numpy.square(xy_gt - xy_pred))</span></p><p class="c2"><span class="c5">4. wh_loss : slice out the 3rd and 4th (tensor[:, :, 2:4]) channel and take their square root and calculate squared difference for w,h error.</span></p><p class="c2"><span class="c5">5. conf_loss :</span></p><p class="c2"><span class="c10 c15">&nbsp; &nbsp; &nbsp; - slice the 5th channel (tensor[:, :, 5]) and apply an object mask [1</span><span class="c10 c15 c32">obj</span><span class="c5">&nbsp;] on every cell. It means Multiply the locations with an object in GT with 1 else 0. Finally calculate mse among the masked </span></p><p class="c2"><span class="c5">tensores. </span></p><p class="c2"><span class="c10 c15">&nbsp; &nbsp; &nbsp;- calculate the loss again this time applying no object mask [1</span><span class="c10 c15 c32">nobj</span><span class="c10 c15">] on every cell. Add this loss with a smaller weight [&#x1d6cc;</span><span class="c3 c15">noobj &nbsp;</span><span class="c5">] because there will be lot of box with no object compared to object boxes in the GT.</span></p><p class="c2"><span class="c5">6.Class confidence loss : </span></p><p class="c2"><span class="c5">&nbsp; &nbsp; &nbsp; - Slice rest of the channels (tensor[:, :, 5: ]) , convert the raw values to conditional probabilities P(class | object) by multiplying with object confidence at the same location. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c2"><span class="c5">&nbsp; &nbsp; &nbsp; - Then calculate the classification loss in the same way as confidence loss. Note currently softmax and cross entropy loss is used at this place in the SOTA methods.</span></p><p class="c2"><span class="c5">7. Finally, backpropagation is done with the calculated loss.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.50px; height: 296.24px;"><img alt="" src="files/yolo/image3.png" style="width: 439.50px; height: 296.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c7">Inference Steps :</span></p><p class="c2"><span class="c5">For drawing the bbox in the original image we need to project the predicted x,y,w,h to the respective locations of the actual image.</span></p><p class="c2"><span class="c5">&nbsp;- If the Image size = (416x416) and model output size = (7x7), the ratio of image and fmap is, &nbsp;grid-scale =416/7 = 13</span></p><p class="c2"><span class="c5">&nbsp;- Each of the 7x7 grids can be projected back to its parent location in the image by multiplying by grid-scale = 13.</span></p><p class="c2"><span class="c5">&nbsp;- Let&#39;s consider the following values for x,y,w,h = [0.3,0.2,4,5], in the above example at grid location [grid_x,grid_y] = [1,2].</span></p><p class="c2"><span class="c5">X = grid-scale * (grid_x + x) = 13 * (1 + 0.3) = &nbsp;16.9 ~ 17</span></p><p class="c2"><span class="c5">Y = grid-scale * (grid_y + y) = 13 * (2 + 0.2) = &nbsp;28.6 ~ 29</span></p><p class="c2"><span class="c5">W = grid-scale * w &nbsp;= 4 &nbsp;* 13 = 52</span></p><p class="c2"><span class="c5">H &nbsp;= grid-scale * h &nbsp; = 5 &nbsp;* 13 &nbsp;= 65</span></p><p class="c2"><span class="c5">The reverse would be like below,</span></p><p class="c2"><span class="c5">x = X / grid-scale - grid_x </span></p><p class="c2"><span class="c5">y = Y / grid-scale - grid_y </span></p><p class="c2"><span class="c5">w = W / grid-scale</span></p><p class="c2"><span class="c5">h = H /grid-scale</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c26 c39 c30">SSD &nbsp;(</span><span class="c15 c26">Single shot multi box detector.</span><span class="c16">)</span></p><hr><p class="c2 c11"><span class="c7"></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c5">Unlike Faster RCNN both YOLO v1 and SSD eliminate the necessity of putting the RoI Feature maps one by one through the dense classifier layers. They are both single pass / single shot detectors. One image passes through the end to end network and predicts the bboxes along with class labels in one pass. </span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c5">SSD is better than YOLO v1 in the following ways :</span></p><p class="c2"><span class="c26 c15">1. Multi scale feature maps : </span><span class="c10 c15">The feature maps with higher resolution would be able to predict the small objects better while smaller ones should take care of the large objects. It uses the different resolution feature maps from different stages of the model for final prediction. It helps the model to predict objects of different sizes more easily. &nbsp;The feature maps would have varying depths while for prediction we need fixed depth consisting of i.e x,y,w,h ,conf etc. They pass the arbitrary depth feature maps from different stages through a 3x3 </span><span class="c10 c15">conv</span><span class="c10 c15">. (convolutional) layer with depth k*(classes+4). From the outputs of the </span><span class="c10 c15">conv</span><span class="c5">. layer they calculate the boxes like YOLO V1. For mxn featuremap with k aspect ratio boxes it can predict k boxes at each of the m*n location resulting boxes = k*m*n. </span></p><p class="c2"><span class="c5">SSD predicts a total of 8732 boxes of different scaled feature maps.</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c17 c26 c36">2. Default boxes :</span></p><p class="c2"><span class="c10">For each of the feature map scales they used the default box. It is similar to the anchor box in RCNN, see the anchor box section in &nbsp;my </span><span class="c31 c10 c15"><a class="c22" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667672801180977&amp;usg=AOvVaw31V1dkAbP608xqHWAiRbEo">RCNN &nbsp;Family</a></span><span class="c10 c15">&nbsp;</span><span class="c10">&nbsp;blog for better understanding of this part. Unlike Faster RCNN they used 9 boxes for each feature map scale with aspect ratios &nbsp;A</span><span class="c3">r</span><span class="c10">&nbsp;= {1,2,3,1/2,1/3}. They scaled the anchor boxes at different feature map levels with a scale factor which ensures the feature map stage emphasizes on learning the intended scales. In Faster RCNN they used three scales 128x128,256x256,512x512 each with 3 aspect ratio 1:1,1:2,2:1. In SSD each of the feature maps from each stage of the network corresponds to one scale, resulting in a total 6 scales. Each of the scales have 4,6,6,6,4,4 no of aspect ratios. The 1st stage of feature map (38x38) which can look at the small objects at higher resolution so they have not assigned the top 2 largest aspect ratios for it i.e A</span><span class="c3">r</span><span class="c0">&nbsp;= {1,2,3,1/2,1/3}. For similar reasons the smallest 2 scales don&rsquo;t care about the large aspect ratio. For example scale 38x38 should learn small objects so they scaled it with small scale factor = 0.2. Scale, s = 0.2 and s = 0.9 all the intermediate feature maps are equally mapped. The small scale (0.2) for the 38x38 pushes it to find small boxes while the 1x1 map with scale 0.9 will try to find very big boxes at the center of the image.</span></p><p class="c2"><span class="c0">Width and height is calculated from scale and aspect ratio as follows - width, &nbsp;w = s * sqrt(A) ; height, h = s/sqrt(A)</span></p><p class="c2 c11"><span class="c0"></span></p><ul class="c38 lst-kix_4gvvutwnlpi-0 start"><li class="c2 c25 li-bullet-0"><span class="c0">l = 1 , 2, &nbsp;&hellip;., L</span></li><li class="c2 c25 li-bullet-0"><span class="c10">s = s</span><span class="c3">min</span><span class="c10">&nbsp;+[ (s</span><span class="c3">max</span><span class="c10">&nbsp;- s</span><span class="c3">min</span><span class="c0">) * (l - 1) ] / (L - 1) = 0.20 , 0.34, 0.48, 0.62, 0.78, 0.90</span></li><li class="c2 c25 li-bullet-0"><span class="c10">A</span><span class="c3">r</span><span class="c0">&nbsp;= {1,2,3,1/2,1/3}</span></li><li class="c2 c25 li-bullet-0"><span class="c0">w = s * sqrt(A)</span></li><li class="c2 c25 li-bullet-0"><span class="c0">h &nbsp;= s / sqrt(A)</span></li></ul><p class="c2 c11"><span class="c9"></span></p><p class="c2 c11"><span class="c9"></span></p><a id="t.4eabd1635ee44005de5fdbd1dd863bd5d2681e49"></a><a id="t.1"></a><table class="c27"><tr class="c21"><td class="c12" colspan="1" rowspan="1"><p class="c6 c11"><span class="c9"></span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c6"><span class="c10 c29">A =3 </span><span class="c8">(N/A)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c6"><span class="c9">A= 2</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c6"><span class="c9">A=1</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c10 c29">A= 1 ; s = sqrt(s</span><span class="c3 c29">l</span><span class="c10 c29">*s</span><span class="c3 c29">l+1</span><span class="c9">))</span></p></td><td class="c40" colspan="1" rowspan="1"><p class="c6"><span class="c9">A= 1/2</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c6"><span class="c10 c29">A= 1/3 </span><span class="c8">(N/A)</span></p></td></tr><tr class="c21"><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">Fmap size = 38x38</span></p><p class="c6"><span class="c9">s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 0.20</span></p><p class="c6"><span class="c9">s&rsquo; = sqrt(0.2*0.38) = 0.28</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c6"><span class="c8">w = 0.2 * sqrt(3) &nbsp;= &nbsp;0.34</span></p><p class="c6"><span class="c8">h &nbsp;= 0.2 / sqrt(3) &nbsp; = 0.11</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.2 * sqrt(2) &nbsp;= 0.28</span></p><p class="c6"><span class="c9">h &nbsp;= &nbsp;0.2 / sqrt(2) = 0.14</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.2 * sqrt(1) = 0.2</span></p><p class="c6"><span class="c9">h &nbsp;= 0.2 * sqrt(1) = 0.2</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.26 * sqrt(1) = 0.26</span></p><p class="c6"><span class="c9">h &nbsp;= 0.26 / sqrt(1) &nbsp;= 0.26</span></p></td><td class="c40" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.2 * sqrt(1/2) = 0.14</span></p><p class="c6"><span class="c9">h &nbsp;= 0.2 / sqrt(1/2) &nbsp;= 0.28</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c6"><span class="c8">w = 0.2 * sqrt(1/3) = 0.11</span></p><p class="c6"><span class="c8">h &nbsp;= 0.2 / sqrt(1/3) = 0.34</span></p></td></tr><tr class="c21"><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c40" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c6"><span class="c9">&hellip;</span></p><p class="c6"><span class="c9">&hellip;</span></p></td></tr><tr class="c21"><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">Fmap size = 1x1</span></p><p class="c6"><span class="c9">s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 0.90</span></p><p class="c6"><span class="c9">s&rsquo; = sqrt(0.9*1) = 0.94</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c6"><span class="c8">w = 0.90 * sqrt(3) &nbsp;= &nbsp;1.56</span></p><p class="c6"><span class="c8">h &nbsp;= 0.90 / sqrt(3) &nbsp; = 0.52</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.90 * sqrt(2) &nbsp;= 1.27</span></p><p class="c6"><span class="c9">h = &nbsp;0.90 / sqrt(2) = 0.63</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.90 * sqrt(1) = 0.9</span></p><p class="c6"><span class="c9">h &nbsp;= 0.90 * sqrt(1) = 0.9</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.94 * sqrt(1) = 0.94</span></p><p class="c6"><span class="c9">h &nbsp;= 0.94 / sqrt(1) &nbsp;= 0.94</span></p></td><td class="c40" colspan="1" rowspan="1"><p class="c6"><span class="c9">w = 0.2 * sqrt(1/2) = 0.63</span></p><p class="c6"><span class="c9">h &nbsp;= 0.2 / sqrt(1/2) &nbsp;= 1.27</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c6"><span class="c8">w = 0.2 * sqrt(1/3) = 0.52</span></p><p class="c6"><span class="c8">h &nbsp;= 0.2 / sqrt(1/3) = 1.56</span></p></td></tr></table><p class="c2"><span class="c10">&nbsp;</span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 253.90px;"><img alt="" src="files/yolo/image1.png" style="width: 286.50px; height: 253.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 995.14px; height: 528.81px;"><img alt="" src="files/yolo/image6.png" style="width: 995.14px; height: 528.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c14">&nbsp;Image taken from SSD paper by Leu at el.</span></p><p class="c2 c11"><span class="c14"></span></p><p class="c2"><span class="c17 c26 c36">Loss Function :</span></p><p class="c2"><span class="c0">&nbsp;The loss function is almost the same as Faster RCNN.</span></p><p class="c2"><span class="c10">- Loss = &nbsp; L</span><span class="c3">cls</span><span class="c10">&nbsp;+ alpha * L</span><span class="c17 c3">loc</span></p><p class="c2"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= &nbsp; L</span><span class="c3">loc</span><span class="c10">&nbsp;is the same as Faster RCNN + L</span><span class="c3">cls</span><span class="c0">&nbsp;is softmax over the classes.</span></p><p class="c2 c11"><span class="c26 c15 c36 c41"></span></p><p class="c2"><span class="c26 c15">3. Less model parameters :</span><span class="c10 c15">&nbsp;YOLO V1 It sends the flattened outputs of the </span><span class="c10 c15">conv</span><span class="c10 c15">. Feature maps to the fully connected layers and resized them again into </span><span class="c10 c15">conv</span><span class="c10 c15">. 7x7x30 (for VOC) feature maps. Dense layers require a lot more parameters (for taking the flattened input from the previous </span><span class="c10 c15">conv</span><span class="c10 c15">. layer) then just another </span><span class="c10 c15">conv</span><span class="c10 c15">. layer. Because it </span><span class="c10 c15">need</span><span class="c5">&nbsp;to be flattened &nbsp;Conv. layer may have parameters</span></p><p class="c2"><span class="c5">If the conv layer was directly bypassed to the next conv layter skipping the dense layers using a 3x3 conv kernel &nbsp;the number of weights would be [3x3x512x1024] + [3x3x1024x30] &nbsp;~ 4.9e6</span></p><p class="c2"><span class="c5">- If we do it with a single dense layer. The dense layer can only take the flattened results of [7x7x512x1024] &nbsp;sized tensor resulting ~ 25e6 input nodes. It should output [7x7x30] ~ 1.4e3 weights so that it can be easily resized to 7x7x30 again. The weight for this dense layer would be [7x7x512x1024] x [7x7x30] ~ 37e9</span></p><p class="c2"><span class="c5">It should require at least &nbsp;~1000 times more parameters for this transition part.</span></p><p class="c2 c11"><span class="c14"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c14"></span></p><p class="c2"><span class="c16">YOLO Version 2</span></p><hr><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">1. YOLO v2 also uses anchor boxes at each of the grid locations in the featuremap like Faster RCNN and SSD. They make the model learn the grid offset in x,y direction, log scale offset is learnt for w and h. Unlike the previous approaches they don&rsquo;t hard code (Faster RCNN) or derive the anchor box from some empirical formula (SSD). They perform k means clustering over all the w and h of the bboxes in train data.</span></p><p class="c2"><span class="c0">2. They use k-means clustering over all the images of the dataset for finding the most optimal k number of anchor boxes for different values of k. They found 5 anchor boxes to work best in YOLO-v2. YOLO-v3 eventually adopted 9 anchor boxes from 3 scales.</span></p><p class="c2"><span class="c0">3. They have found batch normalization very effective.</span></p><p class="c2"><span class="c0">4. They have used higher resolution images this time (416x416). They also trained the model with different resolution images &nbsp;during the training for making it robust to image size.</span></p><p class="c2"><span class="c0">5. They have adopted an identity mapping approach which connects the previous layers directly to a future one for passing rich contexts to deeper layers. This approach can also prevent vanishing gradients for very deep networks e.g ResNet50. They copied the output of an older layer and added it to the activations of deeper layers.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 358.50px; height: 266.97px;"><img alt="" src="files/yolo/image4.png" style="width: 358.50px; height: 266.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c18">&nbsp; &nbsp; &nbsp; &nbsp;Image taken from YOLO v2 paper by Redmon at el. &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c2 c11"><span class="c0"></span></p><ul class="c38 lst-kix_10bg29dgyeg1-0 start"><li class="c2 c25 li-bullet-0"><span class="c10">b</span><span class="c3">x</span><span class="c10">&nbsp;= sig(t</span><span class="c3">x</span><span class="c10">) + c</span><span class="c17 c3">x</span></li><li class="c2 c25 li-bullet-0"><span class="c10">b</span><span class="c3">y</span><span class="c10">&nbsp;= sig(t</span><span class="c3">y</span><span class="c10">) +c</span><span class="c3 c17">y</span></li><li class="c2 c25 li-bullet-0"><span class="c10">b</span><span class="c3">w</span><span class="c10">&nbsp;= </span><span class="c10">p</span><span class="c3">w</span><span class="c10">e</span><span class="c10 c32">tw</span></li><li class="c2 c25 li-bullet-0"><span class="c10">b</span><span class="c3">h</span><span class="c10">&nbsp;= p</span><span class="c10 c32">h</span><span class="c10">e</span><span class="c17 c10 c32">th</span></li></ul><p class="c2 c11"><span class="c17 c10 c32"></span></p><p class="c2"><span class="c0">&nbsp;For x,y the model is learning the difference between b and c which is the grid offset. (To be precise the model is learning the inverse sigmoid of the grid offset.) For w and h, the model is learning the difference of logarithmic values of each of them.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c3">&nbsp; </span><span class="c10">The outputs of the model are &nbsp;t</span><span class="c3">x ,</span><span class="c10">t</span><span class="c3">y, </span><span class="c10">t</span><span class="c3">w, </span><span class="c10">t</span><span class="c3">h</span><span class="c0">&nbsp;which are the offsets being learnt finally. &nbsp;The below equations may give further intuition about them.</span></p><ul class="c38 lst-kix_y89d79xx04k4-0 start"><li class="c2 c25 li-bullet-0"><span class="c10">t</span><span class="c3">x</span><span class="c10">&nbsp;= inv_sig( b</span><span class="c3">x</span><span class="c10">&nbsp;- c</span><span class="c3">x</span><span class="c0">)</span></li><li class="c2 c25 li-bullet-0"><span class="c10">t</span><span class="c3">y</span><span class="c10">&nbsp;= inv_sig((b</span><span class="c3">y</span><span class="c10">&nbsp;- c</span><span class="c3">y</span><span class="c0">)</span></li><li class="c2 c25 li-bullet-0"><span class="c10">t</span><span class="c3">w</span><span class="c10">&nbsp;= log(b</span><span class="c3">w</span><span class="c10">/p</span><span class="c3">w</span><span class="c10">) = log(b</span><span class="c3">w</span><span class="c10">) - log(p</span><span class="c3">w</span><span class="c0">)</span></li><li class="c2 c25 li-bullet-0"><span class="c10">t</span><span class="c3">h</span><span class="c10">&nbsp;= &nbsp;log(b</span><span class="c3">h</span><span class="c10">/p</span><span class="c3">h</span><span class="c10">) = log(b</span><span class="c3">w</span><span class="c10">) - log(p</span><span class="c3">w</span><span class="c0">)</span></li></ul><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c0">where,</span></p><ul class="c38 lst-kix_8s7n3kf664eg-0 start"><li class="c2 c25 li-bullet-0"><span class="c0">sig(x) = Sigmoid(x) &nbsp; = 1/ (1+e-x) </span></li><li class="c2 c25 li-bullet-0"><span class="c0">inv_sig(x) = Inverse Sigmoid(x) = &nbsp;log(x/(1 -x))</span></li></ul><p class="c1"><span class="c0"></span></p><p class="c2"><span class="c10">Sigmoid function is used to bound a (-inf , </span><span class="c10">inf</span><span class="c0">) output within (0-1). So the inverse sigmoid should do the opposite and make the model capable of outputting any arbitrary value from -inf to inf. The model can output any arbitrary value as offset (before the sigmoid is applied) but sigmoid will transform it to corresponding 0-1 value for fitting into the grid while adding some nonlinearity to the process. Addition of non-linearity is very important, because it enables the model to learn complex underlying functions beyond the linear domain.</span></p><p class="c2 c11"><span class="c17 c26 c36"></span></p><p class="c2"><span class="c17 c26 c36">Minor updates :</span></p><p class="c2"><span class="c10">Selective backpropagation : </span><span class="c0">During training if a bbox has low overlap with the ground truth then back propagation can be skipped for that particular loss. </span></p><p class="c2"><span class="c10">Removing low confidence boxes boxes :</span><span class="c10">&nbsp;After prediction is done bboxes lower than a minimum (i.e 0.3) confidence, P(Class|</span><span class="c10">Object</span><span class="c0">) are filtered out for better accuracy.</span></p><p class="c2"><span class="c10">Non Maximum Suppression :</span><span class="c0">&nbsp; If some of the prediction boxes try to predict the same object with different confidence they should have a good overlap. For prediction boxes with IoU more than a threshold (i.e 0.5) only the one with highest confidence is kept.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c26">Word Tree :</span><span class="c0">&nbsp;YOLO v2 introduced a way for incorporating large classification data sets (without bbox labels i.e ImageNet 1000) to be introduced within the framework. During training if an image samples from a classification dataset then only classification loss is calculated else it will calculate all the losses. For unifying the 1000 classes of imageNet to COCO 80 classes the each of the fine grained categories (i.e persian cat) is mapped back to its parent category present in COCO dataset (i.e cat). They have accomplished it by using the concept from Word Net from the Natural language processing domain. </span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 730.85px; height: 428.17px;"><img alt="" src="files/yolo/image5.png" style="width: 730.85px; height: 428.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c18">Figure : The core idea of YOLO is to find a vector like above at every pixel location in the feature map (not image).</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c16">Yolo Version 3</span></p><hr><p class="c2 c11"><span class="c16"></span></p><p class="c2"><span class="c10">- The main improvement in YOLO-v3 is extraction of data from three different scales of the feature map. They were inspired by the FPN[6] networks for taking this direction. The success of SSD with multi scale feature maps can also be a good inspiration. They have taken the feature maps from the last layer for small box prediction. Then </span><span class="c10">upsmple</span><span class="c0">&nbsp;it by doubling the size (width and height of feature map) and concatenate it with the feature maps from 2 layers back to predict medium sized boxes. They have repeated the same and predicted large boxes from feature maps from 2 more laye back. For upsampling transposed convolution (keras ConvTranspose2D layer with stride=2) can be used; it has some learnable parameters as well which allows the model to be more flexible. Another option is using upsample (keras UpSample2D layer) by repeating the data for filling the neighboring missing values at higher resolution.</span></p><p class="c2"><span class="c0">- They have used 9 anchor boxes (instead of 5) , sorted them by size and divided them into three scales: large , medium and small. Each of the 3 anchor boxes are applied on the corresponding output feature map branch (among small, medium and large) extracted from the network.</span></p><p class="c2"><span class="c0">- For confidence loss they used logistic losses instead of mean squared error. Although, it is a very common practice nowadays.</span></p><p class="c2"><span class="c0">- Softmax needs the class labels to be mutually exclusive. Each grid cell can occupy multiple objects from different classes at different scales. Like one person standing behind a car may have their center at the same location. They replaced the idea of using the softmax for each grid cell location .</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">I will show an example dimension through the figure for understanding it better considering the mobile net v2 as a backbone with input image shape 224x224 for other networks or input size it can be different. I will demonstrate with 80 classes considering the coco dataset. The authors used image size ~ 416x416 with darknet 53.</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 770.92px; height: 552.59px;"><img alt="" src="files/yolo/image2.png" style="width: 770.92px; height: 552.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c18">Figure : YOLO V3</span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c26">1x1 </span><span class="c26">conv</span><span class="c17 c26 c36">. + Reshape</span></p><p class="c2"><span class="c10">1x1</span><span class="c10">&nbsp;convolution layer can change the depth of a feature map array keeping the height and width as it is. Here it is used to change the depth of the output featuremap &nbsp;to appropriate dimensions so that it can be reshaped to our desired dimension. For this purpose the 1x1 </span><span class="c10">conv</span><span class="c0">. should change any arbitrary depth to a length equals to anc box* (4+1+no of class). Height and width remains the same after applying 1x1 convolution. Then reshape is applied to make an extra dimension of size=3 (anc. box). Using a separate dimension for the anchor box is convenient for implementation. Numerical operations in many popular libraries can easily be applied on a dimension of an array. </span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c17 c26 c36">Decode outputs to bboxes :</span></p><p class="c2"><span class="c0">1. Decode the bboxes and class from the output 4d array using respective anchor boxes for that feature map scale. Note that the same anchor boxes</span></p><p class="c2"><span class="c0">are also required for loss calculation.</span></p><p class="c2"><span class="c0">2. Low conf. bbox removal</span></p><p class="c2"><span class="c0">3. Non maximum suppression</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 816.50px; height: 357.62px;"><img alt="" src="files/yolo/image8.png" style="width: 816.50px; height: 357.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c18">Figure : &nbsp;(Left) Non Maximum Suppression (Middle) Total 9 anchor boxes at every fmap locations (Right) IoU and multi aspect ratio objects at one location. </span></p><p class="c2 c11"><span class="c18"></span></p><p class="c2 c11"><span class="c18"></span></p><p class="c2"><span class="c26 c39 c30">Symbols and Short forms</span></p><hr><p class="c2 c11"><span class="c41 c26 c15 c36"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 792.79px; height: 324.78px;"><img alt="" src="files/yolo/image10.png" style="width: 792.79px; height: 324.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c5"></span></p><p class="c2"><span class="c5">Below are some short forms that will be used in the later sections of this chapter : </span></p><p class="c2"><span class="c5">RoI &nbsp; &nbsp;: Region of Interest. Small cropped section of an image.</span></p><p class="c2"><span class="c5">Fmap /feat map : Feature Map. &nbsp;Truncated 3-dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.</span></p><p class="c2"><span class="c5">FE &nbsp; &nbsp; : Feature Extractor. It will represent a deep learning model that extracts features from input data. The term &lsquo;FE&rsquo; will be used synonymously with any deep neural network architecture. e.g VGG16 which exactly does the same.</span></p><p class="c2"><span class="c5">FC &nbsp; &nbsp; : Fully Connected Dense Layer/Layers.</span></p><p class="c2"><span class="c5">bbox &nbsp;: Bounding box. [i.e box coordinates x,y,w,h]</span></p><p class="c2"><span class="c5">GT &nbsp; &nbsp;: Ground Truth</span></p><p class="c2"><span class="c5">CNN : Convolutional Neural Network. ; Conv. : Convolutional.</span></p><p class="c2"><span class="c5">For explainability of the figures I have used consistent symbols while drawing part of the model. &nbsp;The symbols and references are defined at the end.</span></p><p class="c2 c11"><span class="c18"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c41 c26 c39 c36">Code :</span></p><p class="c2"><span class="c10">Yolo V4 : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4&amp;sa=D&amp;source=editors&amp;ust=1667672801196553&amp;usg=AOvVaw2txMy2M3aQxgI5SZ8J5qUg">https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4</a></span></p><p class="c2 c11"><span class="c17 c26 c36"></span></p><p class="c2"><span class="c41 c26 c39 c36">References </span></p><hr><p class="c2 c11"><span class="c17 c26 c36"></span></p><p class="c2"><span class="c0">Papers :</span></p><p class="c2"><span class="c10">1. Faster R-CNN: </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1667672801197257&amp;usg=AOvVaw0-iyHf4kwLPGuk-PqL6OUX">https://arxiv.org/abs/1506.01497</a></span></p><p class="c2"><span class="c10">2. You Only Look Once: Unified, Real-Time Object Detection : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640.pdf&amp;sa=D&amp;source=editors&amp;ust=1667672801197483&amp;usg=AOvVaw17rmlct5K5CZetThvXRBOl">https://arxiv.org/pdf/1506.02640.pdf</a></span></p><p class="c2"><span class="c10">3. SSD: Single Shot MultiBox Detector : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.02325&amp;sa=D&amp;source=editors&amp;ust=1667672801197685&amp;usg=AOvVaw3IIP_JFggug3ZsmIeV37Ly">https://arxiv.org/abs/1512.02325</a></span></p><p class="c2"><span class="c10">4. YOLO9000: Better, Faster, Stronger : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/abs/1612.08242&amp;sa=D&amp;source=editors&amp;ust=1667672801197903&amp;usg=AOvVaw16VDVl84qVh69gZdPQu-2c">https://arxiv.org/abs/1612.08242</a></span></p><p class="c2"><span class="c10">5. YOLOv3: An Incremental Improvement : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/abs/1804.02767?e05802c1_page%3D1&amp;sa=D&amp;source=editors&amp;ust=1667672801198184&amp;usg=AOvVaw3gF186ih9eKIBj5DqVtDFT">https://arxiv.org/abs/1804.02767?e05802c1_page=1</a></span></p><p class="c2"><span class="c10">6. Feature Pyramid Networks for Object Detection : </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/pdf/1612.03144.pdf&amp;sa=D&amp;source=editors&amp;ust=1667672801198453&amp;usg=AOvVaw1LaHczSh9djZztrP3uxQeL">https://arxiv.org/pdf/1612.03144.pdf</a></span></p><p class="c2"><span class="c10">7. Mask R-CNN: </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1667672801198656&amp;usg=AOvVaw2x_lMhm98jRXcHFd7L3U_H">https://arxiv.org/abs/1703.06870</a></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Blogs :</span></p><p class="c2"><span class="c10">1. </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/&amp;sa=D&amp;source=editors&amp;ust=1667672801199004&amp;usg=AOvVaw3PX8iP3gEWjDIviFkA1HzP">https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</a></span></p><p class="c2"><span class="c10">2. </span><span class="c10 c31"><a class="c22" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;sa=D&amp;source=editors&amp;ust=1667672801199257&amp;usg=AOvVaw0OmmCOSUnKChXENmaOncOT">https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span></p><p class="c2"><span class="c10">3. </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a&amp;sa=D&amp;source=editors&amp;ust=1667672801199589&amp;usg=AOvVaw3sx6JDAaIA7_nTjq433T97">https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a</a></span></p><p class="c2"><span class="c10">4. </span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06&amp;sa=D&amp;source=editors&amp;ust=1667672801199989&amp;usg=AOvVaw3hswPJypQtmpIb9hxwbJ0s">https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></span></p><p class="c2"><span class="c10">5.</span><span class="c31 c10"><a class="c22" href="https://www.google.com/url?q=https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b%23:~:text%3DSSD%2520contains%25208732%2520default%2520boxes,to%2520obtain%2520the%2520final%2520prediction.&amp;sa=D&amp;source=editors&amp;ust=1667672801200301&amp;usg=AOvVaw1DUdpv5iudpbczCeGxj9_K">https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b#:~:text=SSD%20contains%208732%20default%20boxes,to%20obtain%20the%20final%20prediction.</a></span></p><p class="c2 c11"><span class="c0"></span></p><p class="c2 c11"><span class="c0"></span></p></body></html>