<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c8{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:italic}.c7{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c11{color:#666666;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c13{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c9{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c14{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{color:inherit;text-decoration:inherit}.c10{font-weight:700;font-style:italic}.c16{background-color:#f8f9fa;font-size:9pt}.c2{vertical-align:sub}.c3{vertical-align:super}.c15{font-weight:700}.c12{color:#666666}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c14 doc-content"><p class="c5"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Object Detection : Single Shot Detectors (YOLO V1-V3 , SSD)</span></p><hr><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c10 c13"></span></p><p class="c5"><span class="c13 c10">Faster RCNN Recap :</span></p><hr><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">So far among the RCNN Family algorithms, Faster RCNN (2015) and Mask RCNN (2017) are the fastest and most accurate. Mask RCNN has 2 major improvements than faster RCNN</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp;- &nbsp;(1) It uses RoI Align instead of RoI Pooling</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp;- &nbsp;(2) It can predict segmentation &nbsp;masks in parallel with the object bbox. </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">After Faster RCNN firstly YOLO v1 and then SSD are the most successful object detection algorithms. Both of them are single shot detectors; it means they take the image as input and predict the bboxes by passing the image data only once through different sections of the model. These three algorithms are quite contemporary and share many relevant upgrades. &nbsp;I will start this section with these three then will cover yolo v2 and v3 with more detail. For knowing details about Mask RCNN or Faster RCNN please refer to the blog on RCNN &nbsp;Family. One major drawback of Faster RCNN is that it is still quite slow(~ 7 FPS), consequently fails to support many real of the time applications which require faster predictions.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; The Faster RCNN was able to achieve the region proposals and Fmap RoIs by one pass through a network by sharing most other conv layer weights. The major bottleneck in the regime of speed for Faster RCNN is that it requires one by one passing of the feature maps through the classifier, possibly this mostly helps the algorithm to reach a high level of &nbsp;accuracy. YOLO tried to make a tradeoff between speed and accuracy. It replaced the one by one Fmap processing with an end to end manner. The bbox regressor was a tensor. It has width and height of the feature map and a depth of 4k + 2k , k = number of anchor bbox ; 4 -&gt; {x,y,w,z} ; 2-&gt; {object , no object confidence}. So it was able to predict k no of box coordinates and 2 confidence for each box at </span></p><p class="c5"><span class="c0">every feature map location.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; YOLO v1 enhanced this tensor further by introducing N more channels in the depth direction, here N = = No of object classes. So the feature map has Height and Width as it is but across the depth it has extra channels which can predict the probability of each of the object class per feature map location. This extra channel eliminates the necessity of additional passing of the Fmap RoIs through the classifier one by one instead it does the same thing in one pass.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 746.50px; height: 398.37px;"><img alt="" src="files/yolo/image4.png" style="width: 746.50px; height: 398.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c11 c10">Figure : Faster RCNN </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">YOLO Version 1 </span></p><hr><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c13 c10">Improvements :</span></p><p class="c5"><span class="c15">1. No separate RPN network :</span><span class="c0">&nbsp;For YOLO the RPN tensor , Confidence tensor (objectnes confidence only), the classifier tensor are all stacked together so a outputs of the RPN is not separable. It all stacked together at the end of the network. It will be discussed in thelater section in detail.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">2. Eliminating RoI Align - I :</span><span class="c0">&nbsp;Faster RCNN predicts the Region Proposal bbox wrt the original image size so they need to be scaled down for projecting on the featuremap. YOLO directly predicts the region proposal on top of the feature map so no need of scaling. Instead the predictions are projected back to the original image.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">3. Eliminating RoI align - II :</span><span class="c0">&nbsp;Faster RCNN requires the Fmap RoI to be a fixed sized so that they can be fed to the classifier layers but here the classification is parallely done for every region proposal at every feature map location.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">4. Eliminated One by One Fmap RoI classification :</span><span class="c0">&nbsp;It is done more efficiently by jointly predicting the bboxes and class probabilities (for every object) at every featuremap location.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Draw backs :</span></p><p class="c5"><span class="c0">It does not use an anchor box in v1. Eventually v2 adopted it. </span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c13 c10">How it works : </span></p><p class="c5"><span class="c0">&nbsp;In the context of deep convolutional neural networks, a featuremap is a tensor or high dimensional array, in this case it is a 3D array. The YOLO model takes an image and outputs a tensor of shape 7x7xN. Each of the channels of the output tensor represent - x,y,w,h,objectness confidence, class 1 confidence, class 2 confidence, .......... class n confidence respectively at every 7x7 grid location.</span></p><p class="c5"><span class="c0">&nbsp;</span></p><p class="c5"><span class="c0">The red circle on the grid represents the 1x2 location in the featuremap. So the numeric value at (grid_x,grid_y,chanel)</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x0) = x ; If there is a object at this location, &nbsp;&#39;x&#39; would be the offset of its location along the height</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x1) = y; &nbsp;similarly &#39;y&#39; would be its offset along width.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x2) = w; &nbsp;&#39;w&#39; is the width of the object bbox</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x3) = h; &nbsp; &#39;h&#39; is height of the bbox.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x4) = probability of presence of any class object at that location.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x5) = probability of the presence of a &#39;class 1&#39; object at that location </span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x[n+5]) = probability of the presence of a &#39;class n&#39; object at that location </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">They designed the loss function in such a way that the model only learns the x,y offsets from the grid corner, not the absolute pixel coordinate of the object in the image. (See the enlarged figure in the bottom-right side). Its value is restricted from 0-1 by applying sigmoid activation function, so that it does not cross the current cell boundary. If it needed to learn the absolute coordinate the value of x,y needed to cover the entire image width and height. Learning an object&#39;s location in the entire image precisely is a harder task then just learning its location within the grid.</span></p><p class="c5"><span class="c0">Now x,y covers a small portion of the region but for w, h there is no restriction for staying within the grid. So w,h loss can get very high sometimes for big boxes. t can make the training unstable for all the losses since they are added together for gradient calculation. For handling this issue they use the difference between the square root of w and h instead of their actual value. See the loss function below for better understanding.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 870.50px; height: 431.00px;"><img alt="" src="files/yolo/image2.png" style="width: 870.50px; height: 431.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c11 c10">Figure : Yolo V1</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c10">The Algorithm </span></p><p class="c5"><span class="c0">In the figure one thing is avoided for simplicity. Yolo uses two boxes at each pixel location. So there would be two set of x,y,w,h like -&gt; x1,y1,w1,h1 , x2,y2,w2,h2</span></p><p class="c5"><span class="c0">Resulting the depth as follows - no of box * 4 + 1+ no of class. </span></p><p class="c5"><span class="c0">For VOC dataset they used if no of box = 2 and class = 20 having a depth of 2*4 + 1+ 20 = 30 resulting in the output tensor with shape 7x7x30.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c13 c10">Training Steps </span></p><p class="c5"><span class="c0">1. prepare GT tensor : It is a good practice to prepare the batch data generator functions matching the output format of the YOLO v1 model of size (7x7xN) N = &nbsp;4 + 1 + no of classes. </span></p><p class="c5"><span class="c0">2. forward pass : Feed the image to the model and extract the predicted tensor with shape 7x7xN</span></p><p class="c5"><span class="c0">3. xy_loss : slice the first 2 channels of the ground truth and prediction tensors both having shape 7x7x2 and calculate the squared difference.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; In numpy syntax slice is like this -&gt; gt_tensor[ : , : , :2 ] </span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&gt; predicted_tensor[: , : , :2]</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; -&gt; mean (square(gt_tensor - pfred_tensor))</span></p><p class="c5"><span class="c0">4. wh_loss : slice out the 3rd and 4th (tensor[:, :, 2:4]) channel and take their square root and calculate squared difference for w,h error. [slice -&gt; tensor[: , : , 2:4]]</span></p><p class="c5"><span class="c0">5. conf_loss :</span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; - slice the 5th channel (tensor[:, :, 5]) and apply an object mask [1</span><span class="c3">obj</span><span class="c0">&nbsp;] on every cell. It means Multiply the locations with an object in GT with 1 else 0. Finally calculate mse among the masked </span></p><p class="c5"><span class="c0">tensores. </span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp;- calculate the loss again this time applying no object mask [1</span><span class="c3">nobj</span><span>] on every cell. Add this loss with a smaller weight [Lambda</span><span class="c2">noobj &nbsp;</span><span class="c0">] because there will be lot of box with no object </span></p><p class="c5"><span class="c0">compared to object boxes in GT.</span></p><p class="c5"><span class="c0">6.Class confidence loss : </span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; - Slice rest of the channels (tensor[:, :, 5: ]) , convert the raw values to conditional probabilities P(class | object) by multiplying with object confidence at the same location. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; - Then calculate the classification loss in the same way as confidence loss. Note currently softmax and cross entropy loss is used at this place in the SOTA methods.</span></p><p class="c5"><span class="c0">7. Backpropagation with the calculated loss and apply gradients on the weights.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.50px; height: 296.24px;"><img alt="" src="files/yolo/image7.png" style="width: 439.50px; height: 296.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c13 c10">Inference Steps :</span></p><p class="c5"><span class="c0">For drawing the bbox in the original image we need to project the predicted x,y,w,h to the respective locations of the actual image.</span></p><p class="c5"><span class="c0">&nbsp;- The ratio of image and fmap is grid-scale =416/7 = 13</span></p><p class="c5"><span class="c0">&nbsp;- Each of the 7x7 grids can be projected back to the original image by multiplying by grid-scale = 13.</span></p><p class="c5"><span class="c0">&nbsp;- On the above example grid location [grid_x,grid_y] = [1,2] let&#39;s consider the following values for x,y,w,h = [0.3,0.2,4,5]</span></p><p class="c5"><span class="c0">X = grid-scale * (grid_x + x) = 13 * (1 + 0.3) = &nbsp;16.9 ~ 17</span></p><p class="c5"><span class="c0">Y = grid-scale * (grid_y + y) = 13 * (2 + 0.2) = &nbsp;28.6 ~ 29</span></p><p class="c5"><span class="c0">The reverse is also true,</span></p><p class="c5"><span class="c0">x = X / grid-scale - grid_x </span></p><p class="c5"><span class="c0">y = Y / grid-scale - grid_y </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">So we have x,y,w,h at each feature map location. After transforming them to original image location it will look like below figure -</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c13 c10">SSD </span></p><hr><p class="c1"><span class="c13 c10"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Single shot multi box detector </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Unlike Faster RCNN both YOLO v1 SSD also eliminates the necessity of putting the RoI Fmaps one by one through the dense classifier. They are both single pass / single shot detectors. One image passes through the end to end network at a time and it results in the bboxes within the image all after just one pass. In one line this is the prime reason for these single shot detectors accuracy.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">SSD is better in the following ways :</span></p><p class="c5"><span class="c0">1. YOLO V1 It sends the flattened outputs of the conv Fmaps to the FC layers and resized them again into conv 7x7x30 (for VOC) feature maps. Dense layers require a lot more parameters (for taking the flattened input from the previous conv layer) then just another conv layer. Because it need to be flattened &nbsp;Conv layer may have parameters</span></p><p class="c5"><span class="c0">If the conv layer was directly bypassed to the next conv layter skipping the dense layers using a 3x3 conv kernel &nbsp;the number of weights would be [3x3x512x1024] + [3x3x1024x30] &nbsp;~ 4.9e6</span></p><p class="c5"><span class="c0">- If we do it with a single dense layer. The dense layer can only take the flattened results of [7x7x512x1024] &nbsp;sized tensor resulting ~ 25e6 input nodes. It should output [7x7x30] ~ 1.4e3 weights so that it can be easily resized to 7x7x30 again. The weight for this dense layer would be [7x7x512x1024] x [7x7x30] ~ 37e9</span></p><p class="c5"><span class="c0">It should require at least &nbsp;~1000 times more parameters for this transition part.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">2. It uses the feature maps from different feature layers pass them through a 3x3 conv layer with depth k*(classes+4). From the outputs of the conv layer they calculate the boxes like YOLO V1.</span></p><p class="c5"><span class="c0">For mxn featuremap with k aspect ratio boxes it can predict k boxes at each of the m*n location resulting boxe = k*m*n. </span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 253.90px;"><img alt="" src="files/yolo/image1.png" style="width: 286.50px; height: 253.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 995.14px; height: 528.81px;"><img alt="" src="files/yolo/image3.png" style="width: 995.14px; height: 528.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c10 c12">&nbsp;</span><span class="c10 c12 c16">Image taken from SSD paper by Leu at el.</span></p><p class="c5"><span class="c13 c10">YOLO Version 2</span></p><hr><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">1. Using anchor boxes at each of the grid locations in the featuremap. They make the model learn the grid offset in x,y direction like before. Unlike the previous version they now make the model to learn the log scale w,h offset in a from the prior box size.</span></p><p class="c5"><span class="c0">2. They used k-means clustering over all the images of the dataset for finding the most optimal k number of anchor boxes for different values of k. They found 5 anchor boxes were working best. Using three anchor boxes from 3 scales is also a good practice nowadays.</span></p><p class="c5"><span class="c0">3. They found batch normalization very effective.</span></p><p class="c5"><span class="c0">4. They used higher resolution images this time (416x416). They also trained the model with different resolution images &nbsp;during the training for making it robust to image size.</span></p><p class="c5"><span class="c0">5. They adopted an identity mapping approach similar to resnet. They copied the output of an older layer and added it to the activations of deeper layers.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 358.50px; height: 266.97px;"><img alt="" src="files/yolo/image8.png" style="width: 358.50px; height: 266.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c10">&nbsp; &nbsp; &nbsp; &nbsp;Image taken from YOLO v2 paper by Redmon at el. &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span>b</span><span class="c2">x</span><span>&nbsp;= sig(t</span><span class="c2">x</span><span>) + c</span><span class="c7 c2">x</span></p><p class="c5"><span>b</span><span class="c2">y</span><span>&nbsp;= sig(t</span><span class="c2">y</span><span>) +c</span><span class="c7 c2">y</span></p><p class="c5"><span>b</span><span class="c2">w</span><span>&nbsp;= p</span><span class="c2">w</span><span>e</span><span class="c7 c3">tw</span></p><p class="c5"><span>b</span><span class="c2">h</span><span>&nbsp;= p</span><span class="c3">h</span><span>e</span><span class="c3 c7">th</span></p><p class="c1"><span class="c7 c3"></span></p><p class="c5"><span class="c0">&nbsp;For x,y the model is learning the difference between b and c which is the grid offset. (To be precise the model is learning the inverse sigmoid of the grid offset.) For w and h, the model is learning the difference of logarithmic values of each of them.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span>t</span><span class="c2">x</span><span>&nbsp;= inv_sig( b</span><span class="c2">x</span><span>&nbsp;- c</span><span class="c2">x</span><span class="c0">)</span></p><p class="c5"><span>t</span><span class="c2">y</span><span>&nbsp;= inv_sig((b</span><span class="c2">y</span><span>&nbsp;- c</span><span class="c2">y</span><span class="c0">)</span></p><p class="c5"><span>t</span><span class="c2">w</span><span>&nbsp;= log(b</span><span class="c2">w</span><span>/p</span><span class="c2">w</span><span>) = log(b</span><span class="c2">w</span><span>) - log(p</span><span class="c2">w</span><span class="c0">)</span></p><p class="c5"><span>t</span><span class="c2">h</span><span>&nbsp;= &nbsp;log(b</span><span class="c2">h</span><span>/p</span><span class="c2">h</span><span>) = log(b</span><span class="c2">w</span><span>) - log(p</span><span class="c2">w</span><span class="c0">)</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">Selective backpropagation : </span><span class="c0">During training if an bbox has low overlap with the ground truth then back propagation can be skipped for that particular loss.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">There are some post prediction techniques that are commonly practiced for better accuracy.</span></p><p class="c5"><span class="c15">Removing low confidence boxes boxes :</span><span class="c0">&nbsp;After prediction is done bboxes lower a minimum (i.e 0.3) confidence, P(Class|Object) are filtered out for better accuracy.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">Non Maximum Suppression :</span><span class="c0">&nbsp; If some of the prediction boxes try to predict the same object with different confidence they should have a good overlap. For prediction boxes with IoU more than a threshold (i.e 0.5) only the one with highest confidence is kept.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c15">Word Tree :</span><span class="c0">&nbsp;YOLO v2 introduced a way for incorporating large classification data sets (without bbox labels i.e ImageNet 1000) to be introduced within the framework. During training if an image samples from a classification dataset then only classification loss is calculated else it will calculate all the losses. For unifying the 1000 classes of imageNet to COCO 80 classes the each of the fine grained categories (i.e persian cat) is mapped back to its parent category present in COCO dataset (i.e cat). They have accomplished it by using the concept from Word Net from the Natural language processing domain. </span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 730.85px; height: 428.17px;"><img alt="" src="files/yolo/image9.png" style="width: 730.85px; height: 428.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c10">Figure : The core idea of YOLO is to find a vector like above at every pixel location in the feature map (not image).</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">Yolo Version 3</span></p><hr><p class="c1"><span class="c4"></span></p><p class="c5"><span class="c0">- The main improvement in yolo v3 is extracting data with three different scales from the feature map. They were inspired by FPN networks for this. They have taken the feature maps from the last layer for small box prediction. Then they upsmples it by doubling the size and concatenated it with the feature maps from 2 layers back and predicted medium sized boxes. They have repeated the same and predicted large boxes from feature maps from 2 more laye back.</span></p><p class="c5"><span class="c0">- They have used 9 anchor boxes (instead of 5) , sorted them by size and divided them into three scales: large , medium and small. Each of the 3 anchor boxes are applied on the corresponding feature maps extracted from the network.</span></p><p class="c5"><span class="c0">- For confidence loss they used logistic losses instead of mean squared error. Although, it is a very common practice nowadays.</span></p><p class="c5"><span class="c0">- Softmax needs the class labels to be mutually exclusive. Each grid cell can occupy multiple objects from different classes at different scales. Like one person standing behind a car having their center at the same location in the image. They replaced the idea of using the softmax for each grid cell location </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">I will show an example dimension through the figure for understanding it better considering the mobile net v2</span></p><p class="c5"><span class="c0">as a backbone with input image shape 224x224 for other networks or input size it can be different.</span></p><p class="c5"><span class="c0">I will demonstrate with 80 classes considering the coco dataset. </span></p><p class="c5"><span class="c0">The authors used image size ~ 416x416 with darknet 53</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 770.92px; height: 552.59px;"><img alt="" src="files/yolo/image6.png" style="width: 770.92px; height: 552.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c10 c11">Figure : YOLO V3</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c8">1x1 conv + Reshape</span></p><p class="c5"><span class="c0">1x1 convolution layer can change the depth of a feature map array keeping the height and width as it is. Here it is used to change the depth of the output featuremap &nbsp;to appropriate dimension so that it can be reshaped to our desired dimension. For this purpose the 1x1 conv should change any arbitrary depth to a length equals to anc box* (4+1+no of class). Height and width remains the same after applying 1x1 convolution. Then reshape is applied to make and extra dimension of size=3 (anc. box). Using a separate dimension for the anchor box is convenient for implementation. Numerical operations in many popular libraries can easily be applied on a dimension of an array. </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c8">Decode outputs to bboxes :</span></p><p class="c5"><span class="c0">1. Decode the bboxes and class from the output 4d array using respective anchor boxes for that feature map scale. Note that the same anchor boxes</span></p><p class="c5"><span class="c0">are also required for loss calculation.</span></p><p class="c5"><span class="c0">2. Low conf. bbox removal</span></p><p class="c5"><span class="c0">3. Non maximum suppression</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 816.50px; height: 357.62px;"><img alt="" src="files/yolo/image5.png" style="width: 816.50px; height: 357.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c10">Figure : &nbsp;(Left) Non Maximum Suppression (Middle) Total 9 anchor boxes at every fmap locations (Right) IoU and multi aspect ratio objects at one location. </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c8">Code :</span></p><p class="c5"><span>Yolo V4 : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4&amp;sa=D&amp;source=editors&amp;ust=1665939942086594&amp;usg=AOvVaw3t2hEF_Urr9QR6HXOW1GiN">https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4</a></span></p><p class="c1"><span class="c8"></span></p><p class="c5"><span class="c8">References </span></p><hr><p class="c1"><span class="c8"></span></p><p class="c5"><span class="c0">Papers :</span></p><p class="c5"><span>1. Faster R-CNN: </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1665939942087307&amp;usg=AOvVaw2Bp2l_Si9a-th6rQ6rxiiZ">https://arxiv.org/abs/1506.01497</a></span></p><p class="c5"><span>2. You Only Look Once: Unified, Real-Time Object Detection : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640.pdf&amp;sa=D&amp;source=editors&amp;ust=1665939942087559&amp;usg=AOvVaw0mdXKq0jRq0YLVFSZd1d2_">https://arxiv.org/pdf/1506.02640.pdf</a></span></p><p class="c5"><span>3. SSD: Single Shot MultiBox Detector : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.02325&amp;sa=D&amp;source=editors&amp;ust=1665939942087866&amp;usg=AOvVaw1NeQklBDNy48H_ZEOeaxR7">https://arxiv.org/abs/1512.02325</a></span></p><p class="c5"><span>4. YOLO9000: Better, Faster, Stronger : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1612.08242&amp;sa=D&amp;source=editors&amp;ust=1665939942088099&amp;usg=AOvVaw0hdjwL9_IunXsR3By88qq1">https://arxiv.org/abs/1612.08242</a></span></p><p class="c5"><span>5. YOLOv3: An Incremental Improvement : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1804.02767?e05802c1_page%3D1&amp;sa=D&amp;source=editors&amp;ust=1665939942088404&amp;usg=AOvVaw07o0pTiUZ72H5aDNrBt4sg">https://arxiv.org/abs/1804.02767?e05802c1_page=1</a></span></p><p class="c5"><span>6. Feature Pyramid Networks for Object Detection : </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/pdf/1612.03144.pdf&amp;sa=D&amp;source=editors&amp;ust=1665939942088738&amp;usg=AOvVaw1h_GLfSOWEsEJ8BFAaPH-T">https://arxiv.org/pdf/1612.03144.pdf</a></span></p><p class="c5"><span>7. Mask R-CNN: </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1665939942088994&amp;usg=AOvVaw00Wb0hoMRNalEq2fXlftRx">https://arxiv.org/abs/1703.06870</a></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Blogs :</span></p><p class="c5"><span>1. </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/&amp;sa=D&amp;source=editors&amp;ust=1665939942089497&amp;usg=AOvVaw1znkAsJ920X70LqliZdRy_">https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</a></span></p><p class="c5"><span>2. </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;sa=D&amp;source=editors&amp;ust=1665939942089806&amp;usg=AOvVaw2Y6JB1_KM0-Woo8LEhWLkW">https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span></p><p class="c5"><span>3. </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a&amp;sa=D&amp;source=editors&amp;ust=1665939942090088&amp;usg=AOvVaw1EZpA2rAygReJXHilZy3Cc">https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a</a></span></p><p class="c5"><span>4. </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06&amp;sa=D&amp;source=editors&amp;ust=1665939942090382&amp;usg=AOvVaw32clUNChsOCmkmgyRSI_wt">https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></span></p><p class="c5"><span>5. </span><span class="c9"><a class="c6" href="https://www.google.com/url?q=https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b%23:~:text%3DSSD%2520contains%25208732%2520default%2520boxes,to%2520obtain%2520the%2520final%2520prediction.&amp;sa=D&amp;source=editors&amp;ust=1665939942090771&amp;usg=AOvVaw06ZiRoWJpBxsGPLpqLwokg">https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b#:~:text=SSD%20contains%208732%20default%20boxes,to%20obtain%20the%20final%20prediction.</a></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p></body></html>