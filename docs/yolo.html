<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_8s7n3kf664eg-8{list-style-type:none}ul.lst-kix_8s7n3kf664eg-7{list-style-type:none}ol.lst-kix_f9kzvvr54xty-7.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-7 0}ol.lst-kix_f9kzvvr54xty-2{list-style-type:none}ol.lst-kix_f9kzvvr54xty-1{list-style-type:none}ol.lst-kix_f9kzvvr54xty-4{list-style-type:none}ol.lst-kix_f9kzvvr54xty-3{list-style-type:none}ol.lst-kix_f9kzvvr54xty-6{list-style-type:none}ol.lst-kix_f9kzvvr54xty-5{list-style-type:none}ol.lst-kix_f9kzvvr54xty-8{list-style-type:none}ol.lst-kix_f9kzvvr54xty-7{list-style-type:none}ol.lst-kix_f9kzvvr54xty-0{list-style-type:none}.lst-kix_f9kzvvr54xty-7>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-7}.lst-kix_4gvvutwnlpi-1>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-0>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-5>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-4>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-6>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-8>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-8,lower-roman) ". "}.lst-kix_f9kzvvr54xty-6>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-6}.lst-kix_f9kzvvr54xty-7>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-7,lower-latin) ". "}ol.lst-kix_f9kzvvr54xty-1.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-1 0}ol.lst-kix_f9kzvvr54xty-8.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-8 0}.lst-kix_4gvvutwnlpi-2>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-3>li:before{content:"-  "}ul.lst-kix_1rpjbd1o2ztz-8{list-style-type:none}.lst-kix_f9kzvvr54xty-0>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-0}ul.lst-kix_1rpjbd1o2ztz-5{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-4{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-7{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-6{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-1{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-0{list-style-type:none}ul.lst-kix_1rpjbd1o2ztz-3{list-style-type:none}.lst-kix_p7e7tkpe56mn-8>li:before{content:"-  "}ul.lst-kix_1rpjbd1o2ztz-2{list-style-type:none}.lst-kix_p7e7tkpe56mn-7>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-8>li:before{content:"-  "}.lst-kix_4gvvutwnlpi-7>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-5>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-6>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-8>li:before{content:"-  "}ol.lst-kix_f9kzvvr54xty-2.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-2 0}.lst-kix_mnrrbqvs6ca-6>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-5>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-7>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-0>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-2>li:before{content:"-  "}.lst-kix_1rpjbd1o2ztz-6>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-1>li:before{content:"-  "}.lst-kix_1rpjbd1o2ztz-5>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-4>li:before{content:"-  "}.lst-kix_1rpjbd1o2ztz-3>li:before{content:"-  "}.lst-kix_1rpjbd1o2ztz-4>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-3>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-2{list-style-type:none}.lst-kix_1rpjbd1o2ztz-1>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-1{list-style-type:none}.lst-kix_y89d79xx04k4-0>li:before{content:"-  "}.lst-kix_y89d79xx04k4-1>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-4{list-style-type:none}ul.lst-kix_4gvvutwnlpi-3{list-style-type:none}.lst-kix_y89d79xx04k4-2>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-0{list-style-type:none}.lst-kix_1rpjbd1o2ztz-2>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-3>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-3}ul.lst-kix_4gvvutwnlpi-6{list-style-type:none}.lst-kix_1rpjbd1o2ztz-7>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-5{list-style-type:none}.lst-kix_y89d79xx04k4-7>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-8{list-style-type:none}.lst-kix_1rpjbd1o2ztz-0>li:before{content:"-  "}.lst-kix_1rpjbd1o2ztz-8>li:before{content:"-  "}ul.lst-kix_4gvvutwnlpi-7{list-style-type:none}.lst-kix_y89d79xx04k4-6>li:before{content:"-  "}.lst-kix_y89d79xx04k4-4>li:before{content:"-  "}.lst-kix_y89d79xx04k4-5>li:before{content:"-  "}.lst-kix_y89d79xx04k4-3>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-3>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-8{list-style-type:none}ul.lst-kix_10bg29dgyeg1-7{list-style-type:none}.lst-kix_p7e7tkpe56mn-0>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-4>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-6{list-style-type:none}ul.lst-kix_10bg29dgyeg1-5{list-style-type:none}ul.lst-kix_10bg29dgyeg1-4{list-style-type:none}ol.lst-kix_f9kzvvr54xty-0.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-0 0}ul.lst-kix_10bg29dgyeg1-3{list-style-type:none}ul.lst-kix_10bg29dgyeg1-2{list-style-type:none}ul.lst-kix_10bg29dgyeg1-1{list-style-type:none}.lst-kix_p7e7tkpe56mn-1>li:before{content:"-  "}ul.lst-kix_10bg29dgyeg1-0{list-style-type:none}.lst-kix_p7e7tkpe56mn-2>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-4>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-4}.lst-kix_f9kzvvr54xty-2>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-2,lower-roman) ". "}.lst-kix_f9kzvvr54xty-1>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-1}.lst-kix_8s7n3kf664eg-0>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-2>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-3>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-3,decimal) ". "}.lst-kix_8s7n3kf664eg-1>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-5>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-5,lower-roman) ". "}ol.lst-kix_f9kzvvr54xty-6.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-6 0}.lst-kix_f9kzvvr54xty-4>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-4,lower-latin) ". "}.lst-kix_f9kzvvr54xty-6>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-6,decimal) ". "}ol.lst-kix_f9kzvvr54xty-3.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-3 0}.lst-kix_8s7n3kf664eg-8>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-4>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-0>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-3>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-5>li:before{content:"-  "}ul.lst-kix_bk5el08vcmqt-0{list-style-type:none}.lst-kix_10bg29dgyeg1-2>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-6>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-1>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-2>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-5>li:before{content:"-  "}ul.lst-kix_bk5el08vcmqt-4{list-style-type:none}ul.lst-kix_y89d79xx04k4-5{list-style-type:none}ul.lst-kix_bk5el08vcmqt-3{list-style-type:none}ul.lst-kix_y89d79xx04k4-4{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-8{list-style-type:none}ul.lst-kix_bk5el08vcmqt-2{list-style-type:none}ul.lst-kix_y89d79xx04k4-7{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-7{list-style-type:none}ul.lst-kix_bk5el08vcmqt-1{list-style-type:none}.lst-kix_8s7n3kf664eg-4>li:before{content:"-  "}ul.lst-kix_y89d79xx04k4-6{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-6{list-style-type:none}ul.lst-kix_bk5el08vcmqt-8{list-style-type:none}ul.lst-kix_y89d79xx04k4-1{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-5{list-style-type:none}ul.lst-kix_bk5el08vcmqt-7{list-style-type:none}ul.lst-kix_y89d79xx04k4-0{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-4{list-style-type:none}ul.lst-kix_bk5el08vcmqt-6{list-style-type:none}ul.lst-kix_y89d79xx04k4-3{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-3{list-style-type:none}ul.lst-kix_bk5el08vcmqt-5{list-style-type:none}.lst-kix_8s7n3kf664eg-3>li:before{content:"-  "}ul.lst-kix_y89d79xx04k4-2{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-2{list-style-type:none}.lst-kix_f9kzvvr54xty-1>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-1,lower-latin) ". "}ul.lst-kix_mnrrbqvs6ca-1{list-style-type:none}.lst-kix_bk5el08vcmqt-8>li:before{content:"-  "}ul.lst-kix_mnrrbqvs6ca-0{list-style-type:none}.lst-kix_f9kzvvr54xty-0>li:before{content:"" counter(lst-ctn-kix_f9kzvvr54xty-0,decimal) ". "}ol.lst-kix_f9kzvvr54xty-5.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-5 0}ol.lst-kix_f9kzvvr54xty-4.start{counter-reset:lst-ctn-kix_f9kzvvr54xty-4 0}.lst-kix_10bg29dgyeg1-0>li:before{content:"-  "}.lst-kix_10bg29dgyeg1-1>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-6>li:before{content:"-  "}.lst-kix_8s7n3kf664eg-7>li:before{content:"-  "}.lst-kix_f9kzvvr54xty-2>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-2}.lst-kix_f9kzvvr54xty-8>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-8}.lst-kix_f9kzvvr54xty-5>li{counter-increment:lst-ctn-kix_f9kzvvr54xty-5}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_bk5el08vcmqt-7>li:before{content:"-  "}.lst-kix_y89d79xx04k4-8>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-8{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-7{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-6{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-5{list-style-type:none}.lst-kix_bk5el08vcmqt-5>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-6>li:before{content:"-  "}ul.lst-kix_y89d79xx04k4-8{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-4{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-3{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-2{list-style-type:none}ul.lst-kix_8s7n3kf664eg-4{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-1{list-style-type:none}.lst-kix_10bg29dgyeg1-8>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-3>li:before{content:"-  "}.lst-kix_bk5el08vcmqt-4>li:before{content:"-  "}ul.lst-kix_8s7n3kf664eg-3{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-0{list-style-type:none}ul.lst-kix_8s7n3kf664eg-6{list-style-type:none}.lst-kix_10bg29dgyeg1-7>li:before{content:"-  "}ul.lst-kix_8s7n3kf664eg-5{list-style-type:none}ul.lst-kix_8s7n3kf664eg-0{list-style-type:none}ul.lst-kix_8s7n3kf664eg-2{list-style-type:none}ul.lst-kix_8s7n3kf664eg-1{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c35{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:110.2pt;border-top-color:#000000;border-bottom-style:solid}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117.8pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:138pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:118.5pt;border-top-color:#000000;border-bottom-style:solid}.c4{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:114.8pt;border-top-color:#000000;border-bottom-style:solid}.c18{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:120.8pt;border-top-color:#000000;border-bottom-style:solid}.c33{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:463.5pt;border-top-color:#000000;border-bottom-style:solid}.c34{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c36{color:#ff9900;text-decoration:none;vertical-align:baseline;font-style:normal}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c30{color:#666666;text-decoration:none;vertical-align:baseline;font-size:14pt}.c13{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c23{color:#666666;text-decoration:none;vertical-align:baseline;font-size:9pt}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c28{color:#000000;text-decoration:none;font-size:11pt;font-style:normal}.c39{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c31{color:#666666;text-decoration:none;vertical-align:baseline}.c9{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c41{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c37{color:#000000;text-decoration:none;vertical-align:baseline}.c21{font-weight:700;font-size:11pt;font-family:"Times New Roman"}.c14{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c20{font-weight:700;font-family:"Times New Roman";font-style:italic}.c24{padding:0;margin:0}.c10{font-weight:400;font-family:"Times New Roman"}.c29{font-weight:700;font-family:"Times New Roman"}.c8{color:inherit;text-decoration:inherit}.c27{font-style:italic}.c16{vertical-align:sub}.c40{font-size:14pt}.c15{vertical-align:super}.c25{height:0pt}.c38{margin-left:36pt}.c42{font-size:16pt}.c17{font-size:10pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c41 doc-content"><p class="c32"><span class="c20 c40">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c37 c20 c42">Object Detection : Single Shot Detectors (YOLO V1-V3 , SSD)</span></p><hr><p class="c0"><span class="c37 c20 c40"></span></p><p class="c3"><span class="c23 c20">November 06, 2022</span></p><p class="c0"><span class="c20 c23"></span></p><p class="c3"><span class="c37 c21 c27">Table of Content </span></p><ol class="c24 lst-kix_f9kzvvr54xty-0 start" start="1"><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.wtk9tk94ulk2">RCNN Recap </a></span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.6q4hdx4cue7u">YOLO Version 1</a></span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.oq0cudpdg8ir">SSD (Single shot multi box detector.)</a></span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.fsu19pnk5cpt">YOLO Version 2</a></span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.fq0jql663kbl">Yolo Version 3</a></span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.gsju1bji6owf">Symbols and Short forms</a></span><span class="c21 c27 c37">&nbsp;</span></li><li class="c1 li-bullet-0"><span class="c13 c20"><a class="c8" href="#id.hj7rbnumf70e">Code </a></span><span class="c20">, </span><span class="c13 c20"><a class="c8" href="#id.1a2ii16xnli">References</a></span></li></ol><p class="c0 c38"><span class="c37 c21 c27"></span></p><a id="id.wtk9tk94ulk2"></a><p class="c3"><span class="c14 c27">RCNN Recap :</span></p><p class="c3"><span class="c9">Among the RCNN Family of algorithms discussed in the </span><span class="c13 c9"><a class="c8" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667733635253234&amp;usg=AOvVaw1aZB-XWo0F4n-y3H6sBmaa">previous article</a></span><span class="c9">, Faster RCNN (2015) and Mask RCNN (2017) are the fastest and most accurate. After Faster RCNN &nbsp;YOLO v1 and then SSD was published and started to get popular in the object detection community. Both of them are single shot detectors; it means they take one RGB image as input and predict the bboxes and class (bounding boxes) by passing the image data only once through different sections of the model. These three algorithms are nearly contemporary and share many relevant upgrades. &nbsp;I will start this section with YOLO-v1 and SSD, then will cover YOLO-v2 and YOLO-v3 as well. For details about Mask RCNN or Faster RCNN please refer to the blog on </span><span class="c13 c9"><a class="c8" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667733635253873&amp;usg=AOvVaw2r_d_3q31Uk2YwkioDYlok">RCNN &nbsp;Family</a></span><span class="c11 c9">. </span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; Before we start, let&rsquo;s remember a little about Faster RCNN. &nbsp;One major drawback of Faster RCNN is that it is still quite slow(~ 7 FPS), consequently fails to support many of the real time applications which require faster prediction. It was able to achieve the region proposals and Fmap RoIs by one pass through a network by sharing most of the convolutional layers. The major bottleneck &nbsp;of Faster RCNN is that it still requires one by one passing of the feature maps through the classifier, possibly it is one of the key factors that helps the algorithm to reach its high level of &nbsp;accuracy. YOLO tried to make a tradeoff between speed and accuracy. It replaced the one by one Fmap processing with an end to end manner. The bbox regressor is a tensor having the width and height of the feature map and a depth of 4k + 2k , k = number of anchor bbox ; 4 -&gt; {x,y,w,z} ; 2-&gt; {object , no object confidence}. So at every feature map location it is able to predict k no of box coordinates and confidence. Unlike Faster RCNN it only learns objectness confidence not background confidence.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c9">&nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 746.50px; height: 398.37px;"><img alt="" src="files/yolo/image6.png" style="width: 746.50px; height: 398.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c31 c21 c27">Figure : Faster RCNN </span></p><p class="c0"><span class="c31 c21 c27"></span></p><p class="c0"><span class="c6"></span></p><a id="id.6q4hdx4cue7u"></a><p class="c3"><span class="c37 c20 c40">YOLO Version 1 </span></p><hr><p class="c0"><span class="c37 c20 c40"></span></p><p class="c3"><span class="c9">YOLO v1 stacked the two different branches of RPN together, which were for 4k bbox coordinates and 2k confidence respectively. So across the depth it has one additional channel for class agnostic box confidence. It also stacked extra N channels at the end of this output tensor dedicated for each class. They can predict the probability of each of the object classes per feature map location. It eliminates the necessity of additional &nbsp;classifiers layers and effectively makes it a single shot detector.</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c12">Improvements :</span></p><p class="c3"><span class="c14">1. Joining the Region Proposal Generator and Classifier together &nbsp;:</span><span class="c11 c9">&nbsp;For YOLO the RPN tensor , confidence tensor (objectnes confidence only), the classifier tensors are all stacked together. Consequently, the outputs of the RPN are required to be dealt separately for projecting on top of the feature map. </span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c14">2. Eliminating RoI Align :</span><span class="c11 c9">&nbsp;</span></p><p class="c3"><span class="c11 c9">[i] Faster RCNN predicts the Region Proposal bbox wrt the original image size so they need to be scaled down for projecting on the featuremap. YOLO directly predicts the region proposals on top of the feature map so no need to scale down and projection anymore. </span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c9">[ii] Faster RCNN requires the Fmap RoI to be of fixed size so that they can be fed to the classifier layers but here the classification is done parallely for every region proposal at every feature map location. No need to input the feature maps to the classifier layers again.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c14">3. Eliminated One by One Fmap RoI classification :</span><span class="c11 c9">&nbsp;It is done more efficiently by jointly predicting the bboxes and class probabilities (for every object) at every featuremap location.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c12">Draw backs :</span></p><p class="c3"><span class="c11 c9">It does not use an anchor box in v1. Eventually v2 adopted it. </span></p><p class="c0"><span class="c11 c9"></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c12">How it works : </span></p><p class="c3"><span class="c14">&nbsp; &nbsp; &nbsp; </span><span class="c11 c9">In the context of deep convolutional neural networks the output from every CNN stage is a 3D array which we are calling feature maps. It represents distinct high level information across the channels for each of the spatial pixel locations. &nbsp;Usually, the first two dimensions are width and height gradually shrinks down from the original image while the depth gets increased in parallel. It enables the deeper layers to represent more complex image features by each of its depth channels depthwise.</span></p><p class="c3"><span class="c14">&nbsp; &nbsp; &nbsp; </span><span class="c11 c9">The YOLO model takes an image and outputs an array of shape 7x7xN. Each of the channels of the output tensor represent - &ldquo;x,y,w,h,objectness confidence, class 1 confidence, class 2 confidence, ........., class n confidence&rdquo; respectively at every 7x7 grid location. The following example will make it more clear. The red circle on the grid (figure below) represents the 1x2 location in the featuremap. So the numeric value at (grid_x,grid_y,chanel) will represent the following information -</span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; &nbsp; </span></p><a id="t.8d740092d50839d677c0c827cdd6281d9c117a30"></a><a id="t.0"></a><table class="c39"><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c5"><span class="c11 c9">(Grid X, Grid Y, Channel)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c5"><span class="c11 c9">Information</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x0)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">x ; If there is a object at this location, &nbsp;&#39;x&#39; would be the offset of its location along the height</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x1)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">y; &nbsp;similarly &#39;y&#39; would be its offset along width.</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x2)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">w; &nbsp;&#39;w&#39; is the width of the object bbox</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c9 c11">(1x2x3)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">h; &nbsp; &#39;h&#39; is the height of the bbox.</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x4)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">probability of presence of any class object at that location.</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x5)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">probability of the presence of a &#39;class 1&#39; object at that location</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">&nbsp;&hellip;</span></p><p class="c3"><span class="c11 c9">&nbsp;&hellip;</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c5"><span class="c11 c9">&hellip;</span></p><p class="c5"><span class="c11 c9">&hellip;</span></p></td></tr><tr class="c25"><td class="c22" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">(1x2x n+5)</span></p></td><td class="c33" colspan="1" rowspan="1"><p class="c3"><span class="c11 c9">probability of the presence of a &#39;class n&#39; object at that location </span></p></td></tr></table><p class="c0"><span class="c11 c9"></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c9">They designed the loss function in such a way that the model only learns the x,y offsets from the grid corner, not the absolute pixel coordinate of the object in the image (See x,y values in the enlarged figure in the bottom-right side). The absolute coordinate is derived from the grid location of the feature map. Its value is restricted from 0-1 by applying sigmoid activation function, so that it does not cross the current cell boundary. If it needed to learn the absolute coordinate the value of x,y needed to cover the entire image width and height. Learning an object&#39;s location in the entire image precisely is a harder task then just learning its location within the grid.</span></p><p class="c3"><span class="c11 c9">Now x,y covers a small portion of the region but for w, h there is no restriction for staying within the grid. So w,h loss can get very high sometimes for big boxes. It can make the training unstable for all the losses since they are added together for gradient calculation. For handling this issue they use the difference between the square root of w and h instead of their actual value (Just to remind Faster RCNN was using log scale here). See the loss function below for better understanding.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 860.42px; height: 473.90px;"><img alt="" src="files/yolo/image7.png" style="width: 860.42px; height: 473.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c7">Figure : Yolo V1</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c14 c27">The Algorithm </span></p><p class="c3"><span class="c11 c9">&nbsp;One thing is avoided intentionally for simplicity. Yolo v1 uses two boxes at each pixel location. So there would be two sets of x,y,w,h like -&gt; x1,y1,w1,h1 , x2,y2,w2,h2. &nbsp;Resulting the depth as follows - 2 (two boxes) * 4 + 1+ no of class. &nbsp;For VOC dataset they used if no of box = 2 and class = 20 having a depth of 2*4 + 1+ 20 = 30 resulting in the output tensor with shape 7x7x30.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c12">Training Steps </span></p><p class="c3"><span class="c11 c9">1. Preparing GT tensor : It is a good practice to prepare the mini-batch generator functions for training to output the same data format of the YOLO v1 model output. It should be of size (7x7xN) N = &nbsp;4 + 1 + no of classes. Now the loss can be calculated quite easily by comparing with the model output and train data since they have the same shape.</span></p><p class="c3"><span class="c11 c9">2. Forward pass : Feed the RGB image to the model and extract the predicted tensor with shape 7x7xN</span></p><p class="c3"><span class="c11 c9">3. xy_loss : slice the first 2 channels of the ground truth and prediction tensors both having shape 7x7x2 and calculate the squared difference.</span></p><p class="c3"><span class="c11 c9">In numpy syntax it would be like this -<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &gt;&gt; xy_gt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = gt_tensor[ : , : , :2 ] </span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&gt;&gt; xy_pred &nbsp; &nbsp; &nbsp; &nbsp; = predicted_tensor[: , : , :2]</span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&gt;&gt; xy_loss &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= numpy.mean (numpy.square(xy_gt - xy_pred))</span></p><p class="c3"><span class="c11 c9">4. wh_loss : slice out the 3rd and 4th (tensor[:, :, 2:4]) channel and take their square root and calculate squared difference for w,h error.</span></p><p class="c3"><span class="c11 c9">5. conf_loss :</span></p><p class="c3"><span class="c9">&nbsp; &nbsp; &nbsp; - slice the 5th channel (tensor[:, :, 5]) and apply an object mask [1</span><span class="c9 c15">obj</span><span class="c11 c9">&nbsp;] on every cell. It means Multiply the locations with an object in GT with 1 else 0. Finally calculate mse among the masked </span></p><p class="c3"><span class="c11 c9">tensores. </span></p><p class="c3"><span class="c9">&nbsp; &nbsp; &nbsp;- calculate the loss again this time applying no object mask [1</span><span class="c9 c15">nobj</span><span class="c9">] on every cell. Add this loss with a smaller weight [&#x1d6cc;</span><span class="c9 c16">noobj &nbsp;</span><span class="c11 c9">] because there will be lot of box with no object compared to object boxes in the GT.</span></p><p class="c3"><span class="c11 c9">6.Class confidence loss : </span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; &nbsp; - Slice rest of the channels (tensor[:, :, 5: ]) , convert the raw values to conditional probabilities P(class | object) by multiplying with object confidence at the same location. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c3"><span class="c11 c9">&nbsp; &nbsp; &nbsp; - Then calculate the classification loss in the same way as confidence loss. Note currently softmax and cross entropy loss is used at this place in the SOTA methods.</span></p><p class="c3"><span class="c11 c9">7. Finally, backpropagation is done with the calculated loss.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.50px; height: 296.24px;"><img alt="" src="files/yolo/image3.png" style="width: 439.50px; height: 296.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c12">Inference Steps :</span></p><p class="c3"><span class="c11 c9">For drawing the bbox in the original image we need to project the predicted x,y,w,h to the respective locations of the actual image.</span></p><p class="c3"><span class="c11 c9">&nbsp;- If the Image size = (416x416) and model output size = (7x7), the ratio of image and fmap is, &nbsp;grid-scale =416/7 = 13</span></p><p class="c3"><span class="c11 c9">&nbsp;- Each of the 7x7 grids can be projected back to its parent location in the image by multiplying by grid-scale = 13.</span></p><p class="c3"><span class="c11 c9">&nbsp;- Let&#39;s consider the following values for x,y,w,h = [0.3,0.2,4,5], in the above example at grid location [grid_x,grid_y] = [1,2].</span></p><p class="c3"><span class="c11 c9">X = grid-scale * (grid_x + x) = 13 * (1 + 0.3) = &nbsp;16.9 ~ 17</span></p><p class="c3"><span class="c11 c9">Y = grid-scale * (grid_y + y) = 13 * (2 + 0.2) = &nbsp;28.6 ~ 29</span></p><p class="c3"><span class="c11 c9">W = grid-scale * w &nbsp;= 4 &nbsp;* 13 = 52</span></p><p class="c3"><span class="c11 c9">H &nbsp;= grid-scale * h &nbsp; = 5 &nbsp;* 13 &nbsp;= 65</span></p><p class="c3"><span class="c11 c9">The reverse would be like below,</span></p><p class="c3"><span class="c11 c9">x = X / grid-scale - grid_x </span></p><p class="c3"><span class="c11 c9">y = Y / grid-scale - grid_y </span></p><p class="c3"><span class="c11 c9">w = W / grid-scale</span></p><p class="c3"><span class="c11 c9">h = H /grid-scale</span></p><p class="c0"><span class="c11 c9"></span></p><a id="id.oq0cudpdg8ir"></a><p class="c3"><span class="c20 c40">SSD &nbsp;(</span><span class="c14">Single shot multi box detector.</span><span class="c37 c20 c40">)</span></p><hr><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c9">Unlike Faster RCNN both YOLO v1 and SSD eliminate the necessity of putting the RoI Feature maps one by one through the dense classifier layers. They are both single pass / single shot detectors. One image passes through the end to end network and predicts the bboxes along with class labels in one pass. </span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c9">SSD is better than YOLO v1 in the following ways :</span></p><p class="c3"><span class="c14">1. Multi scale feature maps : </span><span class="c9">The feature maps with higher resolution would be able to predict the small objects better while smaller ones should take care of the large objects. It uses the different resolution feature maps from different stages of the model for final prediction. It helps the model to predict objects of different sizes more easily. &nbsp;The feature maps would have varying depths while for prediction we need fixed depth consisting of i.e x,y,w,h ,conf etc. They pass the arbitrary depth feature maps from different stages through a 3x3 </span><span class="c9">conv</span><span class="c9">. (convolutional) layer with depth k*(classes+4). From the outputs of the </span><span class="c9">conv</span><span class="c11 c9">. layer they calculate the boxes like YOLO V1. For mxn featuremap with k aspect ratio boxes it can predict k boxes at each of the m*n location resulting boxes = k*m*n. </span></p><p class="c3"><span class="c11 c9">SSD predicts a total of 8732 boxes of different scaled feature maps.</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c21">2. Default boxes :</span></p><p class="c3"><span class="c10">For each of the feature map scales they used the default box. It is similar to the anchor box in RCNN, see the anchor box section in &nbsp;my </span><span class="c13 c9"><a class="c8" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1667733635270022&amp;usg=AOvVaw3mTLtAHqCRzy73_JJsrb6k">RCNN &nbsp;Family</a></span><span class="c9">&nbsp;</span><span class="c10">&nbsp;blog for better understanding of this part. Unlike Faster RCNN they used 9 boxes for each feature map scale with aspect ratios &nbsp;A</span><span class="c10 c16">r</span><span class="c10">&nbsp;= {1,2,3,1/2,1/3}. They scaled the anchor boxes at different feature map levels with a scale factor which ensures the feature map stage emphasizes on learning the intended scales. In Faster RCNN they used three scales 128x128,256x256,512x512 each with 3 aspect ratio 1:1,1:2,2:1. In SSD each of the feature maps from each stage of the network corresponds to one scale, resulting in a total 6 scales. Each of the scales have 4,6,6,6,4,4 no of aspect ratios. The 1st stage of feature map (38x38) which can look at the small objects at higher resolution so they have not assigned the top 2 largest aspect ratios for it i.e A</span><span class="c10 c16">r</span><span class="c6">&nbsp;= {1,2,3,1/2,1/3}. For similar reasons the smallest 2 scales don&rsquo;t care about the large aspect ratio. For example scale 38x38 should learn small objects so they scaled it with small scale factor = 0.2. Scale, s = 0.2 and s = 0.9 all the intermediate feature maps are equally mapped. The small scale (0.2) for the 38x38 pushes it to find small boxes while the 1x1 map with scale 0.9 will try to find very big boxes at the center of the image.</span></p><p class="c3"><span class="c6">Width and height is calculated from scale and aspect ratio as follows - width, &nbsp;w = s * sqrt(A) ; height, h = s/sqrt(A)</span></p><p class="c0"><span class="c6"></span></p><ul class="c24 lst-kix_4gvvutwnlpi-0 start"><li class="c1 li-bullet-0"><span class="c6">l = 1 , 2, &nbsp;&hellip;., L</span></li><li class="c1 li-bullet-0"><span class="c10">s = s</span><span class="c10 c16">min</span><span class="c10">&nbsp;+[ (s</span><span class="c10 c16">max</span><span class="c10">&nbsp;- s</span><span class="c10 c16">min</span><span class="c6">) * (l - 1) ] / (L - 1) = 0.20 , 0.34, 0.48, 0.62, 0.78, 0.90</span></li><li class="c1 li-bullet-0"><span class="c10">A</span><span class="c10 c16">r</span><span class="c6">&nbsp;= {1,2,3,1/2,1/3}</span></li><li class="c1 li-bullet-0"><span class="c6">w = s * sqrt(A)</span></li><li class="c1 li-bullet-0"><span class="c6">h &nbsp;= s / sqrt(A)</span></li></ul><p class="c0"><span class="c11 c10 c17"></span></p><p class="c0"><span class="c11 c10 c17"></span></p><a id="t.4eabd1635ee44005de5fdbd1dd863bd5d2681e49"></a><a id="t.1"></a><table class="c39"><tr class="c25"><td class="c18" colspan="1" rowspan="1"><p class="c2"><span class="c11 c10 c17"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">A =3 </span><span class="c36 c10 c17">(N/A)</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">A= 2</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">A=1</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">A= 1 ; s = sqrt(s</span><span class="c10 c16 c17">l</span><span class="c10 c17">*s</span><span class="c10 c16 c17">l+1</span><span class="c11 c10 c17">))</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">A= 1/2</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17">A= 1/3 </span><span class="c36 c10 c17">(N/A)</span></p></td></tr><tr class="c25"><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">Fmap size = 38x38</span></p><p class="c5"><span class="c11 c10 c17">s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 0.20</span></p><p class="c5"><span class="c11 c10 c17">s&rsquo; = sqrt(0.2*0.38) = 0.28</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c5"><span class="c10 c17 c36">w = 0.2 * sqrt(3) &nbsp;= &nbsp;0.34</span></p><p class="c5"><span class="c36 c10 c17">h &nbsp;= 0.2 / sqrt(3) &nbsp; = 0.11</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.2 * sqrt(2) &nbsp;= 0.28</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= &nbsp;0.2 / sqrt(2) = 0.14</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.2 * sqrt(1) = 0.2</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.2 * sqrt(1) = 0.2</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.26 * sqrt(1) = 0.26</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.26 / sqrt(1) &nbsp;= 0.26</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.2 * sqrt(1/2) = 0.14</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.2 / sqrt(1/2) &nbsp;= 0.28</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c5"><span class="c36 c10 c17">w = 0.2 * sqrt(1/3) = 0.11</span></p><p class="c5"><span class="c36 c10 c17">h &nbsp;= 0.2 / sqrt(1/3) = 0.34</span></p></td></tr><tr class="c25"><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">&hellip;</span></p><p class="c5"><span class="c11 c10 c17">&hellip;</span></p></td></tr><tr class="c25"><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">Fmap size = 1x1</span></p><p class="c5"><span class="c11 c10 c17">s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 0.90</span></p><p class="c5"><span class="c11 c10 c17">s&rsquo; = sqrt(0.9*1) = 0.94</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c5"><span class="c36 c10 c17">w = 0.90 * sqrt(3) &nbsp;= &nbsp;1.56</span></p><p class="c5"><span class="c36 c10 c17">h &nbsp;= 0.90 / sqrt(3) &nbsp; = 0.52</span></p></td><td class="c34" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.90 * sqrt(2) &nbsp;= 1.27</span></p><p class="c5"><span class="c11 c10 c17">h = &nbsp;0.90 / sqrt(2) = 0.63</span></p></td><td class="c35" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.90 * sqrt(1) = 0.9</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.90 * sqrt(1) = 0.9</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.94 * sqrt(1) = 0.94</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.94 / sqrt(1) &nbsp;= 0.94</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c11 c10 c17">w = 0.2 * sqrt(1/2) = 0.63</span></p><p class="c5"><span class="c11 c10 c17">h &nbsp;= 0.2 / sqrt(1/2) &nbsp;= 1.27</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c5"><span class="c36 c10 c17">w = 0.2 * sqrt(1/3) = 0.52</span></p><p class="c5"><span class="c36 c10 c17">h &nbsp;= 0.2 / sqrt(1/3) = 1.56</span></p></td></tr></table><p class="c3"><span class="c10">&nbsp;</span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 253.90px;"><img alt="" src="files/yolo/image1.png" style="width: 286.50px; height: 253.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 995.14px; height: 528.81px;"><img alt="" src="files/yolo/image9.png" style="width: 995.14px; height: 528.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c7">&nbsp;Image taken from SSD paper by Leu at el.</span></p><p class="c0"><span class="c7"></span></p><p class="c3"><span class="c11 c21">Loss Function :</span></p><p class="c3"><span class="c6">&nbsp;The loss function is almost the same as Faster RCNN.</span></p><p class="c3"><span class="c10">- Loss = &nbsp; L</span><span class="c10 c16">cls</span><span class="c10">&nbsp;+ alpha * L</span><span class="c28 c10 c16">loc</span></p><p class="c3"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= &nbsp; L</span><span class="c10 c16">loc</span><span class="c10">&nbsp;is the same as Faster RCNN + L</span><span class="c10 c16">cls</span><span class="c6">&nbsp;is softmax over the classes.</span></p><p class="c0"><span class="c11 c14"></span></p><p class="c3"><span class="c14">3. Less model parameters :</span><span class="c9">&nbsp;YOLO V1 It sends the flattened outputs of the </span><span class="c9">conv</span><span class="c9">. Feature maps to the fully connected layers and resized them again into </span><span class="c9">conv</span><span class="c9">. 7x7x30 (for VOC) feature maps. Dense layers require a lot more parameters (for taking the flattened input from the previous </span><span class="c9">conv</span><span class="c9">. layer) then just another </span><span class="c9">conv</span><span class="c9">. layer. Because it </span><span class="c9">need</span><span class="c11 c9">&nbsp;to be flattened &nbsp;Conv. layer may have parameters</span></p><p class="c3"><span class="c11 c9">If the conv layer was directly bypassed to the next conv layter skipping the dense layers using a 3x3 conv kernel &nbsp;the number of weights would be [3x3x512x1024] + [3x3x1024x30] &nbsp;~ 4.9e6</span></p><p class="c3"><span class="c11 c9">- If we do it with a single dense layer. The dense layer can only take the flattened results of [7x7x512x1024] &nbsp;sized tensor resulting ~ 25e6 input nodes. It should output [7x7x30] ~ 1.4e3 weights so that it can be easily resized to 7x7x30 again. The weight for this dense layer would be [7x7x512x1024] x [7x7x30] ~ 37e9</span></p><p class="c3"><span class="c11 c9">It should require at least &nbsp;~1000 times more parameters for this transition part.</span></p><p class="c0"><span class="c7"></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c7"></span></p><a id="id.fsu19pnk5cpt"></a><p class="c3"><span class="c37 c20 c40">YOLO Version 2</span></p><hr><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c6">1. YOLO v2 also uses anchor boxes at each of the grid locations in the featuremap like Faster RCNN and SSD. They make the model learn the grid offset in x,y direction, log scale offset is learnt for w and h. Unlike the previous approaches they don&rsquo;t hard code (Faster RCNN) or derive the anchor box from some empirical formula (SSD). They perform k means clustering over all the w and h of the bboxes in train data.</span></p><p class="c3"><span class="c6">2. They use k-means clustering over all the images of the dataset for finding the most optimal k number of anchor boxes for different values of k. They found 5 anchor boxes to work best in YOLO-v2. YOLO-v3 eventually adopted 9 anchor boxes from 3 scales.</span></p><p class="c3"><span class="c6">3. They have found batch normalization very effective.</span></p><p class="c3"><span class="c6">4. They have used higher resolution images this time (416x416). They also trained the model with different resolution images &nbsp;during the training for making it robust to image size.</span></p><p class="c3"><span class="c6">5. They have adopted an identity mapping approach which connects the previous layers directly to a future one for passing rich contexts to deeper layers. This approach can also prevent vanishing gradients for very deep networks e.g ResNet50. They copied the output of an older layer and added it to the activations of deeper layers.</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 358.50px; height: 266.97px;"><img alt="" src="files/yolo/image4.png" style="width: 358.50px; height: 266.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c31 c21 c27">&nbsp; &nbsp; &nbsp; &nbsp;Image taken from YOLO v2 paper by Redmon at el. &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c0"><span class="c6"></span></p><ul class="c24 lst-kix_10bg29dgyeg1-0 start"><li class="c1 li-bullet-0"><span class="c10">b</span><span class="c10 c16">x</span><span class="c10">&nbsp;= sig(t</span><span class="c10 c16">x</span><span class="c10">) + c</span><span class="c10 c16 c28">x</span></li><li class="c1 li-bullet-0"><span class="c10">b</span><span class="c10 c16">y</span><span class="c10">&nbsp;= sig(t</span><span class="c10 c16">y</span><span class="c10">) +c</span><span class="c28 c10 c16">y</span></li><li class="c1 li-bullet-0"><span class="c10">b</span><span class="c10 c16">w</span><span class="c10">&nbsp;= </span><span class="c10">p</span><span class="c10 c16">w</span><span class="c10">e</span><span class="c10 c15">tw</span></li><li class="c1 li-bullet-0"><span class="c10">b</span><span class="c10 c16">h</span><span class="c10">&nbsp;= p</span><span class="c10 c15">h</span><span class="c10">e</span><span class="c28 c10 c15">th</span></li></ul><p class="c0"><span class="c28 c10 c15"></span></p><p class="c3"><span class="c6">&nbsp;For x,y the model is learning the difference between b and c which is the grid offset. (To be precise the model is learning the inverse sigmoid of the grid offset.) For w and h, the model is learning the difference of logarithmic values of each of them.</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c10 c16">&nbsp; </span><span class="c10">The outputs of the model are &nbsp;t</span><span class="c10 c16">x ,</span><span class="c10">t</span><span class="c10 c16">y, </span><span class="c10">t</span><span class="c10 c16">w, </span><span class="c10">t</span><span class="c10 c16">h</span><span class="c6">&nbsp;which are the offsets being learnt finally. &nbsp;The below equations may give further intuition about them.</span></p><ul class="c24 lst-kix_y89d79xx04k4-0 start"><li class="c1 li-bullet-0"><span class="c10">t</span><span class="c10 c16">x</span><span class="c10">&nbsp;= inv_sig( b</span><span class="c10 c16">x</span><span class="c10">&nbsp;- c</span><span class="c10 c16">x</span><span class="c6">)</span></li><li class="c1 li-bullet-0"><span class="c10">t</span><span class="c10 c16">y</span><span class="c10">&nbsp;= inv_sig((b</span><span class="c10 c16">y</span><span class="c10">&nbsp;- c</span><span class="c10 c16">y</span><span class="c6">)</span></li><li class="c1 li-bullet-0"><span class="c10">t</span><span class="c10 c16">w</span><span class="c10">&nbsp;= log(b</span><span class="c10 c16">w</span><span class="c10">/p</span><span class="c10 c16">w</span><span class="c10">) = log(b</span><span class="c10 c16">w</span><span class="c10">) - log(p</span><span class="c10 c16">w</span><span class="c6">)</span></li><li class="c1 li-bullet-0"><span class="c10">t</span><span class="c10 c16">h</span><span class="c10">&nbsp;= &nbsp;log(b</span><span class="c10 c16">h</span><span class="c10">/p</span><span class="c10 c16">h</span><span class="c10">) = log(b</span><span class="c10 c16">w</span><span class="c10">) - log(p</span><span class="c10 c16">w</span><span class="c6">)</span></li></ul><p class="c0 c38"><span class="c6"></span></p><p class="c3"><span class="c6">where,</span></p><ul class="c24 lst-kix_8s7n3kf664eg-0 start"><li class="c1 li-bullet-0"><span class="c6">sig(x) = Sigmoid(x) &nbsp; = 1/ (1+e-x) </span></li><li class="c1 li-bullet-0"><span class="c6">inv_sig(x) = Inverse Sigmoid(x) = &nbsp;log(x/(1 -x))</span></li></ul><p class="c0 c38"><span class="c6"></span></p><p class="c3"><span class="c10">Sigmoid function is used to bound a (-inf , </span><span class="c10">inf</span><span class="c6">) output within (0-1). So the inverse sigmoid should do the opposite and make the model capable of outputting any arbitrary value from -inf to inf. The model can output any arbitrary value as offset (before the sigmoid is applied) but sigmoid will transform it to corresponding 0-1 value for fitting into the grid while adding some nonlinearity to the process. Addition of non-linearity is very important, because it enables the model to learn complex underlying functions beyond the linear domain.</span></p><p class="c0"><span class="c11 c21"></span></p><p class="c3"><span class="c11 c21">Minor updates :</span></p><p class="c3"><span class="c10">Selective backpropagation : </span><span class="c6">During training if a bbox has low overlap with the ground truth then back propagation can be skipped for that particular loss. </span></p><p class="c3"><span class="c10">Removing low confidence boxes boxes :</span><span class="c10">&nbsp;After prediction is done bboxes lower than a minimum (i.e 0.3) confidence, P(Class|</span><span class="c10">Object</span><span class="c6">) are filtered out for better accuracy.</span></p><p class="c3"><span class="c10">Non Maximum Suppression :</span><span class="c6">&nbsp; If some of the prediction boxes try to predict the same object with different confidence they should have a good overlap. For prediction boxes with IoU more than a threshold (i.e 0.5) only the one with highest confidence is kept.</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c29">Word Tree :</span><span class="c6">&nbsp;YOLO v2 introduced a way for incorporating large classification data sets (without bbox labels i.e ImageNet 1000) to be introduced within the framework. During training if an image samples from a classification dataset then only classification loss is calculated else it will calculate all the losses. For unifying the 1000 classes of imageNet to COCO 80 classes the each of the fine grained categories (i.e persian cat) is mapped back to its parent category present in COCO dataset (i.e cat). They have accomplished it by using the concept from Word Net from the Natural language processing domain. </span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 730.85px; height: 428.17px;"><img alt="" src="files/yolo/image5.png" style="width: 730.85px; height: 428.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c21 c27 c31">Figure : The core idea of YOLO is to find a vector like above at every pixel location in the feature map (not image).</span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c20 c40">YOLO</span><a id="id.fq0jql663kbl"></a><span class="c37 c20 c40">&nbsp;Version 3</span></p><hr><p class="c0"><span class="c37 c20 c40"></span></p><p class="c3"><span class="c10">- The main improvement in YOLO-v3 is extraction of data from three different scales of the feature map. They were inspired by the FPN[6] networks for taking this direction. The success of SSD with multi scale feature maps can also be a good inspiration. They have taken the feature maps from the last layer for small box prediction. Then </span><span class="c10">upsmple</span><span class="c6">&nbsp;it by doubling the size (width and height of feature map) and concatenate it with the feature maps from 2 layers back to predict medium sized boxes. They have repeated the same and predicted large boxes from feature maps from 2 more laye back. For upsampling transposed convolution (keras ConvTranspose2D layer with stride=2) can be used; it has some learnable parameters as well which allows the model to be more flexible. Another option is using upsample (keras UpSample2D layer) by repeating the data for filling the neighboring missing values at higher resolution.</span></p><p class="c3"><span class="c6">- They have used 9 anchor boxes (instead of 5) , sorted them by size and divided them into three scales: large , medium and small. Each of the 3 anchor boxes are applied on the corresponding output feature map branch (among small, medium and large) extracted from the network.</span></p><p class="c3"><span class="c6">- For confidence loss they used logistic losses instead of mean squared error. Although, it is a very common practice nowadays.</span></p><p class="c3"><span class="c6">- Softmax needs the class labels to be mutually exclusive. Each grid cell can occupy multiple objects from different classes at different scales. Like one person standing behind a car may have their center at the same location. They replaced the idea of using the softmax for each grid cell location .</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c6">I will show an example dimension through the figure for understanding it better considering the mobile net v2 as a backbone with input image shape 224x224 for other networks or input size it can be different. I will demonstrate with 80 classes considering the coco dataset. The authors used image size ~ 416x416 with darknet 53.</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 770.92px; height: 552.59px;"><img alt="" src="files/yolo/image2.png" style="width: 770.92px; height: 552.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c31 c21 c27">Figure : YOLO V3</span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c29">1x1 </span><span class="c29">conv</span><span class="c11 c21">. + Reshape</span></p><p class="c3"><span class="c10">1x1</span><span class="c10">&nbsp;convolution layer can change the depth of a feature map array keeping the height and width as it is. Here it is used to change the depth of the output featuremap &nbsp;to appropriate dimensions so that it can be reshaped to our desired dimension. For this purpose the 1x1 </span><span class="c10">conv</span><span class="c6">. should change any arbitrary depth to a length equals to anc box* (4+1+no of class). Height and width remains the same after applying 1x1 convolution. Then reshape is applied to make an extra dimension of size=3 (anc. box). Using a separate dimension for the anchor box is convenient for implementation. Numerical operations in many popular libraries can easily be applied on a dimension of an array. </span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c11 c21">Decode outputs to bboxes :</span></p><p class="c3"><span class="c6">1. Decode the bboxes and class from the output 4d array using respective anchor boxes for that feature map scale. Note that the same anchor boxes</span></p><p class="c3"><span class="c6">are also required for loss calculation.</span></p><p class="c3"><span class="c6">2. Low conf. bbox removal</span></p><p class="c3"><span class="c6">3. Non maximum suppression</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 816.50px; height: 357.62px;"><img alt="" src="files/yolo/image10.png" style="width: 816.50px; height: 357.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c31 c21 c27">Figure : &nbsp;(Left) Non Maximum Suppression (Middle) Total 9 anchor boxes at every fmap locations (Right) IoU and multi aspect ratio objects at one location. </span></p><p class="c0"><span class="c31 c21 c27"></span></p><p class="c0"><span class="c31 c21 c27"></span></p><a id="id.gsju1bji6owf"></a><p class="c3"><span class="c20 c40">Symbols and Short forms</span></p><hr><p class="c0"><span class="c11 c14"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 792.79px; height: 324.78px;"><img alt="" src="files/yolo/image8.png" style="width: 792.79px; height: 324.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c11 c9"></span></p><p class="c3"><span class="c11 c9">Below are some short forms that will be used in the later sections of this chapter : </span></p><p class="c3"><span class="c11 c9">RoI &nbsp; &nbsp;: Region of Interest. Small cropped section of an image.</span></p><p class="c3"><span class="c11 c9">Fmap /feat map : Feature Map. &nbsp;Truncated 3-dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.</span></p><p class="c3"><span class="c11 c9">FE &nbsp; &nbsp; : Feature Extractor. It will represent a deep learning model that extracts features from input data. The term &lsquo;FE&rsquo; will be used synonymously with any deep neural network architecture. e.g VGG16 which exactly does the same.</span></p><p class="c3"><span class="c11 c9">FC &nbsp; &nbsp; : Fully Connected Dense Layer/Layers.</span></p><p class="c3"><span class="c11 c9">bbox &nbsp;: Bounding box. [i.e box coordinates x,y,w,h]</span></p><p class="c3"><span class="c11 c9">GT &nbsp; &nbsp;: Ground Truth</span></p><p class="c3"><span class="c11 c9">CNN : Convolutional Neural Network. ; Conv. : Convolutional.</span></p><p class="c3"><span class="c11 c9">For explainability of the figures I have used consistent symbols while drawing part of the model. &nbsp;The symbols and references are defined at the end.</span></p><p class="c0"><span class="c31 c21 c27"></span></p><p class="c0"><span class="c6"></span></p><a id="id.hj7rbnumf70e"></a><p class="c3"><span class="c11 c29 c40">Code :</span></p><p class="c3"><span class="c10">I have &nbsp;implemented YOLO-v3 with some additional borrowed from YOLO-v4 here in this repository. </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4&amp;sa=D&amp;source=editors&amp;ust=1667733635292996&amp;usg=AOvVaw0VLb327W38xtxgHuX0splO">https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4</a></span></p><p class="c0"><span class="c11 c21"></span></p><a id="id.1a2ii16xnli"></a><p class="c3"><span class="c11 c29 c40">References </span></p><hr><p class="c0"><span class="c11 c21"></span></p><p class="c3"><span class="c11 c21">GitHub Repositories:</span></p><ul class="c24 lst-kix_1rpjbd1o2ztz-0 start"><li class="c1 li-bullet-0"><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3&amp;sa=D&amp;source=editors&amp;ust=1667733635293898&amp;usg=AOvVaw3lhrSjl8CgvImY7wBDgDsw">https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3</a></span></li></ul><ul class="c24 lst-kix_bk5el08vcmqt-0 start"><li class="c1 li-bullet-0"><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://github.com/ultralytics/yolov5&amp;sa=D&amp;source=editors&amp;ust=1667733635294265&amp;usg=AOvVaw2BNhj5pBumpviG67YHuw-V">https://github.com/ultralytics/yolov5</a></span></li></ul><p class="c3"><span class="c11 c21">Papers :</span></p><p class="c3"><span class="c10">1. Faster R-CNN: </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1667733635294773&amp;usg=AOvVaw1eXnfRt79djUIw2A_HrBwy">https://arxiv.org/abs/1506.01497</a></span></p><p class="c3"><span class="c10">2. You Only Look Once: Unified, Real-Time Object Detection : </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640.pdf&amp;sa=D&amp;source=editors&amp;ust=1667733635295206&amp;usg=AOvVaw3m3d4LdHfoAQFCayV0R806">https://arxiv.org/pdf/1506.02640.pdf</a></span></p><p class="c3"><span class="c10">3. SSD: Single Shot MultiBox Detector : </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.02325&amp;sa=D&amp;source=editors&amp;ust=1667733635295564&amp;usg=AOvVaw3Xpb_oYlzDDwZmmlg8EG0r">https://arxiv.org/abs/1512.02325</a></span></p><p class="c3"><span class="c10">4. YOLO9000: Better, Faster, Stronger : </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1612.08242&amp;sa=D&amp;source=editors&amp;ust=1667733635295956&amp;usg=AOvVaw1eLCseMvLndDAUTbjceZ2n">https://arxiv.org/abs/1612.08242</a></span></p><p class="c3"><span class="c10">5. YOLOv3: An Incremental Improvement : </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1804.02767?e05802c1_page%3D1&amp;sa=D&amp;source=editors&amp;ust=1667733635296325&amp;usg=AOvVaw3apdoQCrq2gCVLQJ-wT4UJ">https://arxiv.org/abs/1804.02767?e05802c1_page=1</a></span></p><p class="c3"><span class="c10">6. Feature Pyramid Networks for Object Detection : </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1612.03144.pdf&amp;sa=D&amp;source=editors&amp;ust=1667733635296708&amp;usg=AOvVaw0kurTUYE0Bieib7iEzbYOZ">https://arxiv.org/pdf/1612.03144.pdf</a></span></p><p class="c3"><span class="c10">7. Mask R-CNN: </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1667733635297235&amp;usg=AOvVaw3qrG2xCpNm6ZxVg6ExeDmV">https://arxiv.org/abs/1703.06870</a></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p><p class="c3"><span class="c11 c21">Blogs :</span></p><p class="c3"><span class="c10">1. </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/&amp;sa=D&amp;source=editors&amp;ust=1667733635297988&amp;usg=AOvVaw3qtwsFSu7UTO1JAynMtV5Y">https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</a></span></p><p class="c3"><span class="c10">2. </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;sa=D&amp;source=editors&amp;ust=1667733635298395&amp;usg=AOvVaw0YxWdfeb3SoLo6boD-Jhha">https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span></p><p class="c3"><span class="c10">3. </span><span class="c10 c13"><a class="c8" href="https://www.google.com/url?q=https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a&amp;sa=D&amp;source=editors&amp;ust=1667733635298796&amp;usg=AOvVaw3PKndJ7mdeyzln_0RvBVWO">https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a</a></span></p><p class="c3"><span class="c10">4. </span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06&amp;sa=D&amp;source=editors&amp;ust=1667733635299295&amp;usg=AOvVaw0TUqB-xpMELR_lmwcjHQCQ">https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></span></p><p class="c3"><span class="c10">5.</span><span class="c13 c10"><a class="c8" href="https://www.google.com/url?q=https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b%23:~:text%3DSSD%2520contains%25208732%2520default%2520boxes,to%2520obtain%2520the%2520final%2520prediction.&amp;sa=D&amp;source=editors&amp;ust=1667733635299974&amp;usg=AOvVaw09ByQPdVsvkig4JNRLF5tt">https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b#:~:text=SSD%20contains%208732%20default%20boxes,to%20obtain%20the%20final%20prediction.</a></span></p><p class="c0"><span class="c6"></span></p><p class="c0"><span class="c6"></span></p></body></html>