<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_mnrrbqvs6ca-8{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-7{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-6{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-5{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-4{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-3{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-2{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-1{list-style-type:none}ul.lst-kix_mnrrbqvs6ca-0{list-style-type:none}.lst-kix_p7e7tkpe56mn-8>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-7>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-5>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-6>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-3>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-8>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-6>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-0>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-4>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-5>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-1>li:before{content:"-  "}.lst-kix_p7e7tkpe56mn-2>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-7>li:before{content:"-  "}.lst-kix_mnrrbqvs6ca-0>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-8{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-7{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-6{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-5{list-style-type:none}.lst-kix_mnrrbqvs6ca-2>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-4{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-3{list-style-type:none}.lst-kix_mnrrbqvs6ca-1>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-2{list-style-type:none}ul.lst-kix_p7e7tkpe56mn-1{list-style-type:none}.lst-kix_mnrrbqvs6ca-4>li:before{content:"-  "}ul.lst-kix_p7e7tkpe56mn-0{list-style-type:none}.lst-kix_mnrrbqvs6ca-3>li:before{content:"-  "}ol{margin:0;padding:0}table td,table th{padding:0}.c9{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:italic}.c12{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c26{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c14{color:#000000;text-decoration:none;font-size:11pt;font-style:normal}.c7{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c21{color:#000000;text-decoration:none;font-size:16pt;font-style:italic}.c25{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c2{vertical-align:super;font-family:"Times New Roman";font-weight:400}.c24{color:#000000;text-decoration:none;font-size:11pt}.c20{color:#666666;text-decoration:none;font-style:italic}.c5{font-weight:400;font-family:"Times New Roman"}.c6{color:inherit;text-decoration:inherit}.c19{font-weight:400;font-family:"Arial"}.c17{font-size:14pt;font-style:italic}.c16{font-family:"Times New Roman";font-weight:700}.c22{font-weight:700;font-family:"Arial"}.c23{font-style:italic}.c10{font-size:12pt}.c13{vertical-align:sub}.c8{height:11pt}.c18{vertical-align:baseline}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25 doc-content"><p class="c26"><span class="c16 c17">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c16 c18 c21">Object Detection : Single Shot Detectors (YOLO V1-V3 , SSD)</span></p><hr><p class="c1 c8"><span class="c11"></span></p><p class="c1 c8"><span class="c16 c23 c18 c24"></span></p><p class="c1"><span class="c11">Faster RCNN Recap :</span></p><hr><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c5 c10">Among the RCNN Family of algorithms discussed in the </span><span class="c7 c5 c10"><a class="c6" href="https://www.google.com/url?q=https://irfanhasib0.github.io/blogs/%23/rcnn/&amp;sa=D&amp;source=editors&amp;ust=1666672296921327&amp;usg=AOvVaw1RC3gAostXQ9psijFxhFZj">previous article</a></span><span class="c3">, Faster RCNN (2015) and Mask RCNN (2017) are the fastest and most accurate. After Faster RCNN &nbsp;YOLO v1 and SSD were published and started to get popular in the object detection community. Both of them are single shot detectors; it means they take the image as input and predict the bboxes (bounding boxes) by passing the image data only once through different sections of the model. These three algorithms are quite contemporary and share many relevant upgrades. &nbsp;I will start this section with these three then will cover yolo v2 and v3 with more detail. For knowing details about Mask RCNN or Faster RCNN please refer to the blog on RCNN &nbsp;Family. One major drawback of Faster RCNN is that it is still quite slow(~ 7 FPS), consequently fails to support many real of the time applications which require faster predictions.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; The Faster RCNN was able to achieve the region proposals and Fmap RoIs by one pass through a network by sharing most other conv layer weights. The major bottleneck in the regime of speed for Faster RCNN is that it requires one by one passing of the feature maps through the classifier, possibly this mostly helps the algorithm to reach a high level of &nbsp;accuracy. YOLO tried to make a tradeoff between speed and accuracy. It replaced the one by one Fmap processing with an end to end manner. The bbox regressor was a tensor. It has width and height of the feature map and a depth of 4k + 2k , k = number of anchor bbox ; 4 -&gt; {x,y,w,z} ; 2-&gt; {object , no object confidence}. So it was able to predict k no of box coordinates and 2 confidence for each box at </span></p><p class="c1"><span class="c3">every feature map location.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; YOLO v1 enhanced this tensor further by introducing N more channels in the depth direction, here N = = No of object classes. So the feature map has Height and Width as it is but across the depth it has extra channels which can predict the probability of each of the object class per feature map location. This extra channel eliminates the necessity of additional passing of the Fmap RoIs through the classifier one by one instead it does the same thing in one pass.</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 746.50px; height: 398.37px;"><img alt="" src="https://lh6.googleusercontent.com/hSvZc2th8H4ZtEK4L0u2ONDvPeq0IFxKky7f1oba2GlAXkA1Qw2jMnDIyolvqXREleBa9eHu55YIKb6XlNhu5LLGivLgnk_tA9m8Iy_eO2BeMRWwxNsbJMA2_LnFf02ccXi34blHQJFYATeB7ah6X5IzGbnrilVB-YroViZP922TzBIEsIM" style="width: 746.50px; height: 398.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">Figure : Faster RCNN </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c11">YOLO Version 1 </span></p><hr><p class="c1 c8"><span class="c11"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c9">Improvements :</span></p><p class="c1"><span class="c16 c10">1. No separate RPN network :</span><span class="c3">&nbsp;For YOLO the RPN tensor , Confidence tensor (objectnes confidence only), the classifier tensor are all stacked together so the outputs of the RPN is not separable. It all stacked together at the end of the network. It will be discussed in the later section in detail.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c16 c10">2. Eliminating RoI Align :</span><span class="c3">&nbsp;</span></p><p class="c1"><span class="c3">[i] Faster RCNN predicts the Region Proposal bbox wrt the original image size so they need to be scaled down for projecting on the featuremap. YOLO directly predicts the region proposals on top of the feature map so no need to scale anymore. </span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">[ii] Faster RCNN requires the Fmap RoI to be of fixed size so that they can be fed to the classifier layers but here the classification is done parallely for every region proposal at every feature map location. No need to input the feature maps to the classifier layers; the classifier layers are inherited from the respective feature maps.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c16 c10">3. Eliminated One by One Fmap RoI classification :</span><span class="c3">&nbsp;It is done more efficiently by jointly predicting the bboxes and class probabilities (for every object) at every featuremap location.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c9">Draw backs :</span></p><p class="c1"><span class="c3">It does not use an anchor box in v1. Eventually v2 adopted it. </span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c9">How it works : </span></p><p class="c1"><span class="c16 c10">&nbsp; &nbsp; &nbsp; </span><span class="c3">In the context of deep convolutional neural networks the output from every CNN stage is a featuremap i.e 3D array. It represents distinct high level information across the channels for each of the spatial pixel locations. &nbsp;Usually, the first two dimensions are width and height gradually shrinks down from the original image while the depth gets higher in parallel for representing more complex image features by each of its depth channels.</span></p><p class="c1"><span class="c10 c16">&nbsp; &nbsp; &nbsp; </span><span class="c3">The YOLO model takes an image and outputs an array of shape 7x7xN. Each of the channels of the output tensor represent - x,y,w,h,objectness confidence, class 1 confidence, class 2 confidence, ........., class n confidence respectively at every 7x7 grid location.</span></p><p class="c1"><span class="c3">&nbsp;The following example will make it more clear. The red circle on the grid (figure below) represents the 1x2 location in the featuremap. So the numeric value at (grid_x,grid_y,chanel)</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x0) = x ; If there is a object at this location, &nbsp;&#39;x&#39; would be the offset of its location along the height</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x1) = y; &nbsp;similarly &#39;y&#39; would be its offset along width.</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x2) = w; &nbsp;&#39;w&#39; is the width of the object bbox</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x3) = h; &nbsp; &#39;h&#39; is height of the bbox.</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x4) = probability of presence of any class object at that location.</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x5) = probability of the presence of a &#39;class 1&#39; object at that location </span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (1x2x[n+5]) = probability of the presence of a &#39;class n&#39; object at that location </span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">Like faster-rcnn, they designed the loss function in such a way that the model only learns the x,y offsets from the grid corner, not the absolute pixel coordinate of the object in the image (See x,y values in the enlarged figure in the bottom-right side). The absolute coordinate is derived from the grid location of the feature map. Its value is restricted from 0-1 by applying sigmoid activation function, so that it does not cross the current cell boundary. If it needed to learn the absolute coordinate the value of x,y needed to cover the entire image width and height. Learning an object&#39;s location in the entire image precisely is a harder task then just learning its location within the grid.</span></p><p class="c1"><span class="c3">Now x,y covers a small portion of the region but for w, h there is no restriction for staying within the grid. So w,h loss can get very high sometimes for big boxes. It can make the training unstable for all the losses since they are added together for gradient calculation. For handling this issue they use the difference between the square root of w and h instead of their actual value (Just to remind Faster RCNN was using log scale here). See the loss function below for better understanding.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 860.42px; height: 473.90px;"><img alt="" src="https://lh4.googleusercontent.com/ETb7lIDbUHVDBxhhs9HnyFcbJb1GksjU_E-Tr8cDMzmLktazFv3Em1cN99JE8wpyXWui1780hlPtR3E8Ug5aPf1YCOlZD2vjq0Sptms06xBUVmfwIome2eWvEOAshke_iAFEO5i5GWRck3xkpF_YU56h0Z5cLODf1fpwhm5vQS7fkO7NYLU" style="width: 860.42px; height: 473.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c5 c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c16 c10 c18 c20">Figure : Yolo V1</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c16 c10 c23">The Algorithm </span></p><p class="c1"><span class="c3">In the figure one thing is avoided for simplicity. Yolo uses two boxes at each pixel location. So there would be two set of x,y,w,h like -&gt; x1,y1,w1,h1 , x2,y2,w2,h2. &nbsp;Resulting the depth as follows - 2 (two boxes) * 4 + 1+ no of class. &nbsp;For VOC dataset they used if no of box = 2 and class = 20 having a depth of 2*4 + 1+ 20 = 30 resulting in the output tensor with shape 7x7x30.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c9">Training Steps </span></p><p class="c1"><span class="c3">1. Preparing GT tensor : It is a good practice to prepare the batch data generator functions matching the output format of the YOLO v1 model of size (7x7xN) N = &nbsp;4 + 1 + no of classes. So that the loss can be calculated easily by comparing with the model output.</span></p><p class="c1"><span class="c3">2. Forward pass : Feed the image to the model and extract the predicted tensor with shape 7x7xN</span></p><p class="c1"><span class="c3">3. xy_loss : slice the first 2 channels of the ground truth and prediction tensors both having shape 7x7x2 and calculate the squared difference.</span></p><p class="c1"><span class="c3">In numpy syntax it would be like this -&gt; xy_gt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = gt_tensor[ : , : , :2 ] </span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; xy_pred &nbsp; &nbsp; &nbsp; &nbsp;= predicted_tensor[: , : , :2]</span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; mean (square(xy_gt - xy_pred))</span></p><p class="c1"><span class="c3">4. wh_loss : slice out the 3rd and 4th (tensor[:, :, 2:4]) channel and take their square root and calculate squared difference for w,h error.</span></p><p class="c1"><span class="c3">5. conf_loss :</span></p><p class="c1"><span class="c5 c10">&nbsp; &nbsp; &nbsp; - slice the 5th channel (tensor[:, :, 5]) and apply an object mask [1</span><span class="c2 c10">obj</span><span class="c3">&nbsp;] on every cell. It means Multiply the locations with an object in GT with 1 else 0. Finally calculate mse among the masked </span></p><p class="c1"><span class="c3">tensores. </span></p><p class="c1"><span class="c5 c10">&nbsp; &nbsp; &nbsp;- calculate the loss again this time applying no object mask [1</span><span class="c2 c10">nobj</span><span class="c5 c10">] on every cell. Add this loss with a smaller weight [&#x1d6cc;</span><span class="c5 c10 c13">noobj &nbsp;</span><span class="c3">] because there will be lot of box with no object </span></p><p class="c1"><span class="c3">compared to object boxes in GT.</span></p><p class="c1"><span class="c3">6.Class confidence loss : </span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; - Slice rest of the channels (tensor[:, :, 5: ]) , convert the raw values to conditional probabilities P(class | object) by multiplying with object confidence at the same location. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c1"><span class="c3">&nbsp; &nbsp; &nbsp; - Then calculate the classification loss in the same way as confidence loss. Note currently softmax and cross entropy loss is used at this place in the SOTA methods.</span></p><p class="c1"><span class="c3">7. Finally, backpropagation is done with the calculated loss.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.50px; height: 296.24px;"><img alt="" src="https://lh6.googleusercontent.com/sHhV4x2KdyxL7LfE966OzTsbALuDAwlPGL_XQ_0rie9lPJIGqU4rB52k6-4Dm-I7orfTqamyQ4l7RKImMnHZzbg2PcpQ7J65OTEfjg0zZXlTVPn1EeV_9F7WHIE4zyQJFCEz4-3d_7CLqFYhRsM47gvLQqF3fAJfqj0YnrDLjDJl0fvQDO0" style="width: 439.50px; height: 296.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c9">Inference Steps :</span></p><p class="c1"><span class="c3">For drawing the bbox in the original image we need to project the predicted x,y,w,h to the respective locations of the actual image.</span></p><p class="c1"><span class="c3">&nbsp;- If the Image size = (416x416) and model output size = (7x7), the ratio of image and fmap is, &nbsp;grid-scale =416/7 = 13</span></p><p class="c1"><span class="c3">&nbsp;- Each of the 7x7 grids can be projected back to its parent location in the image by multiplying by grid-scale = 13.</span></p><p class="c1"><span class="c3">&nbsp;- Let&#39;s consider the following values for x,y,w,h = [0.3,0.2,4,5], in the above example at grid location [grid_x,grid_y] = [1,2].</span></p><p class="c1"><span class="c3">X = grid-scale * (grid_x + x) = 13 * (1 + 0.3) = &nbsp;16.9 ~ 17</span></p><p class="c1"><span class="c3">Y = grid-scale * (grid_y + y) = 13 * (2 + 0.2) = &nbsp;28.6 ~ 29</span></p><p class="c1"><span class="c3">W = grid-scale * w = 4 * 13 = 52</span></p><p class="c1"><span class="c3">H &nbsp;= grid-scale * h &nbsp; = 5 * 13 = 65</span></p><p class="c1"><span class="c3">The reverse would be like below,</span></p><p class="c1"><span class="c3">x = X / grid-scale - grid_x </span></p><p class="c1"><span class="c3">y = Y / grid-scale - grid_y </span></p><p class="c1"><span class="c3">w = W / grid-scale</span></p><p class="c1"><span class="c3">h = H /grid-scale</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c17 c16">SSD &nbsp;(</span><span class="c16 c10">Single shot multi box detector.</span><span class="c11">)</span></p><hr><p class="c1 c8"><span class="c9"></span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">Unlike Faster RCNN both YOLO v1 SSD also eliminates the necessity of putting the RoI Feature maps one by one through the dense classifier. They are both single pass / single shot detectors. One image passes through the end to end network and predicts the bboxes within the image all after just one pass. In one line this is the prime reason for these single shot detectors accuracy.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c3">SSD is better than YOLO v1 in the following ways :</span></p><p class="c1"><span class="c16 c10">1. Less model parameters :</span><span class="c3">&nbsp;YOLO V1 It sends the flattened outputs of the conv Feature maps to the fully connected layers and resized them again into conv 7x7x30 (for VOC) feature maps. Dense layers require a lot more parameters (for taking the flattened input from the previous conv layer) then just another conv layer. Because it need to be flattened &nbsp;Conv layer may have parameters</span></p><p class="c1"><span class="c3">If the conv layer was directly bypassed to the next conv layter skipping the dense layers using a 3x3 conv kernel &nbsp;the number of weights would be [3x3x512x1024] + [3x3x1024x30] &nbsp;~ 4.9e6</span></p><p class="c1"><span class="c3">- If we do it with a single dense layer. The dense layer can only take the flattened results of [7x7x512x1024] &nbsp;sized tensor resulting ~ 25e6 input nodes. It should output [7x7x30] ~ 1.4e3 weights so that it can be easily resized to 7x7x30 again. The weight for this dense layer would be [7x7x512x1024] x [7x7x30] ~ 37e9</span></p><p class="c1"><span class="c3">It should require at least &nbsp;~1000 times more parameters for this transition part.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c16 c10">2. Multi scale feature maps : </span><span class="c5 c10">The feature maps with higher resolution would be able to predict the small objects better while smaller ones would be better for large objects. </span><span class="c3">It uses the different resolution feature maps from different stages of the model for final prediction. It helps the model to predict objects of different sizes more easily. &nbsp;The feature maps would have varying depths while for prediction we need fixed depth consisting of i.e x,y,w,h ,conf etc. They pass the arbitrary depth feature maps from different stages through a 3x3 conv layer with depth k*(classes+4). From the outputs of the conv layer they calculate the boxes like YOLO V1.</span></p><p class="c1"><span class="c3">For mxn featuremap with k aspect ratio boxes it can predict k boxes at each of the m*n location resulting boxe = k*m*n. </span></p><p class="c1"><span class="c3">SSD predicts a total of 8732 boxes of different scaled feature maps.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span class="c14 c22 c18">Default boxes :</span></p><p class="c1"><span>For each of the feature map scales they used the default box. It is the same as the anchor box in RCNN, see the corresponding section in &nbsp;&ldquo;RCNN Family&rdquo; blog for better understanding of this part. Unlike Faster RCNN they used 5 boxes for each feature map scale with aspect ratios &nbsp;A</span><span class="c13">r</span><span>&nbsp;= {1,2,3,1/2,1/3}. They scaled the anchor boxes at different feature map levels with a scale factor which ensures the feature map stage emphasizes on learning the intended scales. For example scale 38x38 should learn small objects so they scaled it with small scale factor = 0.2. Scale, s </span><span class="c0">= 0.2 and s = 0.9 all the intermediate feature maps are equally mapped. The small scale (0.2) for the 38x38 pushes it to find small boxes while the 1x1 map with scale 0.9 will try to find very big boxes at the center of the image.</span></p><p class="c1"><span class="c0">Width and height is calculated from scale and aspect ratio as follows - width, &nbsp;w = s * sqrt(A) ; height, h = s/sqrt(A)</span></p><p class="c1 c8"><span class="c0"></span></p><p class="c1"><span class="c14 c22 c18">Loss Function</span></p><p class="c1"><span>- Loss = L</span><span class="c13">cls</span><span>&nbsp;+ alpha * L</span><span class="c14 c13 c19">loc</span></p><p class="c1"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; L</span><span class="c13">loc</span><span>&nbsp;is the same as Faster RCNN while L</span><span class="c13">cls</span><span class="c0">&nbsp;is softmax over the classes.</span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 286.50px; height: 253.90px;"><img alt="" src="https://lh3.googleusercontent.com/aLo4f_hLKvSvNoaXBiy85LFSmenKjui9s4PrvHSNCPNbK_ft1SkKWT8jb9DSkwfCpJzzwm84Ub2xjUph_sxPfk1-CW01sxnVJphIwda0FnRf5I-Z7qnNhe7Xotn30PeEdr1MhF0QsnQNnaiulBao-ycFCSG52zt_EqmZqTU9Fma5qV0OwxU" style="width: 286.50px; height: 253.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c8"><span class="c3"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 995.14px; height: 528.81px;"><img alt="" src="https://lh6.googleusercontent.com/lQWaFSTq197uRXRJuipobz_hNXN_bZVHDh1G1PA2uzDQiAI-ylAj7zKbILcjX_kqpGgZay9uYxESzprGQO4BGzEqbu9sgrr55Q25HUVUUSzWitBMVFvDtHn4EDk57gi0DaiqwVygilyPbbFy8IA08UG1a28vmnVkUEeLYOs57GrQDXwvlxo" style="width: 995.14px; height: 528.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c20 c22 c10 c18">&nbsp;Image taken from SSD paper by Leu at el.</span></p><p class="c1 c8"><span class="c0"></span></p><p class="c1 c8"><span class="c20 c10 c18 c22"></span></p><p class="c1"><span class="c11">YOLO Version 2</span></p><hr><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c4">1. Using anchor boxes at each of the grid locations in the featuremap. They make the model learn the grid offset in x,y direction like before. Unlike the previous version they now make the model to learn the log scale w,h offset in a from the prior box size.</span></p><p class="c1"><span class="c4">2. They used k-means clustering over all the images of the dataset for finding the most optimal k number of anchor boxes for different values of k. They found 5 anchor boxes were working best. Using three anchor boxes from 3 scales is also a good practice nowadays.</span></p><p class="c1"><span class="c4">3. They found batch normalization very effective.</span></p><p class="c1"><span class="c4">4. They used higher resolution images this time (416x416). They also trained the model with different resolution images &nbsp;during the training for making it robust to image size.</span></p><p class="c1"><span class="c4">5. They adopted an identity mapping approach similar to resnet. They copied the output of an older layer and added it to the activations of deeper layers.</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 358.50px; height: 266.97px;"><img alt="" src="https://lh5.googleusercontent.com/m4feLvR8vD76pHcOHe_usnoDV3WIN31ay2UcIXXkhZe90X4CfDOzd9x1l-SQH358FqTMplnPQBvcgZMltAw0UQasGd8Si3LsZYgQty1ZY-YYsIYIYnIMXYrUKRD7NF2n8Jy2LQzXNq5sxE9L9SsvWUcvdlaIsKv6G0KkRnbxljY-9ZuSGvY" style="width: 358.50px; height: 266.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c12">&nbsp; &nbsp; &nbsp; &nbsp;Image taken from YOLO v2 paper by Redmon at el. &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c5">b</span><span class="c5 c13">x</span><span class="c5">&nbsp;= sig(t</span><span class="c5 c13">x</span><span class="c5">) + c</span><span class="c5 c13 c14">x</span></p><p class="c1"><span class="c5">b</span><span class="c5 c13">y</span><span class="c5">&nbsp;= sig(t</span><span class="c5 c13">y</span><span class="c5">) +c</span><span class="c14 c5 c13">y</span></p><p class="c1"><span class="c5">b</span><span class="c5 c13">w</span><span class="c5">&nbsp;= </span><span class="c5">p</span><span class="c5 c13">w</span><span class="c5">e</span><span class="c2">tw</span></p><p class="c1"><span class="c5">b</span><span class="c5 c13">h</span><span class="c5">&nbsp;= p</span><span class="c2">h</span><span class="c5">e</span><span class="c14 c2">th</span></p><p class="c1 c8"><span class="c14 c2"></span></p><p class="c1"><span class="c4">&nbsp;For x,y the model is learning the difference between b and c which is the grid offset. (To be precise the model is learning the inverse sigmoid of the grid offset.) For w and h, the model is learning the difference of logarithmic values of each of them.</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c5">t</span><span class="c5 c13">x</span><span class="c5">&nbsp;= inv_sig( b</span><span class="c5 c13">x</span><span class="c5">&nbsp;- c</span><span class="c5 c13">x</span><span class="c4">)</span></p><p class="c1"><span class="c5">t</span><span class="c5 c13">y</span><span class="c5">&nbsp;= inv_sig((b</span><span class="c5 c13">y</span><span class="c5">&nbsp;- c</span><span class="c5 c13">y</span><span class="c4">)</span></p><p class="c1"><span class="c5">t</span><span class="c5 c13">w</span><span class="c5">&nbsp;= log(b</span><span class="c5 c13">w</span><span class="c5">/p</span><span class="c5 c13">w</span><span class="c5">) = log(b</span><span class="c5 c13">w</span><span class="c5">) - log(p</span><span class="c5 c13">w</span><span class="c4">)</span></p><p class="c1"><span class="c5">t</span><span class="c5 c13">h</span><span class="c5">&nbsp;= &nbsp;log(b</span><span class="c5 c13">h</span><span class="c5">/p</span><span class="c5 c13">h</span><span class="c5">) = log(b</span><span class="c5 c13">w</span><span class="c5">) - log(p</span><span class="c5 c13">w</span><span class="c4">)</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c4">Here, sig(x) -&gt; Sigmoid(x) ; inverse_sig(x) -&gt; Inverse Sigmoid</span></p><p class="c1"><span class="c4">Sigmoid(x) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 1/ (1+e-x) </span></p><p class="c1"><span class="c4">Inverse Sigmoid(x) = &nbsp;log(x/(1 -x))</span></p><p class="c1"><span class="c4">Intuitively, sigmoid bounds a (-inf , inf) function output to (0-1). So the inverse sigmoid should do the opposite and make the model capable of outputting to have value from -inf to inf. So basically the model can output a large value as offset but sigmoid will transform (non linearly) it to corresponding 0-1 value for fitting into the grid which will add some nonlinearity to the process.</span></p><p class="c1 c8"><span class="c14 c16 c18"></span></p><p class="c1"><span class="c16">Selective backpropagation : </span><span class="c4">During training if an bbox has low overlap with the ground truth then back propagation can be skipped for that particular loss.</span></p><p class="c1"><span class="c4">There are some post prediction techniques that are commonly practiced for better accuracy.</span></p><p class="c1"><span class="c16">Removing low confidence boxes boxes :</span><span class="c5">&nbsp;After prediction is done bboxes lower than a minimum (i.e 0.3) confidence, P(Class|</span><span class="c5">Object</span><span class="c4">) are filtered out for better accuracy.</span></p><p class="c1"><span class="c16">Non Maximum Suppression :</span><span class="c4">&nbsp; If some of the prediction boxes try to predict the same object with different confidence they should have a good overlap. For prediction boxes with IoU more than a threshold (i.e 0.5) only the one with highest confidence is kept.</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c16">Word Tree :</span><span class="c4">&nbsp;YOLO v2 introduced a way for incorporating large classification data sets (without bbox labels i.e ImageNet 1000) to be introduced within the framework. During training if an image samples from a classification dataset then only classification loss is calculated else it will calculate all the losses. For unifying the 1000 classes of imageNet to COCO 80 classes the each of the fine grained categories (i.e persian cat) is mapped back to its parent category present in COCO dataset (i.e cat). They have accomplished it by using the concept from Word Net from the Natural language processing domain. </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 730.85px; height: 428.17px;"><img alt="" src="https://lh5.googleusercontent.com/yMYyqI6ADCSugKcp_Yfx2hQVZZQwuJYNJVBX4ehWEov8Jg5h9KHvTKHZ---PT0eGZwkMUbsSP8Ap8NloRH_yokJjLWJQ_XQPOS2VocMnccA7NLUUaSQ8u1MsjmsloQOusaVOgW697Wc0YgJ3sOgU6OgFE5r_uX9oa-udZXornsA3WjZrLdE" style="width: 730.85px; height: 428.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c12">Figure : The core idea of YOLO is to find a vector like above at every pixel location in the feature map (not image).</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c11">Yolo Version 3</span></p><hr><p class="c1 c8"><span class="c11"></span></p><p class="c1"><span class="c4">- The main improvement in yolo v3 is extracting data with three different scales from the feature map. They were inspired by FPN networks for this. They have taken the feature maps from the last layer for small box prediction. Then they upsmples it by doubling the size and concatenated it with the feature maps from 2 layers back and predicted medium sized boxes. They have repeated the same and predicted large boxes from feature maps from 2 more laye back.</span></p><p class="c1"><span class="c4">- They have used 9 anchor boxes (instead of 5) , sorted them by size and divided them into three scales: large , medium and small. Each of the 3 anchor boxes are applied on the corresponding feature maps extracted from the network.</span></p><p class="c1"><span class="c4">- For confidence loss they used logistic losses instead of mean squared error. Although, it is a very common practice nowadays.</span></p><p class="c1"><span class="c4">- Softmax needs the class labels to be mutually exclusive. Each grid cell can occupy multiple objects from different classes at different scales. Like one person standing behind a car having their center at the same location in the image. They replaced the idea of using the softmax for each grid cell location </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c4">I will show an example dimension through the figure for understanding it better considering the mobile net v2</span></p><p class="c1"><span class="c4">as a backbone with input image shape 224x224 for other networks or input size it can be different.</span></p><p class="c1"><span class="c4">I will demonstrate with 80 classes considering the coco dataset. </span></p><p class="c1"><span class="c4">The authors used image size ~ 416x416 with darknet 53</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 770.92px; height: 552.59px;"><img alt="" src="https://lh5.googleusercontent.com/tgXD6J2Yr4V60RsWpYzIREKUyyG7ly3iU9DjUWa4PWYr2nT_Off3lSpcp7XgiD5fYov7m_8CQ8Biq6a_6AKAbDOBluEpUE5WTWrZAvitKwL8DzmB0eJvGbjuVKVEquzfWxRZAExldWPGIsjkix0b-gOhHn49hf4o2zfyBPEG3wHEJnOb3-M" style="width: 770.92px; height: 552.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c5">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c12">Figure : YOLO V3</span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c14 c16 c18">1x1 conv + Reshape</span></p><p class="c1"><span class="c4">1x1 convolution layer can change the depth of a feature map array keeping the height and width as it is. Here it is used to change the depth of the output featuremap &nbsp;to appropriate dimension so that it can be reshaped to our desired dimension. For this purpose the 1x1 conv should change any arbitrary depth to a length equals to anc box* (4+1+no of class). Height and width remains the same after applying 1x1 convolution. Then reshape is applied to make and extra dimension of size=3 (anc. box). Using a separate dimension for the anchor box is convenient for implementation. Numerical operations in many popular libraries can easily be applied on a dimension of an array. </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c14 c16 c18">Decode outputs to bboxes :</span></p><p class="c1"><span class="c4">1. Decode the bboxes and class from the output 4d array using respective anchor boxes for that feature map scale. Note that the same anchor boxes</span></p><p class="c1"><span class="c4">are also required for loss calculation.</span></p><p class="c1"><span class="c4">2. Low conf. bbox removal</span></p><p class="c1"><span class="c4">3. Non maximum suppression</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 816.50px; height: 357.62px;"><img alt="" src="https://lh6.googleusercontent.com/hjbspyqUjv0ih5927NaYybTyK7mr_s6JWuBe9tYsvV7SHBEeDNGVU4chSoLlXSZWrXgcgHsd7UY8_9UEBcOi8ZIl49QeQb21w90wm2ScmGlOjKq-EDLm65aHQ53BZhhUX0e83PGQOAIFcMnUkH9grwVpwwo5MHMZfCYLiSHNYML66PYwE7Y" style="width: 816.50px; height: 357.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c12">Figure : &nbsp;(Left) Non Maximum Suppression (Middle) Total 9 anchor boxes at every fmap locations (Right) IoU and multi aspect ratio objects at one location. </span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c14 c16 c18">Code :</span></p><p class="c1"><span class="c5">Yolo V4 : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4&amp;sa=D&amp;source=editors&amp;ust=1666672296940640&amp;usg=AOvVaw0ctxLR-pXbY9G-u_xGkF_v">https://github.com/irfanhasib0/Deep-Learning-For-Computer-Vision/tree/main/yolo-v4</a></span></p><p class="c1 c8"><span class="c14 c16 c18"></span></p><p class="c1"><span class="c15">References </span></p><hr><p class="c1 c8"><span class="c14 c16 c18"></span></p><p class="c1"><span class="c4">Papers :</span></p><p class="c1"><span class="c5">1. Faster R-CNN: </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1666672296941416&amp;usg=AOvVaw0fwqc2ZMQ3oNv65sRjRivM">https://arxiv.org/abs/1506.01497</a></span></p><p class="c1"><span class="c5">2. You Only Look Once: Unified, Real-Time Object Detection : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640.pdf&amp;sa=D&amp;source=editors&amp;ust=1666672296941746&amp;usg=AOvVaw3l-Phan5soHpxUhQvVVknm">https://arxiv.org/pdf/1506.02640.pdf</a></span></p><p class="c1"><span class="c5">3. SSD: Single Shot MultiBox Detector : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.02325&amp;sa=D&amp;source=editors&amp;ust=1666672296942122&amp;usg=AOvVaw2aonFCTsFBus0YH8xx2DRF">https://arxiv.org/abs/1512.02325</a></span></p><p class="c1"><span class="c5">4. YOLO9000: Better, Faster, Stronger : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1612.08242&amp;sa=D&amp;source=editors&amp;ust=1666672296942513&amp;usg=AOvVaw1iJZIdhI6gJ36EzgvPd8oW">https://arxiv.org/abs/1612.08242</a></span></p><p class="c1"><span class="c5">5. YOLOv3: An Incremental Improvement : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1804.02767?e05802c1_page%3D1&amp;sa=D&amp;source=editors&amp;ust=1666672296942906&amp;usg=AOvVaw3tp_TaMBIs5qqZ_NQkENXq">https://arxiv.org/abs/1804.02767?e05802c1_page=1</a></span></p><p class="c1"><span class="c5">6. Feature Pyramid Networks for Object Detection : </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/pdf/1612.03144.pdf&amp;sa=D&amp;source=editors&amp;ust=1666672296943274&amp;usg=AOvVaw2dIFvY-5aNSstOMH9Uwi_d">https://arxiv.org/pdf/1612.03144.pdf</a></span></p><p class="c1"><span class="c5">7. Mask R-CNN: </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1666672296943608&amp;usg=AOvVaw3CI3PeDxoY9wO8lz9Nigu2">https://arxiv.org/abs/1703.06870</a></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1"><span class="c4">Blogs :</span></p><p class="c1"><span class="c5">1. </span><span class="c5 c7"><a class="c6" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/&amp;sa=D&amp;source=editors&amp;ust=1666672296944179&amp;usg=AOvVaw0ZjW9TLQ9br6SB-nDmSi8x">https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</a></span></p><p class="c1"><span class="c5">2. </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;sa=D&amp;source=editors&amp;ust=1666672296944652&amp;usg=AOvVaw1pdsp8wAxGY34ysAMFDQne">https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></span></p><p class="c1"><span class="c5">3. </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a&amp;sa=D&amp;source=editors&amp;ust=1666672296945056&amp;usg=AOvVaw3Xaf7KJxwbHgrfYIz3yvrE">https://medium.com/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a</a></span></p><p class="c1"><span class="c5">4. </span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06&amp;sa=D&amp;source=editors&amp;ust=1666672296945449&amp;usg=AOvVaw3xsRCP-dXDh_UMXli6q-cU">https://jonathan-hui.medium.com/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></span></p><p class="c1"><span class="c5">5.</span><span class="c7 c5"><a class="c6" href="https://www.google.com/url?q=https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b%23:~:text%3DSSD%2520contains%25208732%2520default%2520boxes,to%2520obtain%2520the%2520final%2520prediction.&amp;sa=D&amp;source=editors&amp;ust=1666672296945970&amp;usg=AOvVaw1yGJYlTYNHiBJfgN99MEIE">https://medium.com/inveterate-learner/real-time-object-detection-part-1-understanding-ssd-65797a5e675b#:~:text=SSD%20contains%208732%20default%20boxes,to%20obtain%20the%20final%20prediction.</a></span></p><p class="c1 c8"><span class="c4"></span></p><p class="c1 c8"><span class="c4"></span></p></body></html>