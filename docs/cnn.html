<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_nebp76l47abp-8>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-8,lower-roman) ". "}.lst-kix_nebp76l47abp-7>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-7,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-8>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-8,lower-roman) ". "}.lst-kix_fh2qtwmwsffy-2>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-4>li:before{content:"-  "}.lst-kix_7yi37vgztglf-1>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-1,lower-latin) ". "}ul.lst-kix_ngpgdtyz59e3-6{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-7.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-7 0}ul.lst-kix_ngpgdtyz59e3-7{list-style-type:none}ul.lst-kix_ngpgdtyz59e3-8{list-style-type:none}.lst-kix_fh2qtwmwsffy-1>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-5>li:before{content:"-  "}.lst-kix_7yi37vgztglf-0>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-0,decimal) ". "}.lst-kix_h7mmrf9tmnyi-7>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-7}.lst-kix_y1j9a0umnveg-2>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-2}ul.lst-kix_ngpgdtyz59e3-2{list-style-type:none}.lst-kix_nebp76l47abp-4>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-4,lower-latin) ". "}ul.lst-kix_ngpgdtyz59e3-3{list-style-type:none}.lst-kix_lzjn5gyciksb-5>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-5}ul.lst-kix_ngpgdtyz59e3-4{list-style-type:none}.lst-kix_nebp76l47abp-5>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-5,lower-roman) ". "}.lst-kix_zanxhs2sqm3l-6>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-6,decimal) ". "}.lst-kix_7yi37vgztglf-5>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-5,lower-roman) ". "}ul.lst-kix_ngpgdtyz59e3-5{list-style-type:none}ol.lst-kix_nebp76l47abp-7.start{counter-reset:lst-ctn-kix_nebp76l47abp-7 0}ol.lst-kix_7yi37vgztglf-5.start{counter-reset:lst-ctn-kix_7yi37vgztglf-5 0}.lst-kix_nebp76l47abp-6>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-6,decimal) ". "}.lst-kix_zanxhs2sqm3l-7>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-7,lower-latin) ". "}ul.lst-kix_ngpgdtyz59e3-0{list-style-type:none}.lst-kix_fh2qtwmwsffy-3>li:before{content:"-  "}.lst-kix_7yi37vgztglf-6>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-6,decimal) ". "}ul.lst-kix_ngpgdtyz59e3-1{list-style-type:none}.lst-kix_nebp76l47abp-0>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-0,decimal) ". "}.lst-kix_nebp76l47abp-1>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-1,lower-latin) ". "}.lst-kix_7yi37vgztglf-7>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-7,lower-latin) ". "}ol.lst-kix_ey615mv2rhwi-8.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-8 0}.lst-kix_7yi37vgztglf-8>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-8,lower-roman) ". "}ol.lst-kix_lzjn5gyciksb-2.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-2 0}.lst-kix_nebp76l47abp-3>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-3,decimal) ". "}.lst-kix_ey615mv2rhwi-7>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-7}.lst-kix_fh2qtwmwsffy-6>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-8>li:before{content:"-  "}.lst-kix_nebp76l47abp-2>li:before{content:"" counter(lst-ctn-kix_nebp76l47abp-2,lower-roman) ". "}.lst-kix_fh2qtwmwsffy-7>li:before{content:"-  "}ol.lst-kix_btqaimccbtkv-8.start{counter-reset:lst-ctn-kix_btqaimccbtkv-8 0}.lst-kix_ezs0rqkirfr6-1>li:before{content:"-  "}ol.lst-kix_zanxhs2sqm3l-3{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-4{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-1{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-2{list-style-type:none}.lst-kix_btqaimccbtkv-4>li{counter-increment:lst-ctn-kix_btqaimccbtkv-4}.lst-kix_zanxhs2sqm3l-0>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-0,decimal) ". "}ol.lst-kix_zanxhs2sqm3l-0{list-style-type:none}.lst-kix_ezs0rqkirfr6-0>li:before{content:"-  "}.lst-kix_zanxhs2sqm3l-1>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-1,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-2>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-2,lower-roman) ". "}ul.lst-kix_koo9nzl9bxrj-8{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-6{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-7{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-7{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-8{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-8.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-8 0}ul.lst-kix_koo9nzl9bxrj-4{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-5{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-5{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-6{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-2{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-3{list-style-type:none}.lst-kix_zanxhs2sqm3l-5>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-5,lower-roman) ". "}.lst-kix_3ip0m81ukxx1-7>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-7}ul.lst-kix_koo9nzl9bxrj-0{list-style-type:none}ul.lst-kix_koo9nzl9bxrj-1{list-style-type:none}.lst-kix_zanxhs2sqm3l-3>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-3,decimal) ". "}.lst-kix_zanxhs2sqm3l-4>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-4,lower-latin) ". "}.lst-kix_sut5qe1gw516-0>li:before{content:"-  "}.lst-kix_3ip0m81ukxx1-3>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-3}ol.lst-kix_zanxhs2sqm3l-2.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-2 0}.lst-kix_sut5qe1gw516-1>li:before{content:"-  "}.lst-kix_sut5qe1gw516-2>li:before{content:"-  "}.lst-kix_sut5qe1gw516-3>li:before{content:"-  "}.lst-kix_sut5qe1gw516-4>li:before{content:"-  "}.lst-kix_nebp76l47abp-3>li{counter-increment:lst-ctn-kix_nebp76l47abp-3}.lst-kix_sut5qe1gw516-7>li:before{content:"-  "}.lst-kix_sut5qe1gw516-8>li:before{content:"-  "}.lst-kix_ijcak9j4a5eq-5>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-5}.lst-kix_sut5qe1gw516-5>li:before{content:"-  "}.lst-kix_sut5qe1gw516-6>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-3>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-3}ol.lst-kix_2ss37fysegfq-4.start{counter-reset:lst-ctn-kix_2ss37fysegfq-4 0}.lst-kix_2ss37fysegfq-4>li{counter-increment:lst-ctn-kix_2ss37fysegfq-4}ol.lst-kix_btqaimccbtkv-3.start{counter-reset:lst-ctn-kix_btqaimccbtkv-3 0}ol.lst-kix_5h488x4jtpzl-8{list-style-type:none}ol.lst-kix_5h488x4jtpzl-4{list-style-type:none}ol.lst-kix_5h488x4jtpzl-5{list-style-type:none}ol.lst-kix_5h488x4jtpzl-6{list-style-type:none}.lst-kix_7yi37vgztglf-4>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-4,lower-latin) ". "}ol.lst-kix_5h488x4jtpzl-7{list-style-type:none}ol.lst-kix_5h488x4jtpzl-0{list-style-type:none}ol.lst-kix_y1j9a0umnveg-5.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-5 0}ol.lst-kix_5h488x4jtpzl-1{list-style-type:none}ol.lst-kix_5h488x4jtpzl-2{list-style-type:none}.lst-kix_7yi37vgztglf-3>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-3,decimal) ". "}ol.lst-kix_5h488x4jtpzl-3{list-style-type:none}.lst-kix_btqaimccbtkv-8>li{counter-increment:lst-ctn-kix_btqaimccbtkv-8}.lst-kix_zanxhs2sqm3l-4>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-4}.lst-kix_7yi37vgztglf-2>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-2,lower-roman) ". "}.lst-kix_1cj1mlvvhc19-7>li:before{content:"-  "}.lst-kix_3ip0m81ukxx1-0>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-0}.lst-kix_ijcak9j4a5eq-7>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-7,lower-latin) ". "}.lst-kix_1cj1mlvvhc19-3>li:before{content:"-  "}.lst-kix_zanxhs2sqm3l-8>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-8}ol.lst-kix_nebp76l47abp-0.start{counter-reset:lst-ctn-kix_nebp76l47abp-0 0}.lst-kix_h7mmrf9tmnyi-0>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-0}.lst-kix_1cj1mlvvhc19-5>li:before{content:"-  "}ul.lst-kix_sut5qe1gw516-5{list-style-type:none}ul.lst-kix_sut5qe1gw516-6{list-style-type:none}.lst-kix_7yi37vgztglf-5>li{counter-increment:lst-ctn-kix_7yi37vgztglf-5}ul.lst-kix_sut5qe1gw516-7{list-style-type:none}ul.lst-kix_sut5qe1gw516-8{list-style-type:none}ul.lst-kix_sut5qe1gw516-1{list-style-type:none}ul.lst-kix_sut5qe1gw516-2{list-style-type:none}.lst-kix_ey615mv2rhwi-0>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-0}ul.lst-kix_sut5qe1gw516-3{list-style-type:none}.lst-kix_ngpgdtyz59e3-3>li:before{content:"-  "}.lst-kix_ngpgdtyz59e3-7>li:before{content:"-  "}ul.lst-kix_sut5qe1gw516-4{list-style-type:none}ol.lst-kix_5h488x4jtpzl-3.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-3 0}.lst-kix_ey615mv2rhwi-6>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-6,decimal) ". "}ol.lst-kix_ijcak9j4a5eq-2.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-2 0}.lst-kix_ngpgdtyz59e3-5>li:before{content:"-  "}.lst-kix_2ss37fysegfq-8>li{counter-increment:lst-ctn-kix_2ss37fysegfq-8}ol.lst-kix_y1j9a0umnveg-3.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-3 0}ol.lst-kix_y1j9a0umnveg-0.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-0 0}ol.lst-kix_zanxhs2sqm3l-5.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-5 0}ol.lst-kix_lzjn5gyciksb-4.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-4 0}.lst-kix_ey615mv2rhwi-8>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-8,lower-roman) ". "}ul.lst-kix_ezs0rqkirfr6-0{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-0.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-0 0}ul.lst-kix_ezs0rqkirfr6-1{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-2{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-3{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-4{list-style-type:none}ul.lst-kix_sut5qe1gw516-0{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-5{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-6{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-7{list-style-type:none}ul.lst-kix_ezs0rqkirfr6-8{list-style-type:none}.lst-kix_koo9nzl9bxrj-8>li:before{content:"-  "}ol.lst-kix_7yi37vgztglf-7.start{counter-reset:lst-ctn-kix_7yi37vgztglf-7 0}ol.lst-kix_lzjn5gyciksb-7.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-7 0}ol.lst-kix_nebp76l47abp-2.start{counter-reset:lst-ctn-kix_nebp76l47abp-2 0}.lst-kix_koo9nzl9bxrj-4>li:before{content:"-  "}.lst-kix_koo9nzl9bxrj-6>li:before{content:"-  "}ol.lst-kix_2ss37fysegfq-7.start{counter-reset:lst-ctn-kix_2ss37fysegfq-7 0}.lst-kix_ucr8ik7s2qss-0>li:before{content:"-  "}.lst-kix_y1j9a0umnveg-6>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-6}.lst-kix_ezs0rqkirfr6-7>li:before{content:"-  "}.lst-kix_ucr8ik7s2qss-2>li:before{content:"-  "}.lst-kix_ezs0rqkirfr6-5>li:before{content:"-  "}.lst-kix_ucr8ik7s2qss-4>li:before{content:"-  "}.lst-kix_ezs0rqkirfr6-3>li:before{content:"-  "}.lst-kix_ngpgdtyz59e3-1>li:before{content:"-  "}.lst-kix_5h488x4jtpzl-3>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-3}ol.lst-kix_5h488x4jtpzl-5.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-5 0}.lst-kix_3ip0m81ukxx1-4>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-4,lower-latin) ". "}ol.lst-kix_h7mmrf9tmnyi-2.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-2 0}.lst-kix_3ip0m81ukxx1-2>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-2,lower-roman) ". "}ol.lst-kix_ijcak9j4a5eq-4.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-4 0}.lst-kix_3ip0m81ukxx1-0>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-0,decimal) ". "}.lst-kix_koo9nzl9bxrj-0>li:before{content:"-  "}.lst-kix_ijcak9j4a5eq-3>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-3,decimal) ". "}.lst-kix_ijcak9j4a5eq-5>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-3>li{counter-increment:lst-ctn-kix_2ss37fysegfq-3}ol.lst-kix_btqaimccbtkv-6.start{counter-reset:lst-ctn-kix_btqaimccbtkv-6 0}.lst-kix_ijcak9j4a5eq-1>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-1,lower-latin) ". "}.lst-kix_koo9nzl9bxrj-2>li:before{content:"-  "}.lst-kix_nebp76l47abp-2>li{counter-increment:lst-ctn-kix_nebp76l47abp-2}.lst-kix_3ip0m81ukxx1-6>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-6}.lst-kix_vs3h4bn9qwkh-3>li:before{content:"-  "}.lst-kix_vs3h4bn9qwkh-0>li:before{content:"-  "}.lst-kix_vs3h4bn9qwkh-4>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-8{list-style-type:none}.lst-kix_yc6zqncxu6ij-6>li:before{content:"-  "}.lst-kix_vs3h4bn9qwkh-7>li:before{content:"-  "}ol.lst-kix_lzjn5gyciksb-6{list-style-type:none}.lst-kix_yc6zqncxu6ij-2>li:before{content:"-  "}ul.lst-kix_jz7o59hmr7i0-1{list-style-type:none}ol.lst-kix_lzjn5gyciksb-7{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-0{list-style-type:none}ol.lst-kix_lzjn5gyciksb-4{list-style-type:none}.lst-kix_yc6zqncxu6ij-1>li:before{content:"-  "}ol.lst-kix_btqaimccbtkv-5.start{counter-reset:lst-ctn-kix_btqaimccbtkv-5 0}ul.lst-kix_jz7o59hmr7i0-3{list-style-type:none}ol.lst-kix_lzjn5gyciksb-5{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-2{list-style-type:none}.lst-kix_vs3h4bn9qwkh-8>li:before{content:"-  "}ol.lst-kix_lzjn5gyciksb-2{list-style-type:none}ol.lst-kix_2ss37fysegfq-6.start{counter-reset:lst-ctn-kix_2ss37fysegfq-6 0}ol.lst-kix_lzjn5gyciksb-3{list-style-type:none}ol.lst-kix_lzjn5gyciksb-0{list-style-type:none}ol.lst-kix_lzjn5gyciksb-1{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-7{list-style-type:none}ol.lst-kix_nebp76l47abp-0{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-6{list-style-type:none}.lst-kix_yc6zqncxu6ij-5>li:before{content:"-  "}.lst-kix_3ip0m81ukxx1-5>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-5,lower-roman) ". "}ol.lst-kix_h7mmrf9tmnyi-5{list-style-type:none}ol.lst-kix_nebp76l47abp-2{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-0.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-0 0}ol.lst-kix_h7mmrf9tmnyi-4{list-style-type:none}ol.lst-kix_nebp76l47abp-1{list-style-type:none}ol.lst-kix_y1j9a0umnveg-1.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-1 0}ol.lst-kix_h7mmrf9tmnyi-3{list-style-type:none}.lst-kix_ijcak9j4a5eq-8>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-8}ol.lst-kix_h7mmrf9tmnyi-2{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-1{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-0{list-style-type:none}ul.lst-kix_12ry3tbrjvnx-3{list-style-type:none}.lst-kix_ucr8ik7s2qss-8>li:before{content:"-  "}.lst-kix_7yi37vgztglf-8>li{counter-increment:lst-ctn-kix_7yi37vgztglf-8}.lst-kix_3ip0m81ukxx1-8>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-8,lower-roman) ". "}ol.lst-kix_nebp76l47abp-8{list-style-type:none}.lst-kix_ijcak9j4a5eq-6>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-6}ul.lst-kix_12ry3tbrjvnx-2{list-style-type:none}ol.lst-kix_nebp76l47abp-7{list-style-type:none}ul.lst-kix_12ry3tbrjvnx-5{list-style-type:none}ul.lst-kix_12ry3tbrjvnx-4{list-style-type:none}ol.lst-kix_7yi37vgztglf-8.start{counter-reset:lst-ctn-kix_7yi37vgztglf-8 0}ul.lst-kix_12ry3tbrjvnx-7{list-style-type:none}ol.lst-kix_nebp76l47abp-4{list-style-type:none}ul.lst-kix_12ry3tbrjvnx-6{list-style-type:none}ol.lst-kix_nebp76l47abp-3{list-style-type:none}.lst-kix_ucr8ik7s2qss-5>li:before{content:"-  "}ol.lst-kix_nebp76l47abp-6{list-style-type:none}ul.lst-kix_12ry3tbrjvnx-8{list-style-type:none}ol.lst-kix_nebp76l47abp-5{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-8{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-5{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-4{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-7{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-6{list-style-type:none}ol.lst-kix_nebp76l47abp-4.start{counter-reset:lst-ctn-kix_nebp76l47abp-4 0}.lst-kix_h7mmrf9tmnyi-8>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-8}.lst-kix_lzjn5gyciksb-4>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-4}ol.lst-kix_2ss37fysegfq-0.start{counter-reset:lst-ctn-kix_2ss37fysegfq-0 0}ul.lst-kix_12ry3tbrjvnx-1{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-5.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-5 0}ul.lst-kix_12ry3tbrjvnx-0{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-1.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-1 0}.lst-kix_lzjn5gyciksb-8>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-8}ol.lst-kix_2ss37fysegfq-4{list-style-type:none}ol.lst-kix_nebp76l47abp-5.start{counter-reset:lst-ctn-kix_nebp76l47abp-5 0}ol.lst-kix_2ss37fysegfq-5{list-style-type:none}ol.lst-kix_2ss37fysegfq-6{list-style-type:none}ol.lst-kix_2ss37fysegfq-7{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-6.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-6 0}ol.lst-kix_2ss37fysegfq-0{list-style-type:none}.lst-kix_ey615mv2rhwi-4>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-4}.lst-kix_7yi37vgztglf-6>li{counter-increment:lst-ctn-kix_7yi37vgztglf-6}ol.lst-kix_2ss37fysegfq-1{list-style-type:none}ol.lst-kix_2ss37fysegfq-2{list-style-type:none}ol.lst-kix_2ss37fysegfq-3{list-style-type:none}ol.lst-kix_5h488x4jtpzl-1.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-1 0}ol.lst-kix_2ss37fysegfq-8{list-style-type:none}.lst-kix_gf9vecvc59t-8>li:before{content:"-  "}.lst-kix_ey615mv2rhwi-2>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-2,lower-roman) ". "}.lst-kix_5h488x4jtpzl-4>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-4}.lst-kix_ey615mv2rhwi-1>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-1,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-1>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-1}.lst-kix_gf9vecvc59t-5>li:before{content:"-  "}ol.lst-kix_lzjn5gyciksb-8{list-style-type:none}.lst-kix_gf9vecvc59t-0>li:before{content:"-  "}.lst-kix_gf9vecvc59t-1>li:before{content:"-  "}ol.lst-kix_2ss37fysegfq-5.start{counter-reset:lst-ctn-kix_2ss37fysegfq-5 0}.lst-kix_1cj1mlvvhc19-1>li:before{content:"-  "}ol.lst-kix_5h488x4jtpzl-2.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-2 0}.lst-kix_1cj1mlvvhc19-0>li:before{content:"-  "}.lst-kix_gf9vecvc59t-4>li:before{content:"-  "}ol.lst-kix_3ip0m81ukxx1-0.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-0 0}.lst-kix_5h488x4jtpzl-3>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-3,decimal) ". "}ol.lst-kix_2ss37fysegfq-2.start{counter-reset:lst-ctn-kix_2ss37fysegfq-2 0}.lst-kix_2ss37fysegfq-7>li{counter-increment:lst-ctn-kix_2ss37fysegfq-7}.lst-kix_ijcak9j4a5eq-8>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-8,lower-roman) ". "}.lst-kix_1cj1mlvvhc19-4>li:before{content:"-  "}.lst-kix_1cj1mlvvhc19-8>li:before{content:"-  "}.lst-kix_5h488x4jtpzl-7>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-7,lower-latin) ". "}.lst-kix_y1j9a0umnveg-4>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-4,lower-latin) ". "}ol.lst-kix_3ip0m81ukxx1-1.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-1 0}.lst-kix_ijcak9j4a5eq-1>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-1}.lst-kix_y1j9a0umnveg-0>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-0,decimal) ". "}.lst-kix_5h488x4jtpzl-6>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-6}.lst-kix_ngpgdtyz59e3-6>li:before{content:"-  "}ol.lst-kix_nebp76l47abp-8.start{counter-reset:lst-ctn-kix_nebp76l47abp-8 0}ol.lst-kix_5h488x4jtpzl-0.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-0 0}.lst-kix_nebp76l47abp-4>li{counter-increment:lst-ctn-kix_nebp76l47abp-4}.lst-kix_ey615mv2rhwi-5>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-5>li{counter-increment:lst-ctn-kix_2ss37fysegfq-5}.lst-kix_nebp76l47abp-6>li{counter-increment:lst-ctn-kix_nebp76l47abp-6}.lst-kix_3ip0m81ukxx1-1>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-1}.lst-kix_lzjn5gyciksb-1>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-1}.lst-kix_btqaimccbtkv-1>li{counter-increment:lst-ctn-kix_btqaimccbtkv-1}.lst-kix_y1j9a0umnveg-8>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-8,lower-roman) ". "}ol.lst-kix_btqaimccbtkv-1.start{counter-reset:lst-ctn-kix_btqaimccbtkv-1 0}.lst-kix_zanxhs2sqm3l-6>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-6}ol.lst-kix_3ip0m81ukxx1-3.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-3 0}.lst-kix_ezs0rqkirfr6-8>li:before{content:"-  "}.lst-kix_koo9nzl9bxrj-5>li:before{content:"-  "}.lst-kix_2ss37fysegfq-7>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-7,lower-latin) ". "}.lst-kix_12ry3tbrjvnx-2>li:before{content:"-  "}.lst-kix_ucr8ik7s2qss-1>li:before{content:"-  "}.lst-kix_3ip0m81ukxx1-8>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-8}.lst-kix_ngpgdtyz59e3-2>li:before{content:"-  "}ol.lst-kix_btqaimccbtkv-0.start{counter-reset:lst-ctn-kix_btqaimccbtkv-0 0}.lst-kix_12ry3tbrjvnx-6>li:before{content:"-  "}ol.lst-kix_2ss37fysegfq-1.start{counter-reset:lst-ctn-kix_2ss37fysegfq-1 0}.lst-kix_ezs0rqkirfr6-4>li:before{content:"-  "}.lst-kix_lzjn5gyciksb-6>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-6}.lst-kix_h7mmrf9tmnyi-4>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-4,lower-latin) ". "}ol.lst-kix_3ip0m81ukxx1-4.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-4 0}.lst-kix_btqaimccbtkv-1>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-1,lower-latin) ". "}.lst-kix_btqaimccbtkv-5>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-5,lower-roman) ". "}.lst-kix_h7mmrf9tmnyi-8>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-8,lower-roman) ". "}.lst-kix_2ss37fysegfq-0>li{counter-increment:lst-ctn-kix_2ss37fysegfq-0}.lst-kix_3ip0m81ukxx1-1>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-1,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-3{list-style-type:none}ol.lst-kix_7yi37vgztglf-2{list-style-type:none}ol.lst-kix_7yi37vgztglf-1{list-style-type:none}.lst-kix_ijcak9j4a5eq-0>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-0,decimal) ". "}.lst-kix_ijcak9j4a5eq-4>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-4,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-0{list-style-type:none}.lst-kix_7yi37vgztglf-1>li{counter-increment:lst-ctn-kix_7yi37vgztglf-1}ol.lst-kix_7yi37vgztglf-7{list-style-type:none}.lst-kix_ey615mv2rhwi-2>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-2}ol.lst-kix_7yi37vgztglf-6{list-style-type:none}ol.lst-kix_7yi37vgztglf-5{list-style-type:none}.lst-kix_fh2qtwmwsffy-0>li:before{content:"-  "}ol.lst-kix_7yi37vgztglf-4{list-style-type:none}.lst-kix_koo9nzl9bxrj-1>li:before{content:"-  "}.lst-kix_2ss37fysegfq-3>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-3,decimal) ". "}ol.lst-kix_7yi37vgztglf-8{list-style-type:none}ol.lst-kix_y1j9a0umnveg-4.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-4 0}ol.lst-kix_btqaimccbtkv-2.start{counter-reset:lst-ctn-kix_btqaimccbtkv-2 0}.lst-kix_ijcak9j4a5eq-7>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-7}ol.lst-kix_h7mmrf9tmnyi-1.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-1 0}.lst-kix_zanxhs2sqm3l-2>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-2}.lst-kix_lzjn5gyciksb-0>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-0,decimal) ". "}ol.lst-kix_ijcak9j4a5eq-3.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-3 0}.lst-kix_btqaimccbtkv-6>li{counter-increment:lst-ctn-kix_btqaimccbtkv-6}.lst-kix_3ip0m81ukxx1-5>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-5}.lst-kix_h7mmrf9tmnyi-1>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-1,lower-latin) ". "}.lst-kix_h7mmrf9tmnyi-2>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-2,lower-roman) ". "}ol.lst-kix_zanxhs2sqm3l-1.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-1 0}.lst-kix_lzjn5gyciksb-8>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-8,lower-roman) ". "}ol.lst-kix_btqaimccbtkv-8{list-style-type:none}.lst-kix_lzjn5gyciksb-7>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-7,lower-latin) ". "}ol.lst-kix_btqaimccbtkv-6{list-style-type:none}.lst-kix_12ry3tbrjvnx-8>li:before{content:"-  "}ol.lst-kix_btqaimccbtkv-7{list-style-type:none}ol.lst-kix_btqaimccbtkv-4{list-style-type:none}ol.lst-kix_btqaimccbtkv-5{list-style-type:none}ul.lst-kix_vs3h4bn9qwkh-3{list-style-type:none}ol.lst-kix_lzjn5gyciksb-8.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-8 0}.lst-kix_h7mmrf9tmnyi-0>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-0,decimal) ". "}ul.lst-kix_vs3h4bn9qwkh-4{list-style-type:none}ul.lst-kix_vs3h4bn9qwkh-5{list-style-type:none}ul.lst-kix_vs3h4bn9qwkh-6{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-2.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-2 0}ul.lst-kix_vs3h4bn9qwkh-7{list-style-type:none}.lst-kix_nebp76l47abp-1>li{counter-increment:lst-ctn-kix_nebp76l47abp-1}ol.lst-kix_2ss37fysegfq-3.start{counter-reset:lst-ctn-kix_2ss37fysegfq-3 0}ul.lst-kix_vs3h4bn9qwkh-8{list-style-type:none}.lst-kix_lzjn5gyciksb-1>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-1,lower-latin) ". "}ul.lst-kix_fh2qtwmwsffy-2{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-1{list-style-type:none}.lst-kix_lzjn5gyciksb-2>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-2,lower-roman) ". "}ul.lst-kix_fh2qtwmwsffy-4{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-6.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-6 0}ul.lst-kix_fh2qtwmwsffy-3{list-style-type:none}.lst-kix_lzjn5gyciksb-3>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-3,decimal) ". "}ul.lst-kix_vs3h4bn9qwkh-0{list-style-type:none}ul.lst-kix_vs3h4bn9qwkh-1{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-0{list-style-type:none}ul.lst-kix_vs3h4bn9qwkh-2{list-style-type:none}.lst-kix_2ss37fysegfq-2>li{counter-increment:lst-ctn-kix_2ss37fysegfq-2}.lst-kix_lzjn5gyciksb-5>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-5,lower-roman) ". "}.lst-kix_lzjn5gyciksb-4>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-4,lower-latin) ". "}.lst-kix_lzjn5gyciksb-6>li:before{content:"" counter(lst-ctn-kix_lzjn5gyciksb-6,decimal) ". "}.lst-kix_lzjn5gyciksb-7>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-7}.lst-kix_y1j9a0umnveg-4>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-4}ul.lst-kix_fh2qtwmwsffy-6{list-style-type:none}ol.lst-kix_nebp76l47abp-1.start{counter-reset:lst-ctn-kix_nebp76l47abp-1 0}ul.lst-kix_fh2qtwmwsffy-5{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-8{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-7{list-style-type:none}.lst-kix_h7mmrf9tmnyi-5>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-5}.lst-kix_btqaimccbtkv-2>li{counter-increment:lst-ctn-kix_btqaimccbtkv-2}.lst-kix_7yi37vgztglf-0>li{counter-increment:lst-ctn-kix_7yi37vgztglf-0}.lst-kix_jz7o59hmr7i0-8>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-7>li:before{content:"-  "}ol.lst-kix_btqaimccbtkv-2{list-style-type:none}ol.lst-kix_btqaimccbtkv-3{list-style-type:none}ol.lst-kix_btqaimccbtkv-0{list-style-type:none}ol.lst-kix_btqaimccbtkv-1{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-8.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-8 0}ul.lst-kix_ucr8ik7s2qss-3{list-style-type:none}ul.lst-kix_ucr8ik7s2qss-2{list-style-type:none}.lst-kix_jz7o59hmr7i0-0>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-1>li:before{content:"-  "}ul.lst-kix_ucr8ik7s2qss-1{list-style-type:none}ul.lst-kix_ucr8ik7s2qss-0{list-style-type:none}ul.lst-kix_ucr8ik7s2qss-7{list-style-type:none}ul.lst-kix_ucr8ik7s2qss-6{list-style-type:none}.lst-kix_jz7o59hmr7i0-3>li:before{content:"-  "}ul.lst-kix_ucr8ik7s2qss-5{list-style-type:none}.lst-kix_5h488x4jtpzl-1>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-1}ul.lst-kix_ucr8ik7s2qss-4{list-style-type:none}.lst-kix_jz7o59hmr7i0-4>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-5>li:before{content:"-  "}ul.lst-kix_ucr8ik7s2qss-8{list-style-type:none}.lst-kix_jz7o59hmr7i0-6>li:before{content:"-  "}ol.lst-kix_3ip0m81ukxx1-7.start{counter-reset:lst-ctn-kix_3ip0m81ukxx1-7 0}ol.lst-kix_lzjn5gyciksb-3.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-3 0}ol.lst-kix_7yi37vgztglf-4.start{counter-reset:lst-ctn-kix_7yi37vgztglf-4 0}ul.lst-kix_1cj1mlvvhc19-2{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-3{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-4{list-style-type:none}ol.lst-kix_nebp76l47abp-6.start{counter-reset:lst-ctn-kix_nebp76l47abp-6 0}ul.lst-kix_1cj1mlvvhc19-5{list-style-type:none}.lst-kix_jz7o59hmr7i0-2>li:before{content:"-  "}ul.lst-kix_1cj1mlvvhc19-0{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-1{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-6{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-7{list-style-type:none}ul.lst-kix_1cj1mlvvhc19-8{list-style-type:none}.lst-kix_5h488x4jtpzl-2>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-2,lower-roman) ". "}.lst-kix_5h488x4jtpzl-6>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-6,decimal) ". "}.lst-kix_h7mmrf9tmnyi-1>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-1}.lst-kix_5h488x4jtpzl-0>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-0,decimal) ". "}.lst-kix_5h488x4jtpzl-8>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-8,lower-roman) ". "}ol.lst-kix_ey615mv2rhwi-7.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-7 0}.lst-kix_y1j9a0umnveg-3>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-3,decimal) ". "}.lst-kix_7yi37vgztglf-3>li{counter-increment:lst-ctn-kix_7yi37vgztglf-3}ol.lst-kix_5h488x4jtpzl-6.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-6 0}.lst-kix_y1j9a0umnveg-1>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-1,lower-latin) ". "}.lst-kix_5h488x4jtpzl-4>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-4,lower-latin) ". "}.lst-kix_btqaimccbtkv-8>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-8,lower-roman) ". "}.lst-kix_ijcak9j4a5eq-3>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-3}.lst-kix_y1j9a0umnveg-7>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-7}.lst-kix_y1j9a0umnveg-7>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-7,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-6.start{counter-reset:lst-ctn-kix_7yi37vgztglf-6 0}.lst-kix_y1j9a0umnveg-5>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-5,lower-roman) ". "}ol.lst-kix_zanxhs2sqm3l-8.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-8 0}ol.lst-kix_3ip0m81ukxx1-8{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-6{list-style-type:none}.lst-kix_h7mmrf9tmnyi-2>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-2}ol.lst-kix_3ip0m81ukxx1-7{list-style-type:none}ol.lst-kix_btqaimccbtkv-4.start{counter-reset:lst-ctn-kix_btqaimccbtkv-4 0}ol.lst-kix_3ip0m81ukxx1-4{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-5{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-2{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-3{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-0{list-style-type:none}ol.lst-kix_3ip0m81ukxx1-1{list-style-type:none}.lst-kix_5h488x4jtpzl-8>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-8}ol.lst-kix_h7mmrf9tmnyi-3.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-3 0}ol.lst-kix_btqaimccbtkv-7.start{counter-reset:lst-ctn-kix_btqaimccbtkv-7 0}ol.lst-kix_y1j9a0umnveg-2.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-2 0}ol.lst-kix_y1j9a0umnveg-7{list-style-type:none}ol.lst-kix_y1j9a0umnveg-8{list-style-type:none}.lst-kix_nebp76l47abp-8>li{counter-increment:lst-ctn-kix_nebp76l47abp-8}ol.lst-kix_y1j9a0umnveg-5{list-style-type:none}ol.lst-kix_y1j9a0umnveg-6{list-style-type:none}.lst-kix_ijcak9j4a5eq-4>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-4}ol.lst-kix_y1j9a0umnveg-3{list-style-type:none}ol.lst-kix_5h488x4jtpzl-4.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-4 0}ol.lst-kix_y1j9a0umnveg-4{list-style-type:none}.lst-kix_y1j9a0umnveg-0>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-0}ol.lst-kix_y1j9a0umnveg-1{list-style-type:none}.lst-kix_2ss37fysegfq-8>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-8,lower-roman) ". "}.lst-kix_12ry3tbrjvnx-1>li:before{content:"-  "}ol.lst-kix_y1j9a0umnveg-2{list-style-type:none}ol.lst-kix_y1j9a0umnveg-0{list-style-type:none}.lst-kix_12ry3tbrjvnx-3>li:before{content:"-  "}ol.lst-kix_ijcak9j4a5eq-5.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-5 0}.lst-kix_12ry3tbrjvnx-7>li:before{content:"-  "}ol.lst-kix_zanxhs2sqm3l-6.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-6 0}.lst-kix_btqaimccbtkv-3>li{counter-increment:lst-ctn-kix_btqaimccbtkv-3}.lst-kix_12ry3tbrjvnx-5>li:before{content:"-  "}.lst-kix_lzjn5gyciksb-3>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-3}.lst-kix_h7mmrf9tmnyi-3>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-3,decimal) ". "}.lst-kix_h7mmrf9tmnyi-5>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-5,lower-roman) ". "}.lst-kix_btqaimccbtkv-2>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-2,lower-roman) ". "}.lst-kix_btqaimccbtkv-6>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-6,decimal) ". "}ol.lst-kix_2ss37fysegfq-8.start{counter-reset:lst-ctn-kix_2ss37fysegfq-8 0}.lst-kix_h7mmrf9tmnyi-7>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-7,lower-latin) ". "}.lst-kix_2ss37fysegfq-0>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-0,decimal) ". "}.lst-kix_btqaimccbtkv-4>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-4,lower-latin) ". "}ol.lst-kix_nebp76l47abp-3.start{counter-reset:lst-ctn-kix_nebp76l47abp-3 0}.lst-kix_2ss37fysegfq-6>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-6,decimal) ". "}ol.lst-kix_ey615mv2rhwi-8{list-style-type:none}ol.lst-kix_ey615mv2rhwi-7{list-style-type:none}ol.lst-kix_ey615mv2rhwi-6{list-style-type:none}ol.lst-kix_ey615mv2rhwi-5{list-style-type:none}ol.lst-kix_ey615mv2rhwi-4{list-style-type:none}.lst-kix_2ss37fysegfq-2>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-2,lower-roman) ". "}ol.lst-kix_ey615mv2rhwi-3{list-style-type:none}.lst-kix_7yi37vgztglf-4>li{counter-increment:lst-ctn-kix_7yi37vgztglf-4}ol.lst-kix_ey615mv2rhwi-2{list-style-type:none}ol.lst-kix_ey615mv2rhwi-1{list-style-type:none}.lst-kix_ey615mv2rhwi-5>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-5}ol.lst-kix_ey615mv2rhwi-0{list-style-type:none}.lst-kix_2ss37fysegfq-4>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-4,lower-latin) ". "}ol.lst-kix_lzjn5gyciksb-6.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-6 0}.lst-kix_btqaimccbtkv-0>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-0,decimal) ". "}.lst-kix_btqaimccbtkv-5>li{counter-increment:lst-ctn-kix_btqaimccbtkv-5}.lst-kix_vs3h4bn9qwkh-1>li:before{content:"-  "}.lst-kix_2ss37fysegfq-1>li{counter-increment:lst-ctn-kix_2ss37fysegfq-1}ol.lst-kix_7yi37vgztglf-2.start{counter-reset:lst-ctn-kix_7yi37vgztglf-2 0}.lst-kix_vs3h4bn9qwkh-6>li:before{content:"-  "}.lst-kix_vs3h4bn9qwkh-5>li:before{content:"-  "}.lst-kix_yc6zqncxu6ij-7>li:before{content:"-  "}ol.lst-kix_y1j9a0umnveg-7.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-7 0}.lst-kix_yc6zqncxu6ij-8>li:before{content:"-  "}ol.lst-kix_zanxhs2sqm3l-4.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-4 0}ol.lst-kix_ijcak9j4a5eq-6.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-6 0}.lst-kix_zanxhs2sqm3l-3>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-3}.lst-kix_y1j9a0umnveg-3>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-3}.lst-kix_yc6zqncxu6ij-3>li:before{content:"-  "}.lst-kix_nebp76l47abp-0>li{counter-increment:lst-ctn-kix_nebp76l47abp-0}ol.lst-kix_lzjn5gyciksb-5.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-5 0}.lst-kix_5h488x4jtpzl-0>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-0}ol.lst-kix_ey615mv2rhwi-0.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-0 0}.lst-kix_yc6zqncxu6ij-4>li:before{content:"-  "}.lst-kix_3ip0m81ukxx1-6>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-6,decimal) ". "}.lst-kix_ey615mv2rhwi-6>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-6}.lst-kix_y1j9a0umnveg-1>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-1}.lst-kix_3ip0m81ukxx1-7>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-7,lower-latin) ". "}.lst-kix_ucr8ik7s2qss-6>li:before{content:"-  "}.lst-kix_ucr8ik7s2qss-7>li:before{content:"-  "}ol.lst-kix_ijcak9j4a5eq-1{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-0{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-3{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-2{list-style-type:none}.lst-kix_5h488x4jtpzl-2>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-2}ol.lst-kix_ijcak9j4a5eq-5{list-style-type:none}.lst-kix_3ip0m81ukxx1-4>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-4}ol.lst-kix_ijcak9j4a5eq-4{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-7{list-style-type:none}ol.lst-kix_5h488x4jtpzl-7.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-7 0}ol.lst-kix_ijcak9j4a5eq-6{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-8{list-style-type:none}.lst-kix_y1j9a0umnveg-5>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-5}ol.lst-kix_5h488x4jtpzl-8.start{counter-reset:lst-ctn-kix_5h488x4jtpzl-8 0}ol.lst-kix_y1j9a0umnveg-6.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-6 0}.lst-kix_lzjn5gyciksb-2>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-2}.lst-kix_nebp76l47abp-7>li{counter-increment:lst-ctn-kix_nebp76l47abp-7}.lst-kix_ey615mv2rhwi-4>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-4,lower-latin) ". "}.lst-kix_ey615mv2rhwi-3>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-3,decimal) ". "}ol.lst-kix_7yi37vgztglf-3.start{counter-reset:lst-ctn-kix_7yi37vgztglf-3 0}ol.lst-kix_ey615mv2rhwi-5.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-5 0}ol.lst-kix_ijcak9j4a5eq-7.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-7 0}.lst-kix_gf9vecvc59t-7>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-4.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-4 0}.lst-kix_h7mmrf9tmnyi-6>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-6}ol.lst-kix_zanxhs2sqm3l-3.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-3 0}.lst-kix_gf9vecvc59t-6>li:before{content:"-  "}.lst-kix_ey615mv2rhwi-0>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-0,decimal) ". "}.lst-kix_5h488x4jtpzl-7>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-7}ul.lst-kix_gf9vecvc59t-8{list-style-type:none}ol.lst-kix_ey615mv2rhwi-6.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-6 0}ul.lst-kix_gf9vecvc59t-0{list-style-type:none}ul.lst-kix_gf9vecvc59t-1{list-style-type:none}ul.lst-kix_gf9vecvc59t-2{list-style-type:none}ul.lst-kix_gf9vecvc59t-3{list-style-type:none}ul.lst-kix_gf9vecvc59t-4{list-style-type:none}.lst-kix_gf9vecvc59t-2>li:before{content:"-  "}.lst-kix_gf9vecvc59t-3>li:before{content:"-  "}ul.lst-kix_gf9vecvc59t-5{list-style-type:none}.lst-kix_zanxhs2sqm3l-7>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-7}ul.lst-kix_gf9vecvc59t-6{list-style-type:none}.lst-kix_vs3h4bn9qwkh-2>li:before{content:"-  "}ul.lst-kix_gf9vecvc59t-7{list-style-type:none}ol.lst-kix_ey615mv2rhwi-4.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-4 0}.lst-kix_5h488x4jtpzl-1>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-1,lower-latin) ". "}.lst-kix_5h488x4jtpzl-5>li:before{content:"" counter(lst-ctn-kix_5h488x4jtpzl-5,lower-roman) ". "}ol.lst-kix_zanxhs2sqm3l-0.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-0 0}.lst-kix_ijcak9j4a5eq-2>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-2}ol.lst-kix_h7mmrf9tmnyi-5.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-5 0}.lst-kix_y1j9a0umnveg-8>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-8}.lst-kix_ijcak9j4a5eq-6>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-6,decimal) ". "}.lst-kix_1cj1mlvvhc19-2>li:before{content:"-  "}ul.lst-kix_yc6zqncxu6ij-7{list-style-type:none}.lst-kix_ey615mv2rhwi-1>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-1}ul.lst-kix_yc6zqncxu6ij-8{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-1{list-style-type:none}.lst-kix_btqaimccbtkv-7>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-7,lower-latin) ". "}ul.lst-kix_yc6zqncxu6ij-2{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-0{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-5{list-style-type:none}.lst-kix_y1j9a0umnveg-2>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-2,lower-roman) ". "}ul.lst-kix_yc6zqncxu6ij-6{list-style-type:none}.lst-kix_btqaimccbtkv-0>li{counter-increment:lst-ctn-kix_btqaimccbtkv-0}ul.lst-kix_yc6zqncxu6ij-3{list-style-type:none}.lst-kix_1cj1mlvvhc19-6>li:before{content:"-  "}ul.lst-kix_yc6zqncxu6ij-4{list-style-type:none}.lst-kix_ijcak9j4a5eq-0>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-0}.lst-kix_ey615mv2rhwi-3>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-3}.lst-kix_ngpgdtyz59e3-4>li:before{content:"-  "}.lst-kix_ngpgdtyz59e3-8>li:before{content:"-  "}.lst-kix_7yi37vgztglf-2>li{counter-increment:lst-ctn-kix_7yi37vgztglf-2}.lst-kix_y1j9a0umnveg-6>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-6,decimal) ". "}.lst-kix_ey615mv2rhwi-7>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-7,lower-latin) ". "}ol.lst-kix_ey615mv2rhwi-1.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-1 0}.lst-kix_5h488x4jtpzl-5>li{counter-increment:lst-ctn-kix_5h488x4jtpzl-5}ol.lst-kix_lzjn5gyciksb-1.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-1 0}.lst-kix_h7mmrf9tmnyi-4>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-4}ol.lst-kix_lzjn5gyciksb-0.start{counter-reset:lst-ctn-kix_lzjn5gyciksb-0 0}.lst-kix_koo9nzl9bxrj-7>li:before{content:"-  "}ol.lst-kix_7yi37vgztglf-0.start{counter-reset:lst-ctn-kix_7yi37vgztglf-0 0}ol.lst-kix_ey615mv2rhwi-2.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-2 0}.lst-kix_zanxhs2sqm3l-0>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-0}.lst-kix_zanxhs2sqm3l-5>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-5}.lst-kix_ezs0rqkirfr6-6>li:before{content:"-  "}.lst-kix_btqaimccbtkv-7>li{counter-increment:lst-ctn-kix_btqaimccbtkv-7}.lst-kix_ngpgdtyz59e3-0>li:before{content:"-  "}.lst-kix_12ry3tbrjvnx-4>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-7.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-7 0}.lst-kix_3ip0m81ukxx1-2>li{counter-increment:lst-ctn-kix_3ip0m81ukxx1-2}.lst-kix_ezs0rqkirfr6-2>li:before{content:"-  "}.lst-kix_ucr8ik7s2qss-3>li:before{content:"-  "}.lst-kix_btqaimccbtkv-3>li:before{content:"" counter(lst-ctn-kix_btqaimccbtkv-3,decimal) ". "}ol.lst-kix_7yi37vgztglf-1.start{counter-reset:lst-ctn-kix_7yi37vgztglf-1 0}.lst-kix_2ss37fysegfq-1>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-1,lower-latin) ". "}.lst-kix_3ip0m81ukxx1-3>li:before{content:"" counter(lst-ctn-kix_3ip0m81ukxx1-3,decimal) ". "}ol.lst-kix_y1j9a0umnveg-8.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-8 0}.lst-kix_yc6zqncxu6ij-0>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-6>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-6,decimal) ". "}.lst-kix_lzjn5gyciksb-0>li{counter-increment:lst-ctn-kix_lzjn5gyciksb-0}ol.lst-kix_ey615mv2rhwi-3.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-3 0}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_12ry3tbrjvnx-0>li:before{content:"-  "}.lst-kix_7yi37vgztglf-7>li{counter-increment:lst-ctn-kix_7yi37vgztglf-7}.lst-kix_koo9nzl9bxrj-3>li:before{content:"-  "}.lst-kix_2ss37fysegfq-5>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-6>li{counter-increment:lst-ctn-kix_2ss37fysegfq-6}.lst-kix_ey615mv2rhwi-8>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-8}ol.lst-kix_h7mmrf9tmnyi-8.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-8 0}.lst-kix_nebp76l47abp-5>li{counter-increment:lst-ctn-kix_nebp76l47abp-5}.lst-kix_ijcak9j4a5eq-2>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-2,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{background-color:#ffffff;margin-left:15pt;padding-top:6pt;padding-bottom:9pt;line-height:0.9128347826086957;orphans:2;widows:2;text-align:left}.c9{background-color:#ffffff;padding-top:6pt;padding-bottom:9pt;line-height:0.9128347826086957;orphans:2;widows:2;text-align:left;height:20pt}.c13{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman"}.c12{background-color:#f8f9fa;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:1pt;font-family:"Courier New";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c23{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c4{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c10{color:#cc4125;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c11{color:#45818e;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman"}.c39{background-color:#ffffff;padding-top:6pt;padding-bottom:9pt;line-height:0.9128347826086957;orphans:2;widows:2;text-align:left}.c20{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:italic}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c32{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Times New Roman"}.c17{color:#b5739d;text-decoration:none;vertical-align:baseline;font-style:italic}.c35{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c37{font-weight:700;font-size:14pt;font-family:"Times New Roman";font-style:normal}.c27{color:#e69138;text-decoration:none;vertical-align:baseline}.c34{color:#000000;text-decoration:none;vertical-align:baseline}.c28{text-decoration:none;vertical-align:baseline;font-style:normal}.c19{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c18{font-size:16pt;font-family:"Times New Roman";font-weight:400}.c16{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c25{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c31{color:#cc4125;text-decoration:none;vertical-align:baseline}.c38{font-size:16pt;font-family:"Times New Roman";font-weight:700}.c24{color:#4a86e8;text-decoration:none;vertical-align:baseline}.c22{padding:0;margin:0}.c3{margin-left:36pt;padding-left:0pt}.c8{color:inherit;text-decoration:inherit}.c26{color:#45818e}.c33{color:#4a86e8}.c21{font-style:italic}.c30{margin-left:36pt}.c29{color:#6aa84f}.c1{height:11pt}.c36{vertical-align:sub}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25 doc-content"><p class="c0"><span class="c21 c32">CNN Architectures (Under progress)</span></p><p class="c0 c1"><span class="c4"></span></p><hr><p class="c0 c1"><span class="c4"></span></p><p class="c0 c1"><span class="c4"></span></p><p class="c0 c30 c1"><span class="c5"></span></p><p class="c0"><span class="c19">In the last decade lots of</span><span class="c5">&nbsp;neural network architectures have been proposed. Every year new architectures are getting attention and eventually suppress the older ones. I will try to go through the most significant architectures till today (2022) in the below section.</span></p><ol class="c22 lst-kix_ey615mv2rhwi-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.70oagfdzamdd">Initial Convolutional Neural Networks </a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.7cjl426tc84g">VGG16 </a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.usmwggyeg7l">GoogleNet / Inception</a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.j2c4fqjlutpf">ResNet Family </a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.6ag6q5b1z3xm">Wide ResNet (WRN)</a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.jzk9bc3j1ill">EfficientNet </a></span></li><li class="c0 c3 li-bullet-0"><span class="c14"><a class="c8" href="#id.2wgyjv3c92i6">Refference</a></span></li></ol><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c16">Convolution Filter :</span><span class="c19">&nbsp;In the figure below </span><span class="c5">we have a 10x10 sized image array and a 3x3 array of numeric values ( 3x3 kernel/filter). The 3x3 matrix is convolved through the entire image from left-right row by row (the zigzag line in the image below shows the path). At every pixel location the 9 numeric numbers of the 3x3 kernel will overlap with a similar 3x3 array (patch of image) from the 10x10 image. This 3x3 kernel and 3x3 patch from the image is multiplied pointwise. The resultant 3x3 matrix flattened and the 9 numeric values are summed algebraically to produce a single pixel value of the output image. Once the whole image is covered, an output image is formed with a slightly smaller size. For a 3x3 kernel the image height and width should shrink by 2 rows and 2 columns i.e 8x8 which can be understood by observing the coverage of the zigzag path below.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1010.50px; height: 249.66px;"><img alt="" src="files/cnn/image14.png" style="width: 1010.50px; height: 249.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c6">Figure : Convolution filter.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">Convolution filters are capable of producing image features like edges, corners etc based on the weight parameters of the filter (or kernel). A combination of these filters can detect even more advanced image features like human eye , noose etc. In convolutional layers the filters are initialized with random numeric weights which are updated with back propagation for learning meaningful features. Some examples of filters are shown below -</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 775.51px; height: 173.43px;"><img alt="" src="files/cnn/image5.png" style="width: 775.51px; height: 173.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c12"></span></p><p class="c0 c1"><span class="c12"></span></p><p class="c0 c1"><span class="c12"></span></p><p class="c0"><span class="c6">Convolution Layer :</span></p><p class="c0"><span class="c5">Convolution layer consists of a combination of convolutional filters. For the below figure the RGB image with 3 channels goes through a &ldquo;convolution layer&rdquo; that produces a 4 channel output feature map. The convolution layer has four separate filters for producing each of the four output feature map channels. For pointwise multiplication (at every pixel location) a convolutional filter requires an equal number of channels with the input image. That&rsquo;s why each filter unit &nbsp;here &nbsp;has 3 channels like the input RGB image. </span></p><p class="c0"><span class="c5">The 3x3x3 filters are convolved over the entire image like the above example. At each pixel location total 3x3x3 = 27 numeric values of one convolution filter and RGB image are element wise multiplied. The 27 numeric values are summed all together for producing one pixel value of the corresponding output channel.</span></p><p class="c0"><span class="c5">&nbsp;3 convolution filters are applied on the 3 input channels of the images all together respectively </span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 744.83px; height: 390.06px;"><img alt="" src="files/cnn/image17.png" style="width: 744.83px; height: 390.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6">Figure : A simple convolution layer.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c38">Initial </span><a id="id.70oagfdzamdd"></a><span class="c38">Convolutional Neural Networks</span><span class="c23">&nbsp;(1998-2012)</span></p><hr><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c6">(i) LeNet &nbsp;(1998) &nbsp;</span></p><p class="c0"><span class="c13"><a class="c8" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/publis/index.html%23lecun-98&amp;sa=D&amp;source=editors&amp;ust=1673533103466333&amp;usg=AOvVaw0FIoqN_ugUwJACbuMzxAYv">Gradient-Based Learning Applied to Document Recognition</a></span></p><p class="c0"><span class="c5">It is the first &nbsp;successful convolutional neural network architecture which was applied to document classification by LeCun at el. &nbsp;A dense neural network can process tabular data i.e 1d feature vectors. Earlier, multilayer neural networks could be used for image classification by converting the images feature vectors. Feature extraction algorithms e.g SIFT, SURF etc were applied on the raw 2D/3D image array for fetching the keypoint descriptors which eventually transformed into feature vectors of fixed size. Then these feature vectors could be passed through a &nbsp;dense neural network for classification.</span></p><p class="c0"><span class="c5">&nbsp; &nbsp; &nbsp;LeNet first introduced the multiple consecutive convolutional filter based neural network where the weights of the filter are learnt by back propagation. Convolutional filters can be directly applied on 2D/3D image data. These layers consisting of a group of convolution filters are called convolutional layers. The combination of convolutional filters all together act as complex but powerful feature extractor. The convolutional layers can be initialized with random numeric values learnt by back propagation automatically. It also gives the algorithm liberty to learn practically any combination of suitable filters necessary for the task in hand.</span></p><p class="c0 c1 c30"><span class="c5"></span></p><p class="c0 c30"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 829.37px; height: 227.44px;"><img alt="" src="files/cnn/image6.png" style="width: 829.37px; height: 227.44px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c30"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c6">&nbsp;Image taken from LeCun at el. 1998.</span></p><p class="c0 c30 c1"><span class="c5"></span></p><p class="c0"><span class="c6">(ii) AlexNet (2012) &nbsp;</span></p><p class="c0"><span class="c16">&nbsp;</span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103467558&amp;usg=AOvVaw0hzl812cehshVXN9C1ebeP">ImageNet Classification with Deep Convolutional Neural Networks</a></span><span class="c5">. </span></p><p class="c0"><span class="c5">It introduced ReLU instead of TanH activation within the architecture. ReLU can converge much faster than TanH. &nbsp;Even today ReLU is the most popular activation function among the practitioners. &nbsp;AlexNet also used dropout for preventing overfitting in their training steps. AexNet is the CNN layer based network which started to get attention after LeNet.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c30 c1"><span class="c4"></span></p><a id="id.7cjl426tc84g"></a><p class="c0"><span class="c20">VGG16 &nbsp;(2014)</span></p><hr><p class="c0 c1"><span class="c16 c21 c27"></span></p><p class="c0 c1"><span class="c27 c16 c21"></span></p><p class="c0"><span class="c13"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1409.1556&amp;sa=D&amp;source=editors&amp;ust=1673533103468338&amp;usg=AOvVaw1sdAu3llxaq7cZZ2tLIbrh">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></span><span class="c19">&nbsp;</span></p><p class="c0"><span class="c19">&nbsp;So far the CNN networks used to be very small compared to current practice. VGG16 was the first Deep CNN architecture which demonstrated outstanding accuracy compared to contemporary smaller CNN networks. First successful </span><span class="c16">Deep</span><span class="c5">&nbsp;Convolutional Neural Network. Their main contribution was pushing the depth of the CNN architecture for increasing accuracy. They pushed &nbsp;the depth up to 16-19 layers while using 3x3 kernels for better accuracy. In the ImageNet 2014 challenge they achieved 1st in localization and 2nd in classification criterion.</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 861.20px; height: 518.48px;"><img alt="" src="files/cnn/image27.png" style="width: 861.20px; height: 518.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6">Figure : LeNet, AlexNet, VGG16 architecture.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><a id="id.usmwggyeg7l"></a><p class="c0"><span class="c20">GoogleNet / Inception (2015)</span></p><hr><p class="c0 c1"><span class="c16 c17"></span></p><p class="c0 c1"><span class="c17 c16"></span></p><p class="c0"><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.4842.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103469617&amp;usg=AOvVaw0t9C15RMSAkx6qmHKxd79e">Going deeper with convolutions</a></span><span class="c5">.</span></p><p class="c0"><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1512.00567.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103469888&amp;usg=AOvVaw1OfjacTQOTREzI_NRaZ4hA">Rethinking the Inception Architecture for Computer Vision</a></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c19">The inception modules are a combination of several convolutional modules of different kernel size</span><span class="c5">. They are so designed that at every layer of the network it can look at the activation map with different receptive fields discovering features at multiple scales. </span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">Inception has tried to approximate optimal sparse local representation &nbsp;of convolutional neural networks which is then repeated through depth. These sparse representations are nothing but the inception modules. They were very much motivated by aura at el [] where they explored to find analogy between biological neurons and nodes in the neural network. The analogy was briefly as follows - the neurons fire together wire together - in one layer of a network if two or more nodes fire together (shows high activation for some specific inputs) then they are very likely to be connected in the deeper layers of the network with good positive weights. So they were optimistic about connection between the similar convolutional modules repeated through the network.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">Since they designed the network in a sparse style making the inception modules wider with a group convolutional modules. A large kernel i.e 5x5 would be very costly at the deeper layers with a lot of channels. For tackling this issue they used 1x1 convolutions a lot in their inception module. Note that 1x1 convolution / Network in Network [1] has its own benefits as well which also made the inception module more effective.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c19">The activations in the 1x1 convolutional also added non-linearity to the model. It is also an auxiliary classifier which helps to maintain the gradients at the deeper layers and helps to regularize the weights. </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1312.4400.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103470748&amp;usg=AOvVaw3noB4qUgiR2FfuZk-9s3Mg">Network inside Network</a></span><span class="c5">&nbsp; is a concept of using a 1x1 convolution layer as a dense connector between feature maps. A 1x1 convolution can effectively be used for changing the output depth of convolutional layers. It has another useful effect. It operates like a dense layer applied at every pixel location depthwise. In the below example the 1x1 convolutional is working like a dense layer that takes the 5 input values (from 5 input channels) at each spatial location of the input activations and results 3 output values for the same spatial location of the 3 output channels. This makes it possible to work exactly like a dense layer for the stacked 2d arrays of data.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 903.16px; height: 705.46px;"><img alt="" src="files/cnn/image24.png" style="width: 903.16px; height: 705.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 844.50px; height: 596.79px;"><img alt="" src="files/cnn/image21.png" style="width: 844.50px; height: 596.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><a id="id.gaddsj7hsv9m"></a><p class="c0"><span class="c23">Xception </span></p><p class="c0 c1"><span class="c15"></span></p><p class="c0"><span class="c35 c18"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1610.02357.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103471823&amp;usg=AOvVaw04SI8Si0idNTg7jNyBih0D">Xception: Deep Learning with Depthwise Separable Convolutions</a></span><span class="c15">&nbsp;(2017)</span></p><p class="c0 c1"><span class="c15"></span></p><p class="c0"><span class="c18">Xception is based on an idea that inception can be interpreted as the bridge between regular and depthwise convolution. Based on this hypothesis they proposed a reasonably simplified version of the inception module (Figure 2 from the original paper attached below). An equivalent of the simplified version (Figure 3 from the original paper attached below). Finally they proposed they extreme version built with depthwise convolution namely the Xception block (Figure 4 from the original paper attached below).</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 465.00px; height: 518.00px;"><img alt="" src="files/cnn/image28.png" style="width: 465.00px; height: 518.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 422.00px; height: 526.00px;"><img alt="" src="files/cnn/image22.png" style="width: 422.00px; height: 526.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><a id="id.j2c4fqjlutpf"></a><p class="c0"><span class="c20">ResNet Family (2015)</span></p><hr><p class="c0 c1"><span class="c16 c24 c21"></span></p><p class="c0 c1"><span class="c16 c21 c24"></span></p><p class="c0"><span class="c19 c33 c21">(i) </span><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1512.03385.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103472686&amp;usg=AOvVaw0Ufa0tvfuE8AVHHpsTXisq">Deep Residual Learning for Image Recognition</a></span></p><p class="c0"><span class="c19 c33 c21">(ii) </span><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1603.05027.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103472949&amp;usg=AOvVaw1_j147C_2LsvNYl7cVUSrO">Identity Mappings in Deep Residual Networks</a></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c6">Vanishing gradient Problem </span></p><p class="c0"><span class="c5">&nbsp;The deeper the network goes the accuracy should be higher or at least equal to the less deep network. It seemed to be a little counter-intuitive that after going deeper than a certain level the accuracy started decreasing. The reason is claimed to be a vanishing gradient. After backpropagating through so many layers the value of gradients starts getting very smaller that practically they go near zero. It Causes the training loss to &nbsp;decrease at a very slow rate. Note that although batch normalization helps to maintain a healthy norm of activations thus gradients throughout the network still that was not enough to handle this issue.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c6">Identity Connection </span></p><p class="c0"><span class="c5">&nbsp;Residual net introduced identity connection / skip connection. It basically added the activation outputs of the i layer to i+1 layer. It continues through the entire length of the network creating a shortcut for upper layer activations to go deep inside the network. During back propagation it makes it possible for the very deep layers to have gradients with reasonable magnitude for making the learning process dynamic till the end. This kind of passage for gradients was introduced in Highway[] earlier.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c19">H(x) = F(x,{W</span><span class="c19 c36">i</span><span class="c19">}) + x{W</span><span class="c19 c36">s</span><span class="c5">}</span></p><p class="c0"><span class="c19">F(x,{W</span><span class="c19 c36">i</span><span class="c5">}) &nbsp;= H(x) - x</span></p><p class="c0"><span class="c5">Ws : Used for dimension matching before addition.</span></p><p class="c0"><span class="c5">For input of 256 and output 64 shape of Ws : (64,256)</span></p><p class="c0"><span class="c19">There are 3 options for use W</span><span class="c19 c36">s</span><span class="c5">&nbsp;by keeping the value of x as it is just altering the dimensions :</span></p><p class="c0"><span class="c5">&nbsp; &nbsp;1. Const value (all ones)</span></p><p class="c0"><span class="c5">&nbsp; &nbsp;2. Zero padding.</span></p><p class="c0"><span class="c5">&nbsp; &nbsp;3. 1x1 convolution to reduce/increase the depth only.</span></p><p class="c0"><span class="c5">&nbsp; &nbsp;** 1x1 is also like network in network since it takes features from each input channel, multiplies with weights and produces points for each output channel.</span></p><p class="c0"><span class="c5">yy</span></p><p class="c0"><span class="c5">Conv2D(k x k x d, s) here k = kernel size, d output depth, s = stride</span></p><p class="c0"><span class="c5">BN &nbsp; &nbsp; = Batch Normalization</span></p><p class="c0"><span class="c5">ReLU = ReLU Activation</span></p><p class="c0"><span class="c5">ReLU and BN are illustrated with icon at some places just to emphasize </span></p><p class="c0"><span class="c5">their presence. &nbsp;Icon and text (BN / ReLU) are interchangeable everywhere.</span></p><p class="c0"><span class="c5">RB(a,b,c,s)</span></p><p class="c0"><span class="c5">&nbsp;a= input depth , b compressed depth, c = output depth, s = stride of last convolution layer</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 965.50px; height: 556.98px;"><img alt="" src="files/cnn/image11.png" style="width: 965.50px; height: 556.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1007.50px; height: 250.00px;"><img alt="" src="files/cnn/image19.png" style="width: 1007.50px; height: 250.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><a id="id.2jt4ykkt67v9"></a><p class="c0"><span class="c20">ResNetXt</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103476130&amp;usg=AOvVaw3_-Of3WK0Af6BJGD55iyLO">Aggregated Residual Transformations for Deep Neural Networks Saining</a></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">ResNetXt adds another dimension to the residual blocks. &nbsp;</span></p><p class="c0"><span class="c5">In a &nbsp;ResNet block </span></p><ul class="c22 lst-kix_12ry3tbrjvnx-0 start"><li class="c0 c3 li-bullet-0"><span class="c5">256 channel inputs are passed through a 1x1 convolution network and gets compressed to 64 channels. </span></li><li class="c0 c3 li-bullet-0"><span class="c5">The 3x3 convolution is applied to this 64 dimensional data.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Finally expanded to 256 dimensions with another 1x1 CNN layer.</span></li></ul><p class="c0"><span class="c5">In ResNetXt &nbsp;</span></p><ul class="c22 lst-kix_ucr8ik7s2qss-0 start"><li class="c0 c3 li-bullet-0"><span class="c5">Instead of single 1x1 convolution a total N number of separate 1x1 convolution layers are applied simultaneously. Each of these 1x1 CNN compresses the 256d data into a much smaller dimension i.e 4d . N (N=32 below) is denoted as the cardinality of the block.</span></li></ul><ul class="c22 lst-kix_ezs0rqkirfr6-0 start"><li class="c0 c3 li-bullet-0"><span class="c5">All these N no of 4d outputs go through 3x3 CNN layers.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Finally the 4d outputs are added together at the end of this block.</span></li></ul><p class="c0"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 558.50px; height: 271.43px;"><img alt="" src="files/cnn/image3.png" style="width: 558.50px; height: 271.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><a id="id.sbskz5eotl4r"></a><p class="c0"><span class="c23">MobileNet :</span></p><p class="c0 c1"><span class="c11"></span></p><hr><p class="c0 c1"><span class="c11"></span></p><p class="c0 c1"><span class="c11"></span></p><p class="c0 c30"><span class="c16 c26">(i) </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1704.04861.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103477554&amp;usg=AOvVaw1oVVIpQI_NDbqbnNgipfLN">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></span><span class="c19 c26">&nbsp;(2017)</span></p><p class="c0 c30"><span class="c19 c26">(ii) </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1801.04381.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103477828&amp;usg=AOvVaw3s7cU5WP8Pbs4xjQCgH0R8">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></span><span class="c19">&nbsp;(2018)</span></p><p class="c0"><span class="c6">Standard Convolution :</span></p><p class="c0"><span class="c19">&nbsp;A standard convolution operation with kernel size 3x3 and an input featuremap with size </span><span class="c19">HxWxC</span><span class="c5">. A standard convolution operation will multiply the input featuremap with a 3x3xC array of weights at every pixel location. It will then pass through the entire image in a sliding window manner to produce the output feature map. The sliding window works by moving the kernel by one pixel (for stride = 1) from left to right for each row of the pixel array one by one. For stride (s) greater than 1 it will move by &#39;s&#39; number of pixels instead of shifting just one pixel location. &nbsp;For an input image with 3 channels (C =3) it will be a 3x3x3 (the blue,green and red mask 3x3 kernel in the figure) array which will be element wise multiplied with the respective pixels input image array. The 27 values then summed up to produce one pixel of the output feature map. The described calculation is required just to produce one channel of the output feature map. For N channel output similar computation is repeated N times.</span></p><p class="c0"><span class="c5">So for a kernel size k</span></p><p class="c0"><span class="c19">input size &nbsp; = </span><span class="c19">HxWxC</span></p><p class="c0"><span class="c19">output size = </span><span class="c19">HxWxN</span><span class="c5">&nbsp;(considering the input is zero padded to keep output size same as input)</span></p><p class="c0"><span class="c5">At each pixel location KxKxC sized weight array is needed for producing one output channel. For N output channel N number of KxKxC parameters are needed.</span></p><p class="c0"><span class="c5">No of parameters = CxNxKxK</span></p><p class="c0"><span class="c5">This computation occurs at every pixel location of the input featuremap. </span></p><p class="c0"><span class="c5">So total computation cost = HxW x CxNxKxK </span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c6">DepthWise Convolution :</span></p><p class="c0"><span class="c5">For depthwise convolution the separate 3x3 kernel is applied to each of the input channels. It will produce an array with the same dimension (with zero padding) of the input. Now at every pixel location a pointwise 1x1 convolution is applied to produce the desired number of output channels. It is explained in the Network In Network concept earlier.</span></p><p class="c0"><span class="c5">So for , kernel size k</span></p><p class="c0"><span class="c19">input size &nbsp; = </span><span class="c19">HxWxC</span></p><p class="c0"><span class="c19">output size = </span><span class="c19">HxWxN</span><span class="c5">&nbsp;(considering the input is zero padded to keep output size same as input)</span></p><p class="c0"><span class="c5">At each pixel location KxKxC sized weight array is needed for producing one output same as input feature map (with padding). For producing the N output channels a weight matrix of MxN is multiplied with this tensor.</span></p><p class="c0"><span class="c5">No of parameters = CxKxK + CxN</span></p><p class="c0"><span class="c5">This computation occurs at every pixel location of the input featuremap. </span></p><p class="c0"><span class="c5">So total computation cost = HxW x ( CxKxK + C x N) </span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">CxNxKxK --&gt; CxKxK + CxN</span></p><p class="c0"><span class="c5">&nbsp;</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c19">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 869.00px; height: 723.20px;"><img alt="" src="files/cnn/image8.png" style="width: 869.00px; height: 723.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c5"></span></p><a id="id.6ag6q5b1z3xm"></a><p class="c0"><span class="c38">Wide ResNet &nbsp;(WRN) </span></p><hr><p class="c0 c1"><span class="c28 c16 c29"></span></p><p class="c0 c1"><span class="c28 c16 c29"></span></p><h1 class="c39" id="h.8p6vc955bvuf"><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1605.07146.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103480029&amp;usg=AOvVaw1ptqv0SH9_s63vqQrjFDXI">Wide Residual Networks</a></span><span class="c19">&nbsp;</span><span class="c6">(2016)</span></h1><h1 class="c9" id="h.a7e0j77hcr31"><span class="c5"></span></h1><p class="c0"><span class="c6">Network Scaling :</span></p><p class="c0"><span class="c5">The ResNet like deep models add an extra convolutional layer at the bottom of an architecture which is known as depth of a network. Each of these convolution layer has a fixed number of output channels (od depth)/ dimensions. For example if a network has 5 layers with 3 , 32, 64, 128, 128, 256 output shapes from each of them respectively. Then no of layers = 5 would be the depth of the network and 32, 64, 128, 128, 256 should refer to the width of each layer. In WRN terminology k is the width multiplier. So if we make k=2 the output from each of the convolutional layers will be 32-&gt;64, 64-&gt;128, 128-&gt;256,128-&gt;256,256-&gt;512.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">So far most of the research focused on increasing the depth of the network by adding additional layers. Improving a small fraction of accuracy used to require almost doubling the network depth e.g ResNet50, ResNet101 etc. These deeper networks are prone to the issue of vanishing gradient which was tackled by introducing identity units in ResNet architecture. The identity connection ensures that gradient flows through the deepest layers but there is no mechanism that can force the intermediate layers weights to learn here. So it doesn&rsquo;t guarantee that all the intermediate layers are actually learning meaningful features. Some &ldquo;idle layers&rdquo; can just have nearly zeros/constant weights and pass the previous layer information to the next via identity connection. &nbsp;This issue may result in a network with lots of unused layers and parameters. WRN uses a less deep network so the vanishing gradient issue is automatically resolved , since identity connection is no more required; we don&#39;t need to worry about the unused layer issue.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">Widening the network results in increased tendency of overfitting due to increased parameters per layer, a problem which was tackled by randomly disabling layers during training, a special case of dropout. WRN effectively used dropout during training for dealing with overfitting.</span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c5">It is claimed by the authors that a 16 layer WRN can perform similar to a 1000 layer deep network. The wide networks nearly 2 times faster converge as well.</span></p><p class="c0 c1"><span class="c5"></span></p><ol class="c22 lst-kix_ijcak9j4a5eq-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c5">Exploring width dimensions opposing the ongoing trend of depth increment.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Using dropout efficiently for reducing overfitting.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Improved a few weaknesses of ResNet networks by reducing depth i.e unused layers and parameters, faster training etc.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Making a 16 times less deep (but wider) network to perform almost equally with comparable parameters.</span></li></ol><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 982.04px; height: 331.68px;"><img alt="" src="files/cnn/image7.png" style="width: 982.04px; height: 331.68px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image from WRN paper by Zagoruyko at el.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c6">Design Choices :</span></p><ul class="c22 lst-kix_gf9vecvc59t-0 start"><li class="c0 c3 li-bullet-0"><span class="c5">In the figure above (b) the model is used for making the network thinner. Since WRN intends to explore the width of the network it is counter intuitive to use (a) so the authors preferred using the (c ) for WRN. It is like (d) when dropout is added to the pattern,</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Since ResNet-V2 showed BN-ReLU-Conv is better than Conv-BN-ReLU , WRN also followed the design pattern from ResNet-V2.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">The width factor &ldquo;k&rdquo; implies how many times more channels result from each convolution layer.</span></li></ul><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span class="c6">Results :</span></p><p class="c0"><span class="c5">In the below table we can see that -</span></p><ul class="c22 lst-kix_fh2qtwmwsffy-0 start"><li class="c0 c3 li-bullet-0"><span class="c5">Increasing the width reduces the loss almost all the time e.g - WRN-40-8 is much better than WRN-40-1.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">A much less deep network WRN-22-10 (4.17) has lower loss than a deeper one WRN-40-8 (4.66) while having comparable parameters.</span></li></ul><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 594.89px; height: 355.90px;"><img alt="" src="files/cnn/image26.png" style="width: 594.89px; height: 355.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5">&nbsp;</span></p><p class="c0"><span class="c6">Table : Taken from WRN paper by Zagoruyko at el. Loss for different depth and width factors (k), k=1 means width is the same as parent ResNet.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><a id="id.aatdzn9tv2sq"></a><p class="c0"><span class="c23">Network Architecture Search </span></p><p class="c0 c1"><span class="c23"></span></p><p class="c0"><span class="c16">NASNet &nbsp; &nbsp; : </span><span class="c35 c16"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1707.07012.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103482523&amp;usg=AOvVaw0-_x9dMO7dFT58dtGNoyvU">Learning Transferable Architectures for Scalable Image Recognition</a></span><span class="c6">&nbsp;(2016)</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c5">Network Architecture Search tries to design a neural network automatically by optimizing accuracy. MNASNet optimizes both accuracy and latency. They use a RNN (Recursive Neural Network) based &ldquo;controller&rdquo; for proposing different architectures and optimizes it using RL (Reinforcement Learning) i.e Policy Gradient. Here accuracy (or accuracy and latency) constructs the reward for the RL optimizer.</span></p><ol class="c22 lst-kix_btqaimccbtkv-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c5">Controller proposes a new architecture. This step will be explained later in detail.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">The model is trained on target dataset e.g ImageNet , CIFAR10</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Accuracy and latency is measured on validation dataset for reward calculation.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">The controller RNN network is optimized with a RL optimizer based on the reward. </span></li></ol><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 666.00px; height: 264.00px;"><img alt="" src="files/cnn/image25.png" style="width: 666.00px; height: 264.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from Tan at el. (MNASNet)</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c5">The step 1 above is a little tricky to deal with. Here one RNN based network is proposing another CNN network model. Total B (B=5 here) number of cells constructs each Normal or Reduction cell cell. Each of the B cells are are predicted by the RNN network in the below steps -</span></p><ol class="c22 lst-kix_nebp76l47abp-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c5">Select one hidden state from the supplied previous states.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Select another hidden state.</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Select an operation for &nbsp;state-1 (selected at step 1).</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Select a operation for state-2 (selected at step 2)</span></li><li class="c0 c3 li-bullet-0"><span class="c5">Select an operation for combining the two outputs.</span></li></ol><p class="c0 c30 c1"><span class="c5"></span></p><p class="c0"><span class="c16">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1154.00px; height: 235.00px;"><img alt="" src="files/cnn/image4.png" style="width: 1154.00px; height: 235.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from Zoph at el.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c5">Below (left) are the optimal Normal and reduction cells designed by the system. Utilizing the above method each of the Normal and Reduction cells are designed which are then put on the skeleton for ImageNet or CIFAR10 shown below (right)</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 646.00px; height: 370.29px;"><img alt="" src="files/cnn/image9.png" style="width: 646.00px; height: 370.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c16">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 343.50px; height: 550.00px;"><img alt="" src="files/cnn/image13.png" style="width: 343.50px; height: 550.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Image taken from Zoph at el.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c16">MNASNet : </span><span class="c16 c35"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.11626.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103484435&amp;usg=AOvVaw0oIrqMvO1lZaI1-MqtTU2q">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a></span><span class="c16">&nbsp;(2019)</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c5">Most of the work before MNASNet worked on designing a unit block that is repeated through the entire architecture. The initial layers deal with a larger amount of information (high resolution feature map) , so the block architecture for the initial layers should differ from the deeper layers. MNASNet gives the design algorithm freedom to design unique architectures for each stage of the network. It achieves layer wise diversity so unlike NASNet here the design of each block is learnt separately. A predefined skeleton of blocks is the backbone of the system. Each of the blocks has the same layer repeated N times. A similar RL based approach of NASNet is used to predict the layer architecture and number of repetition. Same process is repeated for every block. The first layer of every block has stride 2 while others have 1. </span></p><p class="c0"><span class="c5">The search space of MNASNet is centered on MobileNet V2 architecture. For layers in each block it optimized for {0,+1,-1} and for no of filter in each layer it optimized for {1.0,0.75,1.25}.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 985.50px; height: 330.00px;"><img alt="" src="files/cnn/image2.png" style="width: 985.50px; height: 468.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Tan at el.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 475.00px; height: 603.00px;"><img alt="" src="files/cnn/image16.png" style="width: 475.00px; height: 603.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image taken from Tan at el.</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c16 c21">&nbsp;</span><a id="id.jzk9bc3j1ill"></a><span class="c20">EfficientNet &nbsp;(2019)</span></p><p class="c0 c1"><span class="c16 c31 c21"></span></p><hr><p class="c0 c1"><span class="c16 c21 c31"></span></p><h1 class="c2" id="h.oxandx2tth1j"><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1905.11946.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103485724&amp;usg=AOvVaw0dDz-6WZkmg1XzOV6TkMkL">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></span></h1><p class="c0 c1"><span class="c7"></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span>While the Network Architecture Search methods mostly focused on designing the motifs of each block of the predefined skeleton architecture. EfficientNet focused on scaling the skeleton itself. Three kinds of network scaling are combined together for optimal model development i.e depth scaling, width scaling, resolution scaling. The target of this research was to make an optimal trade off between FLOPS and accuracy while selecting depth, width and resolution scales efficiently. Model of different scales has been proposed (EfficientNet-B0 to B7) for different parameter regimes and accuracy </span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span class="c7">Depth scaling &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : Adjusting the number of layers.</span></p><p class="c0"><span class="c7">Width Scaling &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : Adjusting the number of output channels (feature map channels) from each convolution layer.</span></p><p class="c0"><span class="c7">Resolution Scaling &nbsp; : Input Image Resolution.</span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span class="c7">Squeeze and Excitation :</span></p><p class="c0"><span class="c7">Squeeze and Excitation Layer namely SE-Convolution layer is a special type of convolution layer. &nbsp;It learns additional parameters to incorporate a channel wise attention mechanism on the output of the convolution filter. </span></p><p class="c0"><span class="c7">&nbsp; &nbsp; &nbsp; Fsq(.) &nbsp;: In the figure below it applies global average pooling on convolution output feature map (U) denoted by Fsq (.) which produces a 1x1xC vector.</span></p><p class="c0"><span class="c7">&nbsp; &nbsp; &nbsp; Fex(.) &nbsp;: </span></p><p class="c0"><span class="c7">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Then it applies a dense layer followed by ReLU to compress this 1x1xC to 1x1x(C/ratio), typically ratio=16. It encodes the information to a smaller dimension squeezing out the irrelevant information, the ReLU additionally adds some non-linearity to the mechanism.</span></p><p class="c0"><span class="c7">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Expand it to 1x1xC with another dense layer followed by a sigmoid; the sigmoid provides a smooth gating.</span></p><p class="c0"><span class="c7">The resultant 1x1xC vector is multiplied depthwise with the feature map (U) thus emphasizes on relevant feature maps with a negligible number of additional parameters. The two dense layers work like an encoder-decoder mechanism and preserve the most relevant information in the 1x1xC vector, squeezing out the irrelevant data. In the encoder-decoder mechanism the bottleneck i.e the most compressed layer forces it to filter out unimportant information within the vector.</span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 812.91px; height: 169.63px;"><img alt="" src="files/cnn/image23.png" style="width: 812.91px; height: 169.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 896.00px; height: 309.74px;"><img alt="" src="files/cnn/image10.png" style="width: 896.00px; height: 309.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span class="c7">They observed that if one of the scaling parameters is increased keeping others fixed the accuracy saturates fast. For example if the resolution of the image is increased keeping the depth and width fixed the accuracy will get stagnant soon. For this reason they tried compound scaling. They used the same search space of MNASNet &nbsp;for larger FLOPS. The below equations are proposed for guiding the proposed compound scaling method. Firstly, they did a small grid search by keeping phi = 1 &nbsp;and found alpha = 1.2 , beta = 1.1 , gamma = 1.15 for EfficientNet-B0. Then keeping alpha, &nbsp;beta and gamma fixed they optimized equation (3) below for different memory and FLOPS to develop B1-B7.</span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 534.00px; height: 222.00px;"><img alt="" src="files/cnn/image20.png" style="width: 534.00px; height: 222.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 429.00px; height: 239.00px;"><img alt="" src="files/cnn/image18.png" style="width: 548.00px; height: 239.00px; margin-left: -119.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 562.00px; height: 276.00px;"><img alt="" src="files/cnn/image15.png" style="width: 562.00px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 479.20px; height: 322.94px;"><img alt="" src="files/cnn/image1.png" style="width: 479.20px; height: 322.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 806.00px; height: 489.00px;"><img alt="" src="files/cnn/image12.png" style="width: 806.00px; height: 489.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c6">To Do -</span></p><p class="c0 c1"><span class="c6"></span></p><p class="c0"><span class="c6">Designing the Network design Space</span></p><p class="c0"><span class="c6">SE , ShuffleNet, SQueezeNet</span></p><p class="c0 c1"><span class="c6"></span></p><a id="id.2wgyjv3c92i6"></a><p class="c0"><span class="c34 c37">Reference</span></p><p class="c0"><span class="c19 c21">Papers :</span></p><ol class="c22 lst-kix_lzjn5gyciksb-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c16">LeNet &nbsp; &nbsp; : </span><span class="c13"><a class="c8" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/publis/index.html%23lecun-98&amp;sa=D&amp;source=editors&amp;ust=1673533103488494&amp;usg=AOvVaw1gZr5P6ehf9DCFEDpQkrv9">Gradient-Based Learning Applied to Document Recognition</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">AlexNet &nbsp;: </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103488814&amp;usg=AOvVaw2Yk-a4WC6N1atM2sIVhMF_">ImageNet Classification with Deep Convolutional Neural Networks</a></span><span class="c5">. </span></li><li class="c0 c3 li-bullet-0"><span class="c16">VGG16</span><span class="c19">&nbsp; &nbsp;: </span><span class="c13"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1409.1556&amp;sa=D&amp;source=editors&amp;ust=1673533103489137&amp;usg=AOvVaw1szCL63ye8H2ienT4QuIPs">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></span><span class="c5">&nbsp;</span></li><li class="c0 c3 li-bullet-0"><span class="c16 c21">ResNet V1 :</span><span class="c19 c21 c33">&nbsp; </span><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1512.03385.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103489450&amp;usg=AOvVaw2I_FDp2m67F1lHcwEGf2Om">Deep Residual Learning for Image Recognition</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16 c21">ResNet V2 :</span><span class="c19 c33 c21">&nbsp;</span><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1603.05027.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103489827&amp;usg=AOvVaw0Nnr0AL7HF-OGT4P_586X8">Identity Mappings in Deep Residual Networks</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">MobileNet V1 :</span><span class="c19">&nbsp;</span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1704.04861.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103490214&amp;usg=AOvVaw1tjY8eg5gLzneSPCEJQHRJ">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">MobileNet V2 :</span><span class="c19 c26">&nbsp;</span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1801.04381.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103490663&amp;usg=AOvVaw1F9XfkucZxJNLDT2L0DoNh">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">Wide ResNet : </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1605.07146.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103490956&amp;usg=AOvVaw2fm33III-5ud7ZNMuyIwhm">Wide Residual Networks</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">NASNet &nbsp; &nbsp; &nbsp; &nbsp; : </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1707.07012.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103491285&amp;usg=AOvVaw1rlp7FmQQUF-L7Nj1Hgd06">Learning Transferable Architectures for Scalable Image Recognition</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">MNASNet &nbsp; &nbsp; : </span><span class="c14"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.11626.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103491600&amp;usg=AOvVaw11ZH5xh0rGfpWSeC3u80Hr">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a></span></li><li class="c0 c3 li-bullet-0"><span class="c16">EfficientNet &nbsp;:</span><span class="c19">&nbsp;</span><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1905.11946.pdf&amp;sa=D&amp;source=editors&amp;ust=1673533103491908&amp;usg=AOvVaw2PB1blNAKFOzK2pVb3guHT">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></span></li></ol><p class="c0 c1"><span class="c10"></span></p><p class="c0 c1"><span class="c10"></span></p><p class="c0"><span class="c19 c21 c34">Blogs :</span></p><ol class="c22 lst-kix_5h488x4jtpzl-0 start" start="1"><li class="c0 c3 li-bullet-0"><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d&amp;sa=D&amp;source=editors&amp;ust=1673533103492549&amp;usg=AOvVaw0nMWAhzkt_a4IsSKwLYBPM">https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d</a></span></li><li class="c0 c3 li-bullet-0"><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://towardsdatascience.com/wide-residual-networks-with-interactive-code-f9ef6b0bab29&amp;sa=D&amp;source=editors&amp;ust=1673533103492870&amp;usg=AOvVaw1FKbMQBu_FNqMsLVgxhYae">https://towardsdatascience.com/wide-residual-networks-with-interactive-code-f9ef6b0bab29</a></span></li><li class="c0 c3 li-bullet-0"><span class="c14 c21"><a class="c8" href="https://www.google.com/url?q=https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c&amp;sa=D&amp;source=editors&amp;ust=1673533103493170&amp;usg=AOvVaw27mEFd4G9wZgZwZmpltOFM">https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c</a></span></li></ol><p class="c0 c30 c1"><span class="c10"></span></p><p class="c0 c1"><span class="c7"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c5"></span></p><p class="c0 c1"><span class="c6"></span></p></body></html>