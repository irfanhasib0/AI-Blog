<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_zanxhs2sqm3l-8>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-8,lower-roman) ". "}.lst-kix_fh2qtwmwsffy-2>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-4>li:before{content:"-  "}.lst-kix_7yi37vgztglf-1>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-1,lower-latin) ". "}ul.lst-kix_ngpgdtyz59e3-6{list-style-type:none}ol.lst-kix_y1j9a0umnveg-4.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-4 0}ol.lst-kix_zanxhs2sqm3l-7.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-7 0}ul.lst-kix_ngpgdtyz59e3-7{list-style-type:none}ul.lst-kix_ngpgdtyz59e3-8{list-style-type:none}.lst-kix_h7mmrf9tmnyi-7>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-7}.lst-kix_fh2qtwmwsffy-1>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-5>li:before{content:"-  "}.lst-kix_7yi37vgztglf-0>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-0,decimal) ". "}ul.lst-kix_ngpgdtyz59e3-2{list-style-type:none}.lst-kix_y1j9a0umnveg-2>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-2}ul.lst-kix_ngpgdtyz59e3-3{list-style-type:none}.lst-kix_ijcak9j4a5eq-7>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-7}ul.lst-kix_ngpgdtyz59e3-4{list-style-type:none}.lst-kix_zanxhs2sqm3l-6>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-6,decimal) ". "}ul.lst-kix_ngpgdtyz59e3-5{list-style-type:none}.lst-kix_7yi37vgztglf-5>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-5,lower-roman) ". "}ol.lst-kix_h7mmrf9tmnyi-1.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-1 0}.lst-kix_zanxhs2sqm3l-7>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-7,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-5.start{counter-reset:lst-ctn-kix_7yi37vgztglf-5 0}ul.lst-kix_ngpgdtyz59e3-0{list-style-type:none}ul.lst-kix_ngpgdtyz59e3-1{list-style-type:none}.lst-kix_fh2qtwmwsffy-3>li:before{content:"-  "}.lst-kix_7yi37vgztglf-6>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-6,decimal) ". "}.lst-kix_zanxhs2sqm3l-2>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-2}.lst-kix_7yi37vgztglf-7>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-7,lower-latin) ". "}ol.lst-kix_ijcak9j4a5eq-3.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-3 0}ol.lst-kix_ey615mv2rhwi-8.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-8 0}.lst-kix_7yi37vgztglf-8>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-8,lower-roman) ". "}.lst-kix_h7mmrf9tmnyi-1>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-1,lower-latin) ". "}.lst-kix_ey615mv2rhwi-7>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-7}.lst-kix_fh2qtwmwsffy-6>li:before{content:"-  "}.lst-kix_fh2qtwmwsffy-8>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-2>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-2,lower-roman) ". "}ol.lst-kix_zanxhs2sqm3l-1.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-1 0}.lst-kix_fh2qtwmwsffy-7>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-0>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-0,decimal) ". "}ol.lst-kix_zanxhs2sqm3l-3{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-4{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-1{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-2{list-style-type:none}ol.lst-kix_2ss37fysegfq-3.start{counter-reset:lst-ctn-kix_2ss37fysegfq-3 0}.lst-kix_zanxhs2sqm3l-0>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-0,decimal) ". "}ol.lst-kix_zanxhs2sqm3l-0{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-2{list-style-type:none}.lst-kix_zanxhs2sqm3l-1>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-1,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-2>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-2,lower-roman) ". "}ul.lst-kix_fh2qtwmwsffy-1{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-6.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-6 0}ul.lst-kix_fh2qtwmwsffy-4{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-3{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-7{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-8{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-5{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-0{list-style-type:none}.lst-kix_2ss37fysegfq-2>li{counter-increment:lst-ctn-kix_2ss37fysegfq-2}ol.lst-kix_zanxhs2sqm3l-6{list-style-type:none}.lst-kix_zanxhs2sqm3l-5>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-5,lower-roman) ". "}.lst-kix_y1j9a0umnveg-4>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-4}ul.lst-kix_fh2qtwmwsffy-6{list-style-type:none}.lst-kix_zanxhs2sqm3l-3>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-3,decimal) ". "}.lst-kix_zanxhs2sqm3l-4>li:before{content:"" counter(lst-ctn-kix_zanxhs2sqm3l-4,lower-latin) ". "}ul.lst-kix_fh2qtwmwsffy-5{list-style-type:none}ul.lst-kix_fh2qtwmwsffy-8{list-style-type:none}.lst-kix_h7mmrf9tmnyi-5>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-5}ul.lst-kix_fh2qtwmwsffy-7{list-style-type:none}ol.lst-kix_zanxhs2sqm3l-2.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-2 0}.lst-kix_7yi37vgztglf-0>li{counter-increment:lst-ctn-kix_7yi37vgztglf-0}.lst-kix_jz7o59hmr7i0-8>li:before{content:"-  "}.lst-kix_ijcak9j4a5eq-5>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-5}.lst-kix_jz7o59hmr7i0-7>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-3>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-3}ol.lst-kix_2ss37fysegfq-4.start{counter-reset:lst-ctn-kix_2ss37fysegfq-4 0}ol.lst-kix_ijcak9j4a5eq-8.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-8 0}.lst-kix_2ss37fysegfq-4>li{counter-increment:lst-ctn-kix_2ss37fysegfq-4}.lst-kix_jz7o59hmr7i0-0>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-1>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-3>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-4>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-5>li:before{content:"-  "}.lst-kix_jz7o59hmr7i0-6>li:before{content:"-  "}ol.lst-kix_7yi37vgztglf-4.start{counter-reset:lst-ctn-kix_7yi37vgztglf-4 0}.lst-kix_7yi37vgztglf-4>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-4,lower-latin) ". "}ol.lst-kix_y1j9a0umnveg-5.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-5 0}.lst-kix_7yi37vgztglf-3>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-3,decimal) ". "}.lst-kix_zanxhs2sqm3l-4>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-4}.lst-kix_jz7o59hmr7i0-2>li:before{content:"-  "}.lst-kix_7yi37vgztglf-2>li:before{content:"" counter(lst-ctn-kix_7yi37vgztglf-2,lower-roman) ". "}.lst-kix_ijcak9j4a5eq-7>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-7,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-8>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-8}.lst-kix_h7mmrf9tmnyi-1>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-1}ol.lst-kix_ey615mv2rhwi-7.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-7 0}.lst-kix_y1j9a0umnveg-3>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-3,decimal) ". "}.lst-kix_7yi37vgztglf-3>li{counter-increment:lst-ctn-kix_7yi37vgztglf-3}.lst-kix_h7mmrf9tmnyi-0>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-0}.lst-kix_y1j9a0umnveg-1>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-1,lower-latin) ". "}.lst-kix_ijcak9j4a5eq-3>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-3}.lst-kix_7yi37vgztglf-5>li{counter-increment:lst-ctn-kix_7yi37vgztglf-5}.lst-kix_y1j9a0umnveg-7>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-7}.lst-kix_ey615mv2rhwi-0>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-0}.lst-kix_ngpgdtyz59e3-3>li:before{content:"-  "}.lst-kix_ngpgdtyz59e3-7>li:before{content:"-  "}.lst-kix_y1j9a0umnveg-7>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-7,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-6.start{counter-reset:lst-ctn-kix_7yi37vgztglf-6 0}.lst-kix_ey615mv2rhwi-6>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-6,decimal) ". "}ol.lst-kix_ijcak9j4a5eq-2.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-2 0}.lst-kix_y1j9a0umnveg-5>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-5,lower-roman) ". "}.lst-kix_ngpgdtyz59e3-5>li:before{content:"-  "}.lst-kix_2ss37fysegfq-8>li{counter-increment:lst-ctn-kix_2ss37fysegfq-8}ol.lst-kix_y1j9a0umnveg-3.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-3 0}ol.lst-kix_zanxhs2sqm3l-8.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-8 0}ol.lst-kix_y1j9a0umnveg-0.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-0 0}ol.lst-kix_zanxhs2sqm3l-5.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-5 0}.lst-kix_ey615mv2rhwi-8>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-8,lower-roman) ". "}.lst-kix_h7mmrf9tmnyi-2>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-2}ol.lst-kix_h7mmrf9tmnyi-0.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-0 0}ol.lst-kix_h7mmrf9tmnyi-3.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-3 0}ol.lst-kix_7yi37vgztglf-7.start{counter-reset:lst-ctn-kix_7yi37vgztglf-7 0}ol.lst-kix_y1j9a0umnveg-2.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-2 0}ol.lst-kix_y1j9a0umnveg-7{list-style-type:none}ol.lst-kix_y1j9a0umnveg-8{list-style-type:none}ol.lst-kix_y1j9a0umnveg-5{list-style-type:none}ol.lst-kix_y1j9a0umnveg-6{list-style-type:none}.lst-kix_ijcak9j4a5eq-4>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-4}ol.lst-kix_y1j9a0umnveg-3{list-style-type:none}ol.lst-kix_y1j9a0umnveg-4{list-style-type:none}.lst-kix_y1j9a0umnveg-0>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-0}ol.lst-kix_y1j9a0umnveg-1{list-style-type:none}.lst-kix_2ss37fysegfq-8>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-8,lower-roman) ". "}ol.lst-kix_y1j9a0umnveg-2{list-style-type:none}ol.lst-kix_y1j9a0umnveg-0{list-style-type:none}ol.lst-kix_2ss37fysegfq-7.start{counter-reset:lst-ctn-kix_2ss37fysegfq-7 0}.lst-kix_y1j9a0umnveg-6>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-6}ol.lst-kix_ijcak9j4a5eq-5.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-5 0}ol.lst-kix_zanxhs2sqm3l-6.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-6 0}.lst-kix_ngpgdtyz59e3-1>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-3>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-3,decimal) ". "}.lst-kix_h7mmrf9tmnyi-5>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-5,lower-roman) ". "}ol.lst-kix_2ss37fysegfq-8.start{counter-reset:lst-ctn-kix_2ss37fysegfq-8 0}ol.lst-kix_h7mmrf9tmnyi-2.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-2 0}.lst-kix_h7mmrf9tmnyi-7>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-7,lower-latin) ". "}.lst-kix_2ss37fysegfq-0>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-0,decimal) ". "}ol.lst-kix_ijcak9j4a5eq-4.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-4 0}.lst-kix_2ss37fysegfq-6>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-6,decimal) ". "}.lst-kix_ijcak9j4a5eq-3>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-3,decimal) ". "}.lst-kix_ijcak9j4a5eq-5>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-3>li{counter-increment:lst-ctn-kix_2ss37fysegfq-3}ol.lst-kix_ey615mv2rhwi-8{list-style-type:none}ol.lst-kix_ey615mv2rhwi-7{list-style-type:none}ol.lst-kix_ey615mv2rhwi-6{list-style-type:none}ol.lst-kix_ey615mv2rhwi-5{list-style-type:none}ol.lst-kix_ey615mv2rhwi-4{list-style-type:none}.lst-kix_2ss37fysegfq-2>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-2,lower-roman) ". "}ol.lst-kix_ey615mv2rhwi-3{list-style-type:none}ol.lst-kix_ey615mv2rhwi-2{list-style-type:none}.lst-kix_7yi37vgztglf-4>li{counter-increment:lst-ctn-kix_7yi37vgztglf-4}ol.lst-kix_ey615mv2rhwi-1{list-style-type:none}.lst-kix_ey615mv2rhwi-5>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-5}.lst-kix_ijcak9j4a5eq-1>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-1,lower-latin) ". "}ol.lst-kix_ey615mv2rhwi-0{list-style-type:none}.lst-kix_2ss37fysegfq-4>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-4,lower-latin) ". "}.lst-kix_2ss37fysegfq-1>li{counter-increment:lst-ctn-kix_2ss37fysegfq-1}ol.lst-kix_7yi37vgztglf-2.start{counter-reset:lst-ctn-kix_7yi37vgztglf-2 0}ol.lst-kix_h7mmrf9tmnyi-8{list-style-type:none}.lst-kix_yc6zqncxu6ij-6>li:before{content:"-  "}.lst-kix_yc6zqncxu6ij-7>li:before{content:"-  "}ol.lst-kix_y1j9a0umnveg-7.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-7 0}ol.lst-kix_zanxhs2sqm3l-4.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-4 0}.lst-kix_yc6zqncxu6ij-8>li:before{content:"-  "}ol.lst-kix_ijcak9j4a5eq-6.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-6 0}.lst-kix_zanxhs2sqm3l-3>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-3}ul.lst-kix_jz7o59hmr7i0-1{list-style-type:none}.lst-kix_yc6zqncxu6ij-2>li:before{content:"-  "}ul.lst-kix_jz7o59hmr7i0-0{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-3{list-style-type:none}.lst-kix_y1j9a0umnveg-3>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-3}.lst-kix_yc6zqncxu6ij-1>li:before{content:"-  "}.lst-kix_yc6zqncxu6ij-3>li:before{content:"-  "}ul.lst-kix_jz7o59hmr7i0-2{list-style-type:none}ol.lst-kix_2ss37fysegfq-6.start{counter-reset:lst-ctn-kix_2ss37fysegfq-6 0}ol.lst-kix_h7mmrf9tmnyi-7{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-6{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-5{list-style-type:none}.lst-kix_yc6zqncxu6ij-5>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-4{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-0.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-0 0}ol.lst-kix_ey615mv2rhwi-0.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-0 0}ol.lst-kix_h7mmrf9tmnyi-3{list-style-type:none}ol.lst-kix_y1j9a0umnveg-1.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-1 0}.lst-kix_yc6zqncxu6ij-4>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-2{list-style-type:none}.lst-kix_ijcak9j4a5eq-8>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-8}ol.lst-kix_h7mmrf9tmnyi-1{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-0{list-style-type:none}.lst-kix_ey615mv2rhwi-6>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-6}.lst-kix_7yi37vgztglf-8>li{counter-increment:lst-ctn-kix_7yi37vgztglf-8}.lst-kix_ijcak9j4a5eq-6>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-6}.lst-kix_y1j9a0umnveg-1>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-1}ol.lst-kix_7yi37vgztglf-8.start{counter-reset:lst-ctn-kix_7yi37vgztglf-8 0}ul.lst-kix_jz7o59hmr7i0-8{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-5{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-4{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-7{list-style-type:none}ul.lst-kix_jz7o59hmr7i0-6{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-1{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-0{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-3{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-2{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-5{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-4{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-7{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-6{list-style-type:none}ol.lst-kix_ijcak9j4a5eq-8{list-style-type:none}.lst-kix_h7mmrf9tmnyi-8>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-8}ol.lst-kix_2ss37fysegfq-0.start{counter-reset:lst-ctn-kix_2ss37fysegfq-0 0}ol.lst-kix_ijcak9j4a5eq-1.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-1 0}.lst-kix_y1j9a0umnveg-5>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-5}ol.lst-kix_2ss37fysegfq-4{list-style-type:none}ol.lst-kix_y1j9a0umnveg-6.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-6 0}ol.lst-kix_2ss37fysegfq-5{list-style-type:none}ol.lst-kix_2ss37fysegfq-6{list-style-type:none}ol.lst-kix_2ss37fysegfq-7{list-style-type:none}ol.lst-kix_2ss37fysegfq-0{list-style-type:none}.lst-kix_ey615mv2rhwi-4>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-4}ol.lst-kix_2ss37fysegfq-1{list-style-type:none}.lst-kix_7yi37vgztglf-6>li{counter-increment:lst-ctn-kix_7yi37vgztglf-6}ol.lst-kix_2ss37fysegfq-2{list-style-type:none}ol.lst-kix_2ss37fysegfq-3{list-style-type:none}.lst-kix_ey615mv2rhwi-4>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-4,lower-latin) ". "}ol.lst-kix_2ss37fysegfq-8{list-style-type:none}.lst-kix_ey615mv2rhwi-3>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-3,decimal) ". "}ol.lst-kix_ey615mv2rhwi-5.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-5 0}ol.lst-kix_7yi37vgztglf-3.start{counter-reset:lst-ctn-kix_7yi37vgztglf-3 0}ol.lst-kix_ijcak9j4a5eq-7.start{counter-reset:lst-ctn-kix_ijcak9j4a5eq-7 0}.lst-kix_gf9vecvc59t-8>li:before{content:"-  "}.lst-kix_ey615mv2rhwi-2>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-2,lower-roman) ". "}.lst-kix_gf9vecvc59t-7>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-4.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-4 0}.lst-kix_ey615mv2rhwi-1>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-1,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-1>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-1}.lst-kix_gf9vecvc59t-5>li:before{content:"-  "}.lst-kix_h7mmrf9tmnyi-6>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-6}ol.lst-kix_zanxhs2sqm3l-3.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-3 0}.lst-kix_gf9vecvc59t-6>li:before{content:"-  "}.lst-kix_ey615mv2rhwi-0>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-0,decimal) ". "}ul.lst-kix_gf9vecvc59t-8{list-style-type:none}.lst-kix_gf9vecvc59t-0>li:before{content:"-  "}.lst-kix_gf9vecvc59t-1>li:before{content:"-  "}ol.lst-kix_2ss37fysegfq-5.start{counter-reset:lst-ctn-kix_2ss37fysegfq-5 0}ol.lst-kix_ey615mv2rhwi-6.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-6 0}ul.lst-kix_gf9vecvc59t-0{list-style-type:none}ul.lst-kix_gf9vecvc59t-1{list-style-type:none}.lst-kix_gf9vecvc59t-4>li:before{content:"-  "}ul.lst-kix_gf9vecvc59t-2{list-style-type:none}ul.lst-kix_gf9vecvc59t-3{list-style-type:none}ul.lst-kix_gf9vecvc59t-4{list-style-type:none}.lst-kix_gf9vecvc59t-2>li:before{content:"-  "}.lst-kix_gf9vecvc59t-3>li:before{content:"-  "}ul.lst-kix_gf9vecvc59t-5{list-style-type:none}.lst-kix_zanxhs2sqm3l-7>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-7}ul.lst-kix_gf9vecvc59t-6{list-style-type:none}ul.lst-kix_gf9vecvc59t-7{list-style-type:none}ol.lst-kix_ey615mv2rhwi-4.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-4 0}ol.lst-kix_2ss37fysegfq-2.start{counter-reset:lst-ctn-kix_2ss37fysegfq-2 0}.lst-kix_2ss37fysegfq-7>li{counter-increment:lst-ctn-kix_2ss37fysegfq-7}.lst-kix_ijcak9j4a5eq-8>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-8,lower-roman) ". "}ol.lst-kix_zanxhs2sqm3l-0.start{counter-reset:lst-ctn-kix_zanxhs2sqm3l-0 0}.lst-kix_ijcak9j4a5eq-2>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-2}ol.lst-kix_h7mmrf9tmnyi-5.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-5 0}.lst-kix_y1j9a0umnveg-8>li{counter-increment:lst-ctn-kix_y1j9a0umnveg-8}.lst-kix_ijcak9j4a5eq-6>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-6,decimal) ". "}.lst-kix_y1j9a0umnveg-4>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-4,lower-latin) ". "}.lst-kix_ey615mv2rhwi-1>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-1}ul.lst-kix_yc6zqncxu6ij-7{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-8{list-style-type:none}.lst-kix_ijcak9j4a5eq-1>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-1}ul.lst-kix_yc6zqncxu6ij-1{list-style-type:none}.lst-kix_y1j9a0umnveg-0>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-0,decimal) ". "}ul.lst-kix_yc6zqncxu6ij-2{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-0{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-5{list-style-type:none}.lst-kix_y1j9a0umnveg-2>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-2,lower-roman) ". "}ul.lst-kix_yc6zqncxu6ij-6{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-3{list-style-type:none}ul.lst-kix_yc6zqncxu6ij-4{list-style-type:none}.lst-kix_ngpgdtyz59e3-6>li:before{content:"-  "}.lst-kix_ijcak9j4a5eq-0>li{counter-increment:lst-ctn-kix_ijcak9j4a5eq-0}.lst-kix_ey615mv2rhwi-3>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-3}.lst-kix_ngpgdtyz59e3-4>li:before{content:"-  "}.lst-kix_ngpgdtyz59e3-8>li:before{content:"-  "}.lst-kix_7yi37vgztglf-2>li{counter-increment:lst-ctn-kix_7yi37vgztglf-2}.lst-kix_ey615mv2rhwi-5>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-5>li{counter-increment:lst-ctn-kix_2ss37fysegfq-5}.lst-kix_y1j9a0umnveg-6>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-6,decimal) ". "}.lst-kix_ey615mv2rhwi-7>li:before{content:"" counter(lst-ctn-kix_ey615mv2rhwi-7,lower-latin) ". "}ol.lst-kix_ey615mv2rhwi-1.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-1 0}.lst-kix_y1j9a0umnveg-8>li:before{content:"" counter(lst-ctn-kix_y1j9a0umnveg-8,lower-roman) ". "}.lst-kix_h7mmrf9tmnyi-4>li{counter-increment:lst-ctn-kix_h7mmrf9tmnyi-4}.lst-kix_zanxhs2sqm3l-6>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-6}ol.lst-kix_7yi37vgztglf-0.start{counter-reset:lst-ctn-kix_7yi37vgztglf-0 0}ol.lst-kix_ey615mv2rhwi-2.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-2 0}.lst-kix_zanxhs2sqm3l-0>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-0}.lst-kix_2ss37fysegfq-7>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-7,lower-latin) ". "}.lst-kix_zanxhs2sqm3l-5>li{counter-increment:lst-ctn-kix_zanxhs2sqm3l-5}.lst-kix_ngpgdtyz59e3-0>li:before{content:"-  "}ol.lst-kix_h7mmrf9tmnyi-7.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-7 0}.lst-kix_ngpgdtyz59e3-2>li:before{content:"-  "}ol.lst-kix_2ss37fysegfq-1.start{counter-reset:lst-ctn-kix_2ss37fysegfq-1 0}.lst-kix_h7mmrf9tmnyi-4>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-4,lower-latin) ". "}ol.lst-kix_7yi37vgztglf-1.start{counter-reset:lst-ctn-kix_7yi37vgztglf-1 0}.lst-kix_2ss37fysegfq-1>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-1,lower-latin) ". "}.lst-kix_h7mmrf9tmnyi-8>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-8,lower-roman) ". "}ol.lst-kix_y1j9a0umnveg-8.start{counter-reset:lst-ctn-kix_y1j9a0umnveg-8 0}.lst-kix_h7mmrf9tmnyi-6>li:before{content:"" counter(lst-ctn-kix_h7mmrf9tmnyi-6,decimal) ". "}.lst-kix_yc6zqncxu6ij-0>li:before{content:"-  "}ol.lst-kix_ey615mv2rhwi-3.start{counter-reset:lst-ctn-kix_ey615mv2rhwi-3 0}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_2ss37fysegfq-0>li{counter-increment:lst-ctn-kix_2ss37fysegfq-0}ol.lst-kix_7yi37vgztglf-3{list-style-type:none}ol.lst-kix_7yi37vgztglf-2{list-style-type:none}.lst-kix_7yi37vgztglf-7>li{counter-increment:lst-ctn-kix_7yi37vgztglf-7}.lst-kix_2ss37fysegfq-5>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-5,lower-roman) ". "}.lst-kix_2ss37fysegfq-6>li{counter-increment:lst-ctn-kix_2ss37fysegfq-6}ol.lst-kix_7yi37vgztglf-1{list-style-type:none}.lst-kix_ijcak9j4a5eq-0>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-0,decimal) ". "}.lst-kix_ijcak9j4a5eq-4>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-4,lower-latin) ". "}.lst-kix_ey615mv2rhwi-8>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-8}ol.lst-kix_7yi37vgztglf-0{list-style-type:none}ol.lst-kix_h7mmrf9tmnyi-8.start{counter-reset:lst-ctn-kix_h7mmrf9tmnyi-8 0}.lst-kix_ey615mv2rhwi-2>li{counter-increment:lst-ctn-kix_ey615mv2rhwi-2}.lst-kix_7yi37vgztglf-1>li{counter-increment:lst-ctn-kix_7yi37vgztglf-1}ol.lst-kix_7yi37vgztglf-7{list-style-type:none}ol.lst-kix_7yi37vgztglf-6{list-style-type:none}ol.lst-kix_7yi37vgztglf-5{list-style-type:none}.lst-kix_fh2qtwmwsffy-0>li:before{content:"-  "}ol.lst-kix_7yi37vgztglf-4{list-style-type:none}.lst-kix_2ss37fysegfq-3>li:before{content:"" counter(lst-ctn-kix_2ss37fysegfq-3,decimal) ". "}.lst-kix_ijcak9j4a5eq-2>li:before{content:"" counter(lst-ctn-kix_ijcak9j4a5eq-2,lower-roman) ". "}ol.lst-kix_7yi37vgztglf-8{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:italic}.c3{color:#e69138;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c23{background-color:#ffffff;padding-top:6pt;padding-bottom:9pt;line-height:0.9128347826086957;orphans:2;widows:2;text-align:left}.c2{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c4{color:#45818e;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c22{color:#6aa84f;text-decoration:none;vertical-align:baseline;font-style:normal}.c24{color:#45818e;text-decoration:none;vertical-align:baseline;font-style:normal}.c19{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c20{color:#4a86e8;text-decoration:none;vertical-align:baseline;font-style:italic}.c16{color:#000000;text-decoration:none;vertical-align:baseline;font-style:italic}.c13{color:#b5739d;text-decoration:none;vertical-align:baseline;font-style:italic}.c21{background-color:#f8f9fa;font-weight:400;font-size:1pt;font-family:"Courier New"}.c10{color:#cc4125;text-decoration:none;vertical-align:baseline;font-style:italic}.c15{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c29{font-weight:700;font-size:18pt;font-family:"Times New Roman"}.c6{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c17{font-size:16pt;font-family:"Times New Roman";font-weight:700}.c12{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c14{padding:0;margin:0}.c9{margin-left:36pt;padding-left:0pt}.c30{max-width:648pt;padding:72pt 72pt 72pt 72pt}.c8{color:inherit;text-decoration:inherit}.c26{background-color:#ffffff}.c25{font-style:italic}.c28{color:#4a86e8}.c27{margin-left:15pt}.c18{vertical-align:sub}.c11{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c26 c30 doc-content"><p class="c5"><span class="c16 c29">CNN Architectures (Under progress)</span></p><p class="c1"><span class="c6 c16"></span></p><hr><p class="c1"><span class="c16 c6"></span></p><p class="c1"><span class="c16 c6"></span></p><p class="c1 c11"><span class="c0"></span></p><p class="c5"><span class="c12">In the last decade lots of</span><span class="c0">&nbsp;neural network architectures have been proposed. Every year new architectures are getting attention and eventually suppress the older ones. I will try to go through the most significant architectures till today (2022) in the below section.</span></p><ol class="c14 lst-kix_ey615mv2rhwi-0 start" start="1"><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.70oagfdzamdd">Initial Convolutional Neural Networks </a></span></li><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.7cjl426tc84g">VGG16 </a></span></li><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.usmwggyeg7l">GoogleNet / Inception</a></span></li><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.j2c4fqjlutpf">ResNet Family </a></span></li><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.6ag6q5b1z3xm">Wide ResNet (WRN)</a></span></li><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="#id.jzk9bc3j1ill">EfficientNet </a></span></li></ol><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c6">Convolution Filter :</span><span class="c0">&nbsp;If we have a 10x10 sized image array and a 3x3 array of numeric values ( 3x3 kernel/filter). The 3x3 matrix can be convolved through the entire image from left-right row by row (the zick-zack line in the image below shows the path). At every pixel location the 9 numeric numbers of the 3x3 kernel will overlap with a similar 3x3 array (patch of image) from the 10x10 image. Now both of this 3x3 kernel and 3x3 patch from &nbsp;image can be pointwise multiplied and algebraically summed to produce a single numeric value on the output image. Once the whole image is covered, an output image can be formed with a slightly smaller size. For a 3x3 kernel the image height and width should shrink by 2 rows and 2 columns i.e 8x8 which can be understood by observing the zick-zack path of the convolution filter.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1010.50px; height: 249.66px;"><img alt="" src="files/cnn/image14.png" style="width: 1010.50px; height: 249.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c12">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c2">Figure : Convolution filter.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Convolution filters are capable of producing image features like edges, corners etc based on the weight parameters of the filter (or kernel). A combination of these filters can detect even more advanced image features like human eye , noose etc. In convolutional layers the filters are initialized with random numeric weights which are updated with back propagation for learning meaningful features. Some examples of filters are shown below -</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 775.51px; height: 173.43px;"><img alt="" src="files/cnn/image4.png" style="width: 775.51px; height: 173.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c19 c21"></span></p><p class="c1"><span class="c19 c21"></span></p><p class="c1"><span class="c19 c21"></span></p><p class="c5"><span class="c2">Convolution Layer :</span></p><p class="c5"><span class="c0">Convolution layer consists of a combination of convolutional filters. For the below figure the RGB image with 3 channels goes through a &ldquo;convolution layer&rdquo; that produces a 4 channel output feature map. The convolution layer has four separate filters for producing each of the four output feature map channels. For pointwise multiplication (at every pixel location) a convolutional filter requires an equal number of channels with the input image. That&rsquo;s why each filter unit &nbsp;here &nbsp;has 3 channels like the input RGB image. </span></p><p class="c5"><span class="c0">The 3x3x3 filters are convolved over the entire image like the above example. At each pixel location total 3x3x3 = 27 numeric values of one convolution filter and RGB image are element wise multiplied. The 27 numeric values are summed all together for producing one pixel value of the corresponding output channel.</span></p><p class="c5"><span class="c0">&nbsp;3 convolution filters are applied on the 3 input channels of the images all together respectively </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 744.83px; height: 390.06px;"><img alt="" src="files/cnn/image1.png" style="width: 744.83px; height: 390.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c12">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c2">Figure : A simple convolution layer.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c17">Initial </span><a id="id.70oagfdzamdd"></a><span class="c17">Convolutional Neural Networks</span><span class="c17 c19">&nbsp;(1998-2012)</span></p><hr><p class="c1"><span class="c2"></span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span class="c2">(i) LeNet &nbsp;(1998) &nbsp;</span></p><p class="c5"><span class="c15 c12 c26"><a class="c8" href="https://www.google.com/url?q=http://yann.lecun.com/exdb/publis/index.html%23lecun-98&amp;sa=D&amp;source=editors&amp;ust=1673210874898535&amp;usg=AOvVaw0JyCBoI9YlPFuXJK6c2GnA">Gradient-Based Learning Applied to Document Recognition</a></span></p><p class="c5"><span class="c0">It is the first &nbsp;successful convolutional neural network architecture which was applied to document classification by LeCun at el. &nbsp;A dense neural network can process tabular data i.e 1d feature vectors. Earlier, multilayer neural networks could be used for image classification by converting the images feature vectors. Feature extraction algorithms e.g SIFT, SURF etc were applied on the raw 2D/3D image array for fetching the keypoint descriptors which eventually transformed into feature vectors of fixed size. Then these feature vectors could be passed through a &nbsp;dense neural network for classification.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp;LeNet first introduced the multiple consecutive convolutional filter based neural network where the weights of the filter are learnt by back propagation. Convolutional filters can be directly applied on 2D/3D image data. These layers consisting of a group of convolution filters are called convolutional layers. The combination of convolutional filters all together act as complex but powerful feature extractor. The convolutional layers can be initialized with random numeric values learnt by back propagation automatically. It also gives the algorithm liberty to learn practically any combination of suitable filters necessary for the task in hand.</span></p><p class="c1 c11"><span class="c0"></span></p><p class="c5 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 829.37px; height: 227.44px;"><img alt="" src="files/cnn/image5.png" style="width: 829.37px; height: 227.44px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c11"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Figure : Image taken from LeCun at el. 1998.</span></p><p class="c1 c11"><span class="c0"></span></p><p class="c5"><span class="c2">(ii) AlexNet (2012) &nbsp;</span></p><p class="c5"><span class="c6">&nbsp;</span><span class="c15 c12"><a class="c8" href="https://www.google.com/url?q=https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874899543&amp;usg=AOvVaw16g0lHBhEKERxT2XUfACXn">ImageNet Classification with Deep Convolutional Neural Networks</a></span><span class="c0">. </span></p><p class="c5"><span class="c0">It introduced ReLU instead of TanH activation within the architecture. ReLU can converge much faster than TanH. &nbsp;Even today ReLU is the most popular activation function among the practitioners. &nbsp;AlexNet also used dropout for preventing overfitting in their training steps. AexNet is the CNN layer based network which started to get attention after LeNet.</span></p><p class="c1"><span class="c0"></span></p><p class="c1 c11"><span class="c16 c6"></span></p><a id="id.7cjl426tc84g"></a><p class="c5"><span class="c7">VGG16 &nbsp;(2014)</span></p><hr><p class="c1"><span class="c3"></span></p><p class="c1"><span class="c3"></span></p><p class="c5"><span class="c15 c12 c26"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/abs/1409.1556&amp;sa=D&amp;source=editors&amp;ust=1673210874900270&amp;usg=AOvVaw2K_WS0XQ8p4HSUVonkSpFJ">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></span><span class="c12">&nbsp;</span></p><p class="c5"><span class="c12">&nbsp;So far the CNN networks used to be very small compared to current practice. VGG16 was the first Deep CNN architecture which demonstrated outstanding accuracy compared to contemporary smaller CNN networks. First successful </span><span class="c6">Deep</span><span class="c0">&nbsp;Convolutional Neural Network. Their main contribution was pushing the depth of the CNN architecture for increasing accuracy. They pushed &nbsp;the depth up to 16-19 layers while using 3x3 kernels for better accuracy. In the ImageNet 2014 challenge they achieved 1st in localization and 2nd in classification criterion.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 861.20px; height: 518.48px;"><img alt="" src="files/cnn/image12.png" style="width: 861.20px; height: 518.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><a id="id.usmwggyeg7l"></a><p class="c5"><span class="c7">GoogleNet / Inception (2015)</span></p><hr><p class="c1"><span class="c13 c6"></span></p><p class="c1"><span class="c6 c13"></span></p><p class="c5"><span class="c15 c12 c25"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.4842.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874901251&amp;usg=AOvVaw2IxokfOSAVCO1wRr2Sfi_J">Going deeper with convolutions</a></span><span class="c12">.</span></p><p class="c5"><span class="c12">The inception modules are a combination of several convolutional modules of different kernel size</span><span class="c0">. They are so designed that at every layer of the network it can look at the activation map with different receptive fields discovering features at multiple scales. </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Inception has tried to approximate optimal sparse local representation &nbsp;of convolutional neural networks which is then repeated through depth. These sparse representations are nothing but the inception modules. They were very much motivated by aura at el [] where they explored to find analogy between biological neurons and nodes in the neural network. The analogy was briefly as follows - the neurons fire together wire together - in one layer of a network if two or more nodes fire together (shows high activation for some specific inputs) then they are very likely to be connected in the deeper layers of the network with good positive weights. So they were optimistic about connection between the similar convolutional modules repeated through the network.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Since they designed the network in a sparse style making the inception modules wider with a group convolutional modules. A large kernel i.e 5x5 would be very costly at the deeper layers with a lot of channels. For tackling this issue they used 1x1 convolutions a lot in their inception module. Note that 1x1 convolution / Network in Network [1] has its own benefits as well which also made the inception module more effective.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c12">The activations in the 1x1 convolutional also added non-linearity to the model. It is also an auxiliary classifier which helps to maintain the gradients at the deeper layers and helps to regularize the weights. </span><span class="c12 c15"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1312.4400.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874902125&amp;usg=AOvVaw2hfY9VsSC01crOb4IMqe_O">Network inside Network</a></span><span class="c0">&nbsp; is a concept of using a 1x1 convolution layer as a dense connector between feature maps. A 1x1 convolution can effectively be used for changing the output depth of convolutional layers. It has another useful effect. It operates like a dense layer applied at every pixel location depthwise. In the below example the 1x1 convolutional is working like a dense layer that takes the 5 input values (from 5 input channels) at each spatial location of the input activations and results 3 output values for the same spatial location of the 3 output channels. This makes it possible to work exactly like a dense layer for the stacked 2d arrays of data.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 903.16px; height: 705.46px;"><img alt="" src="files/cnn/image10.png" style="width: 903.16px; height: 705.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 844.50px; height: 596.79px;"><img alt="" src="files/cnn/image6.png" style="width: 844.50px; height: 596.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><a id="id.j2c4fqjlutpf"></a><p class="c5"><span class="c7">ResNet Family (2015)</span></p><hr><p class="c1"><span class="c6 c20"></span></p><p class="c1"><span class="c20 c6"></span></p><p class="c5"><span class="c12 c25 c28">(i) </span><span class="c15 c12 c25"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1512.03385.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874903434&amp;usg=AOvVaw1czexUNm0Olzm3lB-3WXJk">Deep Residual Learning for Image Recognition</a></span></p><p class="c5"><span class="c12 c25 c28">(ii) </span><span class="c15 c12 c25"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1603.05027.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874903727&amp;usg=AOvVaw1YrKWoNk2w6SkreaSVzf1s">Identity Mappings in Deep Residual Networks</a></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c2">Vanishing gradient Problem :</span></p><p class="c5"><span class="c0">1. The deeper the network goes the accuracy should be higher at least equal to the less deep network. It seemed to be a little counter-intuitive that after going deeper than a certain level the accuracy started decreasing. The reason is claimed to be a vanishing gradient. After backpropagating through so many layers the value of gradients starts getting very smaller that practically they go near zero. It Causes the training loss to &nbsp;decrease at a very slow rate. Note that although batch normalization helps to maintain a healthy norm of activations thus gradients throughout the network still that was not enough to handle this issue.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c2">Identity Connection :</span></p><p class="c5"><span class="c0">2. Residual net introduced identity connection / skip connection. It basically added the activation outputs of the i layer to i+1 layer. It continues through the entire length of the network creating a shortcut for upper layer activations to go deep inside the network. During back propagation it makes it possible for the very deep layers to have gradients with reasonable magnitude for making the learning process dynamic till the end. This kind of passage for gradients was introduced in Highway[] earlier.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c12">H(x) = F(x,{W</span><span class="c12 c18">i</span><span class="c12">}) + x{W</span><span class="c12 c18">s</span><span class="c0">}</span></p><p class="c5"><span class="c12">F(x,{W</span><span class="c12 c18">i</span><span class="c0">}) &nbsp;= H(x) - x</span></p><p class="c5"><span class="c0">Ws : Used for dimension matching before addition.</span></p><p class="c5"><span class="c0">For input of 256 and output 64 shape of Ws : (64,256)</span></p><p class="c5"><span class="c12">There are 3 options for use W</span><span class="c12 c18">s</span><span class="c0">&nbsp;by keeping the value of x as it is just altering the dimensions :</span></p><p class="c5"><span class="c0">&nbsp; &nbsp;1. Const value (all ones)</span></p><p class="c5"><span class="c0">&nbsp; &nbsp;2. Zero padding.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp;3. 1x1 convolution to reduce/increase the depth only.</span></p><p class="c5"><span class="c0">&nbsp; &nbsp;** 1x1 is also like network in network since it takes features from each input channel, multiplies with weights and produces points for each output channel.</span></p><p class="c5"><span class="c0">yy</span></p><p class="c5"><span class="c0">Conv2D(k x k x d, s) here k = kernel size, d output depth, s = stride</span></p><p class="c5"><span class="c0">BN &nbsp; &nbsp; = Batch Normalization</span></p><p class="c5"><span class="c0">ReLU = ReLU Activation</span></p><p class="c5"><span class="c0">ReLU and BN are illustrated with icon at some places just to emphasize </span></p><p class="c5"><span class="c0">their presence. &nbsp;Icon and text (BN / ReLU) are interchangeable everywhere.</span></p><p class="c5"><span class="c0">RB(a,b,c,s)</span></p><p class="c5"><span class="c0">&nbsp;a= input depth , b compressed depth, c = output depth, s = stride of last convolution layer</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 965.50px; height: 556.98px;"><img alt="" src="files/cnn/image13.png" style="width: 965.50px; height: 556.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1007.50px; height: 250.00px;"><img alt="" src="files/cnn/image3.png" style="width: 1007.50px; height: 250.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><a id="id.sbskz5eotl4r"></a><p class="c5"><span class="c19 c17">MobileNet :</span></p><p class="c1"><span class="c4"></span></p><hr><p class="c1"><span class="c4"></span></p><p class="c1"><span class="c4"></span></p><ul class="c14 lst-kix_ngpgdtyz59e3-0 start"><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1704.04861.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874906634&amp;usg=AOvVaw387qgy_TC9KdpK_hHQpYeN">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></span></li></ul><ul class="c14 lst-kix_jz7o59hmr7i0-0 start"><li class="c5 c9 li-bullet-0"><span class="c15 c12"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1801.04381.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874906929&amp;usg=AOvVaw3Pa-MG1AvyN4nzsh_vsiPg">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></span></li></ul><p class="c5"><span class="c2">Standard Convolution :</span></p><p class="c5"><span class="c12">&nbsp;A standard convolution operation with kernel size 3x3 and an input featuremap with size </span><span class="c12">HxWxC</span><span class="c0">. A standard convolution operation will multiply the input featuremap with a 3x3xC array of weights at every pixel location. It will then pass through the entire image in a sliding window manner to produce the output feature map. The sliding window works by moving the kernel by one pixel (for stride = 1) from left to right for each row of the pixel array one by one. For stride (s) greater than 1 it will move by &#39;s&#39; number of pixels instead of shifting just one pixel location. &nbsp;For an input image with 3 channels (C =3) it will be a 3x3x3 (the blue,green and red mask 3x3 kernel in the figure) array which will be element wise multiplied with the respective pixels input image array. The 27 values then summed up to produce one pixel of the output feature map. The described calculation is required just to produce one channel of the output feature map. For N channel output similar computation is repeated N times.</span></p><p class="c5"><span class="c0">So for a kernel size k</span></p><p class="c5"><span class="c12">input size &nbsp; = </span><span class="c12">HxWxC</span></p><p class="c5"><span class="c12">output size = </span><span class="c12">HxWxN</span><span class="c0">&nbsp;(considering the input is zero padded to keep output size same as input)</span></p><p class="c5"><span class="c0">At each pixel location KxKxC sized weight array is needed for producing one output channel. For N output channel N number of KxKxC parameters are needed.</span></p><p class="c5"><span class="c0">No of parameters = CxNxKxK</span></p><p class="c5"><span class="c0">This computation occurs at every pixel location of the input featuremap. </span></p><p class="c5"><span class="c0">So total computation cost = HxW x CxNxKxK </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c2">DepthWise Convolution :</span></p><p class="c5"><span class="c0">For depthwise convolution the separate 3x3 kernel is applied to each of the input channels. It will produce an array with the same dimension (with zero padding) of the input. Now at every pixel location a pointwise 1x1 convolution is applied to produce the desired number of output channels. It is explained in the Network In Network concept earlier.</span></p><p class="c5"><span class="c0">So for , kernel size k</span></p><p class="c5"><span class="c12">input size &nbsp; = </span><span class="c12">HxWxC</span></p><p class="c5"><span class="c12">output size = </span><span class="c12">HxWxN</span><span class="c0">&nbsp;(considering the input is zero padded to keep output size same as input)</span></p><p class="c5"><span class="c0">At each pixel location KxKxC sized weight array is needed for producing one output same as input feature map (with padding). For producing the N output channels a weight matrix of MxN is multiplied with this tensor.</span></p><p class="c5"><span class="c0">No of parameters = CxKxK + CxN</span></p><p class="c5"><span class="c0">This computation occurs at every pixel location of the input featuremap. </span></p><p class="c5"><span class="c0">So total computation cost = HxW x ( CxKxK + C x N) </span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">CxNxKxK --&gt; CxKxK + CxN</span></p><p class="c5"><span class="c0">&nbsp;</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 868.84px; height: 774.02px;"><img alt="" src="files/cnn/image8.png" style="width: 868.84px; height: 774.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><a id="id.6ag6q5b1z3xm"></a><p class="c5"><span class="c19 c17">Wide ResNet &nbsp;(WRN)</span></p><p class="c1"><span class="c22 c6"></span></p><hr><p class="c1"><span class="c6 c22"></span></p><p class="c1"><span class="c22 c6"></span></p><h1 class="c23" id="h.8p6vc955bvuf"><span class="c15 c12"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1605.07146.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874909726&amp;usg=AOvVaw23y7IdKIDhYoaRVI0vdRHI">Wide Residual Networks</a></span></h1><p class="c5"><span class="c2">Network Scaling :</span></p><p class="c5"><span class="c0">The ResNet like deep models add an extra convolutional layer at the bottom of an architecture which is known as depth of a network. Each of these convolution layer has a fixed number of output channels (od depth)/ dimensions. For example if a network has 5 layers with 3 , 32, 64, 128, 128, 256 output shapes from each of them respectively. Then no of layers = 5 would be the depth of the network and 32, 64, 128, 128, 256 should refer to the width of each layer. In WRN terminology k is the width multiplier. So if we make k=2 the output from each of the convolutional layers will be 32-&gt;64, 64-&gt;128, 128-&gt;256,128-&gt;256,256-&gt;512.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">So far most of the research focused on increasing the depth of the network by adding additional layers. Improving a small fraction of accuracy used to require almost doubling the network depth e.g ResNet50, ResNet101 etc. These deeper networks are prone to the issue of vanishing gradient which was tackled by introducing identity units in ResNet architecture. The identity connection ensures that gradient flows through the deepest layers but there is no mechanism that can force the intermediate layers weights to learn here. So it doesn&rsquo;t guarantee that all the intermediate layers are actually learning meaningful features. Some &ldquo;idle layers&rdquo; can just have nearly zeros/constant weights and pass the previous layer information to the next via identity connection. &nbsp;This issue may result in a network with lots of unused layers and parameters. WRN uses a less deep network so the vanishing gradient issue is automatically resolved , since identity connection is no more required; we don&#39;t need to worry about the unused layer issue.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Widening the network results in increased tendency of overfitting due to increased parameters per layer, a problem which was tackled by randomly disabling layers during training, a special case of dropout. WRN effectively used dropout during training for dealing with overfitting.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">It is claimed by the authors that a 16 layer WRN can perform similar to a 1000 layer deep network. The wide networks nearly 2 times faster converge as well.</span></p><p class="c1"><span class="c0"></span></p><ol class="c14 lst-kix_ijcak9j4a5eq-0 start" start="1"><li class="c5 c9 li-bullet-0"><span class="c0">Exploring width dimensions opposing the ongoing trend of depth increment.</span></li><li class="c5 c9 li-bullet-0"><span class="c0">Using dropout efficiently for reducing overfitting.</span></li><li class="c5 c9 li-bullet-0"><span class="c0">Improved a few weaknesses of ResNet networks by reducing depth i.e unused layers and parameters, faster training etc.</span></li><li class="c5 c9 li-bullet-0"><span class="c0">Making a 16 times less deep (but wider) network to perform almost equally with comparable parameters.</span></li></ol><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 982.04px; height: 331.68px;"><img alt="" src="files/cnn/image7.png" style="width: 982.04px; height: 331.68px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c2">Figure : Figure from WRN paper by Zagoruyko at el.</span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span class="c2">Design Choices :</span></p><ul class="c14 lst-kix_gf9vecvc59t-0 start"><li class="c5 c9 li-bullet-0"><span class="c0">In the figure above (b) the model is used for making the network thinner. Since WRN intends to explore the width of the network it is counter intuitive to use (a) so the authors preferred using the (c ) for WRN. It is like (d) when dropout is added to the pattern,</span></li><li class="c5 c9 li-bullet-0"><span class="c0">Since ResNet-V2 showed BN-ReLU-Conv is better than Conv-BN-ReLU , WRN also followed the design pattern from ResNet-V2.</span></li><li class="c5 c9 li-bullet-0"><span class="c0">The width factor &ldquo;k&rdquo; implies how many times more channels result from each convolution layer.</span></li></ul><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c2">Results :</span></p><p class="c5"><span class="c0">In the below table we can see that -</span></p><ul class="c14 lst-kix_fh2qtwmwsffy-0 start"><li class="c5 c9 li-bullet-0"><span class="c0">Increasing the width reduces the loss almost all the time e.g - WRN-40-8 is much better than WRN-40-1.</span></li><li class="c5 c9 li-bullet-0"><span class="c0">A much less deep network WRN-22-10 (4.17) has lower loss than a deeper one WRN-40-8 (4.66) while having comparable parameters.</span></li></ul><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 594.89px; height: 355.90px;"><img alt="" src="files/cnn/image11.png" style="width: 594.89px; height: 355.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">&nbsp;</span></p><p class="c5"><span class="c2">Table : Taken from WRN paper by Zagoruyko at el. Loss for different depth and width factors (k), k=1 means width is the same as parent ResNet.</span></p><p class="c1"><span class="c2"></span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span class="c6 c25">&nbsp;</span><a id="id.jzk9bc3j1ill"></a><span class="c7">EfficientNet </span></p><p class="c1"><span class="c6 c10"></span></p><hr><p class="c1"><span class="c10 c6"></span></p><p class="c1"><span class="c10 c6"></span></p><h1 class="c23 c27" id="h.oxandx2tth1j"><span class="c15 c12 c25"><a class="c8" href="https://www.google.com/url?q=https://arxiv.org/pdf/1905.11946.pdf&amp;sa=D&amp;source=editors&amp;ust=1673210874913035&amp;usg=AOvVaw1lJGLxjwWADQz_hUR3K_E-">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></span></h1><p class="c1"><span class="c0"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 598.02px; height: 402.79px;"><img alt="" src="files/cnn/image2.png" style="width: 598.02px; height: 402.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c10 c6"></span></p><p class="c1"><span class="c2"></span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 896.17px; height: 353.22px;"><img alt="" src="files/cnn/image9.png" style="width: 896.17px; height: 353.22px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span class="c2">To Do -</span></p><p class="c1"><span class="c2"></span></p><p class="c5"><span class="c2">Network Architecture Search.</span></p></body></html>