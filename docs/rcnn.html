<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_qptiuu6mlbhu-0{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-1{list-style-type:none}.lst-kix_b2x7vhnewr2g-0>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-0}ul.lst-kix_qptiuu6mlbhu-2{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-1.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-1 0}.lst-kix_b2x7vhnewr2g-5>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-5}ul.lst-kix_qptiuu6mlbhu-7{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-8{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-3{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-4{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-4.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-4 0}ul.lst-kix_qptiuu6mlbhu-5{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-6{list-style-type:none}.lst-kix_b2x7vhnewr2g-1>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-1}.lst-kix_b2x7vhnewr2g-7>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-7,lower-latin) ". "}.lst-kix_qptiuu6mlbhu-7>li:before{content:"-  "}.lst-kix_qptiuu6mlbhu-8>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-5.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-5 0}.lst-kix_b2x7vhnewr2g-5>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-5,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-6>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-6,decimal) ". "}.lst-kix_b2x7vhnewr2g-4>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-4,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-1>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-1,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-2>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-2,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-6>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-6}.lst-kix_b2x7vhnewr2g-3>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-3,decimal) ". "}.lst-kix_b2x7vhnewr2g-3>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-3}ol.lst-kix_b2x7vhnewr2g-8.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-8 0}.lst-kix_b2x7vhnewr2g-0>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-0,decimal) ". "}ol.lst-kix_b2x7vhnewr2g-6.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-6 0}.lst-kix_b2x7vhnewr2g-8>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-8}.lst-kix_b2x7vhnewr2g-2>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-2}ol.lst-kix_b2x7vhnewr2g-1{list-style-type:none}.lst-kix_qptiuu6mlbhu-2>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-2{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-0{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-5{list-style-type:none}.lst-kix_qptiuu6mlbhu-3>li:before{content:"-  "}.lst-kix_qptiuu6mlbhu-4>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-6{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-3{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-2.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-2 0}ol.lst-kix_b2x7vhnewr2g-4{list-style-type:none}.lst-kix_qptiuu6mlbhu-5>li:before{content:"-  "}.lst-kix_qptiuu6mlbhu-6>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-7{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-8{list-style-type:none}.lst-kix_b2x7vhnewr2g-8>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-8,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-4>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-4}ol.lst-kix_b2x7vhnewr2g-7.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-7 0}ol.lst-kix_b2x7vhnewr2g-0.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-0 0}.lst-kix_b2x7vhnewr2g-7>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-7}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_qptiuu6mlbhu-1>li:before{content:"-  "}.lst-kix_qptiuu6mlbhu-0>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-3.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-3 0}ol{margin:0;padding:0}table td,table th{padding:0}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c5{background-color:#ffffff;font-size:12pt;font-family:"Times New Roman";color:#202124;font-weight:400}.c0{vertical-align:sub;font-size:12pt;font-family:"Times New Roman";font-weight:400}.c13{font-size:14pt;font-family:"Times New Roman";font-style:italic;font-weight:700}.c9{color:#666666;text-decoration:none;vertical-align:baseline;font-style:italic}.c11{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c19{color:#333333;text-decoration:none;font-style:normal}.c1{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c16{color:#000000;text-decoration:none;font-style:normal}.c21{font-weight:700;font-size:16pt;font-family:"Times New Roman"}.c25{font-weight:700;font-size:15pt;font-family:"Times New Roman"}.c29{font-weight:700;font-size:14pt;font-family:"Times New Roman"}.c31{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c18{color:#000000;text-decoration:none;font-style:italic}.c6{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c32{color:#666666;text-decoration:none}.c20{margin-left:36pt;padding-left:0pt}.c24{color:#000000;text-decoration:none}.c26{color:#333333;text-decoration:none}.c14{padding:0;margin:0}.c7{color:inherit;text-decoration:inherit}.c28{text-decoration:none;font-style:normal}.c27{font-style:italic}.c15{background-color:#f8f9fa}.c17{vertical-align:baseline}.c12{vertical-align:super}.c23{vertical-align:sub}.c22{color:#67ab9f}.c3{height:11pt}.c10{color:#202124}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c31 doc-content"><p class="c8"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c1">&nbsp; &nbsp; &nbsp;</span><span class="c16 c17 c21">&nbsp; Object Detection : RCNN Family</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2"><span class="c1 c16 c17">&nbsp;</span></p><p class="c2"><span class="c1 c27">Object Detection</span><span class="c4">&nbsp;is the combination of two sub tasks (i) Object Localization (ii) Object Classification. For the purpose of implementation, we can further break it down to total three steps -</span></p><p class="c2"><span class="c1">[A]</span><span class="c6">&nbsp;</span><span class="c1">Object localization :</span><span class="c4">&nbsp;This step takes the rgb image and extracts it to the N set of (x,y,w,h) bounding box coordinates for potential objects present in it. It can be done using a specific algorithm e.g Selective search. </span></p><p class="c2"><span class="c1">[B]</span><span class="c6">&nbsp;</span><span class="c1">Feature map Extraction :</span><span class="c4">&nbsp;It takes an image array and converts it to a high dimensional feature map array using a convolutional layer based neural network e.g vgg16, resnet 50 etc.</span></p><p class="c2"><span class="c1">[C]</span><span class="c6">&nbsp;</span><span class="c1">Classification :</span><span class="c4">&nbsp;This step takes each of the extracted feature maps and classifies it to an object category using dense layers or some machine learning &nbsp;based classifier algorithm e.g SVM (support vector machine). Detailed explanation of step [A],[B] and [C] can be found on the right section.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c13 c24 c17">RCNN</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">The RCNN method uses the following modules to perform object detection :</span></p><p class="c2"><span class="c1">Selective Search &nbsp;:</span><span class="c4">&nbsp;It is an algorithm for generating region proposal suggestions. It suggests potential bboxes (bounding boxes) that may contain an object. &#39;Objectness&#39; is another similar algorithm that can be used for region proposal generation. SOTA works now use a simple CNN based architecture (region proposal network) for this purpose. See the right section for more about selective search. </span></p><p class="c2"><span class="c1">Feature Extractor (FE) : &nbsp;</span><span class="c4">It can be any deep convolutional neural network. The output of the network is truncated from the last dense layer (if a 1d feature vector is necessary) or any of the intermediate convolutional layers (if a 2 dimensional feature map is required). The 2d featuremap also preserves the spatial relationship with the input image due to the sliding nature of convolution operation. &nbsp;For RCNN they don&#39;t require the spatial relation, so use the output of a dense layer as a 1d feature vector. For fast RCNN it will need a 2d feature map instead.</span></p><p class="c2"><span class="c1">Binary SVM :</span><span class="c4">&nbsp;Support vector machine algorithm is used for binary (yes/no) classification every single class. Binary SVM is specially suited for single class classification since it tries to select the hyperplane that separates the two classes with highest margin.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 850.50px; height: 705.15px;"><img alt="" src="files/rcnn/image9.png" style="width: 850.50px; height: 705.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c1 c9">Figure : RCNN (top) and Fast RCNN (bottom)</span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c1">BBOX Regression : </span></p><p class="c8"><span class="c4">This approach was Inspired from the Deformable part model by Girshick at el. 2010. Let&rsquo;s assume the following -</span></p><ul class="c14 lst-kix_qptiuu6mlbhu-0 start"><li class="c8 c20 li-bullet-0"><span class="c6">The initial bbox predictions are P = &nbsp;[P</span><span class="c0">x</span><span class="c6">,P</span><span class="c0">y</span><span class="c6">,P</span><span class="c0">w</span><span class="c6">,P</span><span class="c0">h</span><span class="c4">]</span></li><li class="c8 c20 li-bullet-0"><span class="c6">The initial bbox ground truths are G = [G</span><span class="c0">x</span><span class="c6">,G</span><span class="c0">y</span><span class="c6">,G</span><span class="c0">w</span><span class="c6">,G</span><span class="c0">h</span><span class="c4">]</span></li><li class="c8 c20 li-bullet-0"><span class="c6">d</span><span class="c0">x</span><span class="c6">(P) , d</span><span class="c0">y</span><span class="c6">(P), d</span><span class="c0">w</span><span class="c6">(P), d</span><span class="c0">h</span><span class="c4">(P) is the output of regression i.e the errors/offsets to be corrected on P for matching with G more accurately.</span></li></ul><p class="c8"><span class="c6">For x and y, a scale invariant transformation d</span><span class="c0">x</span><span class="c6">(P) d</span><span class="c0">y</span><span class="c4">(P) is learnt. It is an offset from an initial less accurate prediction by selective search, which needs to be added to the predictions. Since dx,dy are scale invariant offset, they are multiplied with the size (Pw , Ph) of the box before adding to the prediction box i.e &nbsp;Gx = Pw * Dx(P) + Px. For w and h a log scale transformation is learnt. It makes it capable of learning a wide range of change in w and h direction. Unlike x,y offsets, here it is just the difference of their logarithmic value instead of their actual value (eq [5] below).</span></p><p class="c8"><span class="c6">&nbsp;Most of the SOTA methods use an updated version of the bbox regression method introduced in RCNN. For example yolo-v3 uses &nbsp;G</span><span class="c0">x</span><span class="c6">&nbsp;= Sigmoid(P) +P</span><span class="c0">x </span><span class="c6">. They wrap the d</span><span class="c0">x &nbsp;</span><span class="c4">with a sigmoid function to restrict its output between 0-1. Some methods use a fixed grid system with a potential bbox at every location of the featuremap as an initial bbox then corrects it with regression. We will explain about these in detail in respective sections.</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c1">Hard Negative Mining :</span><span class="c4">&nbsp; It is an approach that oversamples the difficult samples more and thus tries to make it learn faster. It was possible to achieve good accuracy after just one pass over the data using hard negative mining for the SVM models.</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 869.44px; height: 459.77px;"><img alt="" src="files/rcnn/image6.png" style="width: 869.44px; height: 459.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c18 c17">RCNN Algorithm :</span></p><p class="c2"><span class="c4">1. [A] Selective Search algorithm is used for generating ~ 2000 sets of bounding box coordinate (x,y,w,h) proposals per image. Using the bbox coordinates found from selective search, image RoIs (Region of Interest) of different sizes are cropped.</span></p><p class="c2"><span class="c4">2. [A] The SVM classifier can only take fixed sized inputs. So the outputs of the feature extractor should have constant size. It is possible if the input image RoIs to the feature extractor have fixed size. For this reason each RoI is &#39;wrapped&#39; to a fixed size [resizing with bilinear/nearest sampling] before inputting them to the feature extractor model. </span></p><p class="c2"><span class="c4">3. [B] A deep convolutional neural network is used for extracting feature maps from each of the image RoI one by one. The model output is truncated from the bottom and output from a dense layer with 4096 nodes is extracted as a feature map.</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Pre training with ICSLV-2012 images with 200 classes lr 0.01 on CNN architecture by krizhevsky at el. 2012.</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Oversampling positive classes 32 positive sample + 96 neg sample per mini-batch</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Fine tuning with lr 0.001 with wrapped region proposal with N + 1 class. N is no of object class 1 is background</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During fine tuning a region proposal with more than 0.50 IoU with a positive sample was taken as positive.</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During Feature Extraction the last layer is removed and features from an internal dense layer are collected. i.e 1024 or 2048 features</span></p><p class="c2"><span class="c4">4. [C] &nbsp;Separate binary classifiers for each of the N (N=no of categories) are trained to categorize the fmaps to respective object categories.</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; One binary SVM classifier is used for each of the classes.</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Hard negative mining [4] is found to be useful for fast convergence. For detail see right section.</span></p><p class="c2"><span class="c4">5. [A*] Fine tuning of the localization boxes are done by bbox regression using Ridge regression with lambda = 1000. The regressor only learns the x,y offset from the corner of the bbox predicted by selective search. For w,h it is log offset instead. We have already explained it above.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">As per our today&#39;s practice on SOTA object detection algorithms several things might seem to be redundant -</span></p><p class="c2"><span class="c4">[i] Extracting feature maps for every RoI one by one. [It is solved in Fast RCNN eventually]</span></p><p class="c2"><span class="c4">[ii] Using a separate SVM for each class instead of using the softmax class outputs of the feature extractor network. [Improved in Fast RCNN]</span></p><p class="c2"><span class="c4">[iii] The word &quot;wrapped&quot; image is used in the paper for resizing each Fmap RoI to a fixed size. Nowadays a good practice is using bilinear sampling. Note that - rcnn first resizes (wrapped) each of the image RoI then inputs them to Feature extractor but Fast RCNN will resize the feature maps using RoI pooling layer.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">&nbsp;Using additional SVM classifier instead of the dense layer output from the feature extractor network :</span></p><p class="c2"><span class="c4">Quote from the paper&#39;s appendix B : &quot;We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classi- fier was trained on randomly sampled negative examples rather than on the subset of &ldquo;hard negatives&rdquo; used for SVM training.&quot;</span></p><p class="c2"><span class="c4">Anyway, in Fast RCNN the authors were finally able to do it using a small dense network after the feature extractor, without any SVM classifier.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c13 c17 c26">Fast RCNN</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2 c3"><span class="c1 c26 c17 c27"></span></p><p class="c2"><span class="c1 c16 c17">Improvements in Fast RCNN:</span></p><p class="c2"><span class="c4">- Fast RCNN eliminated one by one RoI input into the feature extractor. There are lots of common regions between RoI&#39;s specially when nearly 2000 RoI generated from one image. This method applies a feature extractor for the whole image just once and then it crops the corresponding RoI for each of the ~ 2000 corresponding regions from the feature map. </span></p><p class="c2"><span class="c4">- Replacing image wrapping with RoI Pooling.</span></p><p class="c2"><span class="c4">- Getting rid of per class svm by using the softmax outputs from dense layers after feature extractor.</span></p><p class="c2 c3"><span class="c1 c28 c17 c22"></span></p><p class="c2"><span class="c1 c22">&nbsp;</span><span class="c1 c16 c17">Fast RCNN Algorithm :</span></p><p class="c2"><span class="c4">1.[A] Selective Search generates ~ 2k RoI proposals per image. </span></p><p class="c2"><span class="c4">2.[B] A CNN based network i.e VGG16 is used for feature extraction.</span></p><p class="c2"><span class="c4">3.[B] The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps RoIs so that it can fit into the dense layer afterwards. (H=7, W=7 for VGG 16) </span></p><p class="c2"><span class="c6">4. [C] The last fully connected layer is replaced with 2 sibling dense layers (i) one &nbsp;for classification (ii) another for bbox regression. For regression it adopted the same fine tuning mechanism (optimizing t</span><span class="c0">x</span><span class="c6">,t</span><span class="c0">x</span><span class="c6">,t</span><span class="c0">x</span><span class="c6">,t</span><span class="c0">x</span><span class="c4">) like RCNN. </span></p><p class="c2 c3"><span class="c4"></span></p><p class="c8"><span class="c1 c17 c18">RoI Pooling Layer</span></p><p class="c8"><span class="c4">RoI Pooling Layer : The &nbsp;operator [*] represents the flooring operation here. e.g [1.13] = 1.00</span></p><p class="c8"><span class="c4">The functionality of RoI Pooling layer can be divided into two parts -</span></p><p class="c8"><span class="c1">[1] Scaling the image RoIs to fmap dimension : </span><span class="c4">Due to multiple polling operations in the consecutive convolutional layers, the feature map array has smaller (e.g 16~32 times smaller) width and height compared to the original image. In the example below the feat map is 16 times smaller than the original image in terms of width and height. The RoI Pooling layer scales down each of the RoI coordinates (generated at the image scale) so that it fits on top of the feature map in the corresponding feature location for that image RoI. Each RoI coordinate needs to be divided by the scale factor (here it is 16) for fitting on the feature map. In this way the 188x188 RoI window coordinates are converted to 11x11 below. The center coordinates (x,y) of the RoI is also divided by the factor like (w,h) which is not shown in the figure for brevity.</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c1">[2] Resizing the Fmap RoI to fit the fixed input of dense layers :</span><span class="c4">&nbsp;The dense layer has fixed input size i.e 25 dense nodes. If a 5x5x1 feature map tensor is flattened (5x5x1=25) only then it will fit the dense input. For 5x5x10 the dense should have 250 input nodes. For each of the RoI Feature maps do a maxpool operation with kernel size ([h/h&#39;&#39;], [w/w&#39;&#39;]) for below example it will be ([11/5],[11/5]) = (2,2). Now an arbitrary feature map of size 11x11 is operated with a maxpool of kernel size 2 for fitting fixed 5x5 input size of the dense layers afterward. Another example can be like - an orbitary Fmap RoI of size 20x20 would require a kernel of 4x4 for fitting it into 5x5 input size</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 701.50px; height: 576.46px;"><img alt="" src="files/rcnn/image12.png" style="width: 701.50px; height: 576.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">&nbsp;Figure : RoI Pooling Layer top 2d view with numerical examples bottom 3d view of the same thing</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c27">&nbsp;</span><span class="c13 c17 c24">Faster RCNN</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2"><span class="c1 c16 c17">Improvements :</span></p><p class="c2"><span class="c4">- The selective search is replaced with a Region Proposal Network that mostly shares weights with the Feature Extractor Network.</span></p><p class="c2"><span class="c4">- The region proposals are further broken down to 3 different scales with 3 different aspect ratios called anchor boxes.</span></p><p class="c2 c3"><span class="c1 c18 c17"></span></p><p class="c2"><span class="c1 c18 c17">The Algorithm:</span></p><p class="c2"><span class="c1 c16 c17">[A] RPN (Region Proposal Network): </span></p><p class="c2"><span class="c4">&nbsp;- Region Proposal Network proposes possible box coordinates and objectness confidence at each pixel location of the feature map. This is a class agnostic box predictor so it has nothing to do with the category of the object. The output of the RPN module is converted to bbox coordinates at every feature map location and trained with the bbox regressor. </span></p><p class="c2"><span class="c6">&nbsp; - The output of the last shared convolutional layer is passed through a n</span><span class="c1">x</span><span class="c4">n convolution layer (e,g 3x3) then forwarded to - 2 sibling 1x1 convolution layers for bbox and confidence prediction at every fmap location. The 1x1 convolution layer can also be interpreted as a dense layer applied at every pixel locations and results in an array with same height and width while having depth of 4k and 2k respectively, k= number of anchor box per location. &nbsp;</span></p><p class="c2"><span class="c4">&nbsp;- At each pixel location we can get k=9 boxes and respective confidence scores. It will make it possible to detect two objects centered at the same image location (having different aspect ratio or scale). If the output of the RPN has width m and height n the depth would be 2k and 4k for confidence and bbox outputs. So a total of m*n*k set of (x,y,w,h) can be extracted from the depthwise 4k data points at each of the m*n locations. Similarly m*n*k set of (object conf,background conf) can be found from 2k data points depthwise. For more on it see &#39;Anchor box&#39; section below.</span></p><p class="c2"><span class="c4">&nbsp;- The RPN is trained with binary classification labels. Among the potential k boxes at every feat. map location, boxes having IoU more than 0.7 (if multiple is found then the highest one is taken only) it is taken as a positive sample. Boxes with less than 0.3 IoU with all the GT (Ground Truth) boxes are taken as negative samples (background), others are ignored. </span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">[B] Feature Extractor :</span></p><p class="c2"><span class="c4">&nbsp;- It used the CNN (Convolutional Neural Network) network feature extractor which shares its weights with the RPN (until the RPN specific layers). It used the VGG16 and also ZF [zeiler at el.] pre-trained on ImageNet dataset for demonstrating their algorithm performance.</span></p><p class="c2"><span class="c4">&nbsp;- The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps for every Fmap RoI so that it can fit into the first dense layer (H=7, W=7 for VGG 16) </span></p><p class="c2"><span class="c4">&nbsp;- RoI pooling layer projects the RoI coordinates on top of the feature map array in a max pooling manner, the difference here is it adjusts the pooling window size for keeping the output size fixed for all the fmap. RoIs.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1">[C] Classifier :</span><span class="c4">&nbsp;Each of the output (fmap RoIs) from the RoI Pooling layer is sent to a small dense network ending with 2 sibling dense layers (i) one for classification (ii) another for bbox regression. The regressor here </span></p><p class="c2"><span class="c4">learns the x,y offset and log offset for w and h from the prior boxes predicted by RPN initially.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 907.77px; height: 801.39px;"><img alt="" src="files/rcnn/image13.png" style="width: 907.77px; height: 801.39px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c9 c1">Figure : Faster RCNN</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c18 c17">Anchor Box / Multi Box / Default Box</span></p><p class="c2 c3"><span class="c1 c17 c19"></span></p><p class="c2"><span class="c6">Gw = P</span><span class="c0">w</span><span class="c4">&nbsp;exp(width offset from RPN output) --- [1] &nbsp; &nbsp; &nbsp; </span></p><p class="c2"><span class="c6">- bbox width from RPN output = log(Gw/P</span><span class="c0">w</span><span class="c6">) = log(Gw) -log( P</span><span class="c0">w</span><span class="c4">)</span></p><p class="c2"><span class="c6">Gh &nbsp;= P</span><span class="c0">h</span><span class="c4">&nbsp;exp(height offset from RPN output) &nbsp;---- [2]</span></p><p class="c2"><span class="c6">- bbox width from RPN output = log(Gw/P</span><span class="c0">w</span><span class="c6">) = log(Gw) -log( P</span><span class="c0">w</span><span class="c4">)</span></p><p class="c2"><span class="c4">Anchor boxes are prior box estimates at every feature map location. They are determined before the training and fed into the network beforehand. &nbsp;The prior boxes (Pw , Ph) at every image location are further fine tuned by multiplying with the exponential value of the corresponding RPN outputs (see eq [1] and [2] above). For width and height a log offset is learnt between the prior box and GT box like Fast-RCNN. It is already explained in the bbox regression section above. &nbsp;In short, the model basically learns how to fit each of the anchor boxes to the object present at that location by multiplying it with a factor i.e exp(RPN output). At the same time it learns two confidence scores for object and background for each anchor box. That&#39;s why the 2k scores are associated with each anchor box.</span></p><p class="c2"><span class="c6">Anchor boxes were first used by erhan at el. , szegedy at el in their respective works. It is referred to as the multibox approach or default box in other works. &nbsp;</span><span class="c6 c15">Initial anchor box methods used different anchors (prior boxes) at different locations of the image resulting in a lot of unique anchors. For example, the</span><span class="c4">&nbsp;multibox method by szegedy at el. was not translation invariant. It used 800 anchor boxes at different locations of the image. They found this 800 anchors by k means clustering of the [x,y,w,h] of all the Ground truth bboxes. In contrast, faster rcnn does k means clustering for h,w only irrespective of their x,y location in image. These 9 boxes can just fit any arbitrary x,y location in the image instead of 800 different boxes for various x,y locations. They conducted some experiments with different values of k and found k=9 anchor boxes are optimal for achieving the best mAP. &nbsp;This 9 boxes are now capable of producing w*h*9 ~ 2400 x 9 RoI per image at maximum.</span></p><p class="c2"><span class="c4">&nbsp;-&gt; For Multibox the shape of tensor for RoI prediction was 4+1* 800 dimensional &nbsp;fully connected layer</span></p><p class="c2"><span class="c4">&nbsp;-&gt; &nbsp;For Faster RCNN it reduced to 4+2*9 convolutional output layer.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 637.50px; height: 310.45px;"><img alt="" src="files/rcnn/image5.png" style="width: 637.50px; height: 310.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c9 c1">&nbsp; &nbsp;Image from Faster RCNN paper Ren et al 2016</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 488.50px; height: 406.64px;"><img alt="" src="files/rcnn/image7.png" style="width: 488.50px; height: 406.64px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure : &nbsp;Total 9 anchor boxes at one pixel locations</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c8"><span class="c1 c16 c17">RPN Loss :</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c6">x, x</span><span class="c0">a</span><span class="c6">, x</span><span class="c6 c12">*</span><span class="c4">&nbsp;are prediction , anchor box, ground truth.</span></p><p class="c8"><span class="c6">t</span><span class="c0">i </span><span class="c6">&nbsp;: x,y,w,h of &nbsp;i</span><span class="c6 c12">th</span><span class="c6">&nbsp;bbox , p</span><span class="c0">i</span><span class="c6">&nbsp;and p</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c4">&nbsp;are prediction and GT confidence.</span></p><p class="c8"><span class="c6">p</span><span class="c6 c12">obj</span><span class="c6">,p</span><span class="c6 c12">bg</span><span class="c4">&nbsp;in for object and background confidence.</span></p><p class="c8"><span class="c6">L</span><span class="c0">reg</span><span class="c6">(t</span><span class="c0">x</span><span class="c6">&nbsp;, t</span><span class="c0">x</span><span class="c6 c12">*</span><span class="c6">&nbsp;) &nbsp;= (x-x*)/w &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;L</span><span class="c0">reg</span><span class="c6">(t</span><span class="c0">y</span><span class="c6">&nbsp;, t</span><span class="c0">y</span><span class="c6 c12">*</span><span class="c6">&nbsp;) = (y - y</span><span class="c6 c12">*</span><span class="c6">)/h</span><span class="c0">a</span><span class="c4">&nbsp;;</span></p><p class="c8"><span class="c6">L</span><span class="c0">reg</span><span class="c6">(t</span><span class="c0">w</span><span class="c6">&nbsp;, t</span><span class="c0">w</span><span class="c6 c12">*</span><span class="c6">&nbsp;) = log(w / w</span><span class="c6 c12">*</span><span class="c6">)/w</span><span class="c0">a</span><span class="c6">&nbsp; &nbsp;; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;L</span><span class="c0">reg</span><span class="c6">(t</span><span class="c0">h</span><span class="c6">&nbsp;, t</span><span class="c0">h</span><span class="c6 c12">*</span><span class="c6">&nbsp;) &nbsp;= log(h / h</span><span class="c6 c12">*</span><span class="c6">)/h</span><span class="c0 c16">a</span></p><p class="c8"><span class="c4">&nbsp;</span></p><p class="c8"><span class="c6">L</span><span class="c0">i</span><span class="c6">&nbsp;= &nbsp;L</span><span class="c0">cls</span><span class="c6">&nbsp;+ L</span><span class="c0 c16">bbox</span></p><p class="c8"><span class="c6">L</span><span class="c0">cls</span><span class="c6">&nbsp; = 1/N</span><span class="c0">cls</span><span class="c6">&nbsp;x </span><span class="c5">&Sigma;</span><span class="c6 c12 c10">N</span><span class="c0 c10">i </span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c6 c12 c10">M</span><span class="c0 c10">j </span><span class="c6">-p</span><span class="c6 c12">obj</span><span class="c0">i</span><span class="c6 c12">* </span><span class="c6">log(p</span><span class="c6 c12">obj</span><span class="c0">i</span><span class="c6">) - p</span><span class="c6 c12">bg</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">log(p</span><span class="c6 c12">bg</span><span class="c0">i</span><span class="c4">) ]</span></p><p class="c8"><span class="c6">L</span><span class="c0">reg</span><span class="c6">&nbsp;= &lambda; x 1/N</span><span class="c0">reg &nbsp;</span><span class="c6">x </span><span class="c5">&Sigma;</span><span class="c6 c12 c10">N</span><span class="c0 c10">i</span><span class="c6">&nbsp;p</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">&nbsp;x Lreg(t</span><span class="c0">i</span><span class="c6">, t</span><span class="c0">i</span><span class="c6">* &nbsp;) + Robust L</span><span class="c0">1</span><span class="c4">&nbsp;</span></p><p class="c8"><span class="c6">here N</span><span class="c0">cls</span><span class="c6">&nbsp;= 256 ; N</span><span class="c0">reg</span><span class="c4">&nbsp;= 2400 ; &nbsp;Lambda = 10 </span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c1">Fast-RCNN</span><span class="c1 c12">*</span><span class="c1 c16 c17">&nbsp; &nbsp;Loss :</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c4">Here the regression loss is mostly the same except for the fact that it represents the RPN predicted coordinates here (instead of the GT). So here the loss function learns the offset from RPN predicted coordinates.</span></p><p class="c8"><span class="c6">Loss = L</span><span class="c0">cls</span><span class="c6">&nbsp;+ L</span><span class="c0 c16">bbox</span></p><p class="c8"><span class="c6">L</span><span class="c0">bbox</span><span class="c6">&nbsp;= 1/N x </span><span class="c5">&Sigma;</span><span class="c5 c12">N</span><span class="c5 c23">i</span><span class="c6">&nbsp;P</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">&nbsp;x (t </span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">- t</span><span class="c0">i</span><span class="c6">&nbsp;) ^2 ; t</span><span class="c0">i </span><span class="c6">&nbsp;: x,y,w,h of &nbsp;i</span><span class="c6 c12">th</span><span class="c4">&nbsp;bbox + Robust L1</span></p><p class="c8"><span class="c6">L</span><span class="c0">cls</span><span class="c6">&nbsp;= 1/N x </span><span class="c5">&Sigma;</span><span class="c5 c12">N</span><span class="c5 c23">i</span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c6 c12 c10">M</span><span class="c0 c10">j </span><span class="c6">-p</span><span class="c6 c12">j</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">log(p</span><span class="c6 c12">j</span><span class="c0">i</span><span class="c4">) ]</span></p><p class="c8"><span class="c6">i : i</span><span class="c6 c12">th</span><span class="c4">&nbsp;data sample in mini batch</span></p><p class="c8"><span class="c6">j : j</span><span class="c6 c12">th</span><span class="c4">&nbsp;object category</span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8 c3"><span class="c4"></span></p><p class="c8"><span class="c1 c16 c17">Back propagation :</span></p><p class="c8"><span class="c1">(1) Alternate training :</span><span class="c4">&nbsp;(Used by the authors)</span></p><p class="c8"><span class="c4">RPN and Fast-RCNN share the same network (the &quot;Shared CNN Layers&quot; part in above figure). Rest of the layers are task specific i.e RPN Layers and Dense Layers at the end. Both of the networks are initialized with imagenet pre-trained weights for either VGG-16 (first 13 CNN layers) or ZF network (first 5 CNN layers) &nbsp;[zeiler at el.]. *Fast-RCNN is a special case of Fast-RCNN, &nbsp;that will be trained with region proposals from RPN predictions instead of selective search. </span></p><p class="c8"><span class="c4">- First the RPN is trained. </span></p><p class="c8"><span class="c4">- Then using the region proposal results of RPN is used as input for training the *Fast-RCNN.</span></p><p class="c8"><span class="c4">- Next RPN is initialized with the Fast-RCNN weights (only the shared part) and trained again for the RPN specific layers at the bottom are trained only keeping the shared layers fixed.</span></p><p class="c8"><span class="c4">- Finally, both the *Fast-RCNN is trained again keeping the common layers fixed.</span></p><p class="c8"><span class="c1">(2) Approximate Joint training :</span><span class="c4">&nbsp;Using the same network for RPN and Fast-RCNN* from the beginning. Training them together. For Fast-RCNN the outputs from RPN during forward pass are taken as input assuming them fixed predefined values. Since the RoI pooling layer is not differentiable that&#39;s why they needed to consider the outputs of RPN fixed values.</span></p><p class="c8"><span class="c1">(3) Non Approximate Joint training :</span><span class="c4">&nbsp;In this method they proposed using a differentiable RoI Pooling layer where so that the outputs taken from RoI pooling and fed back to the Fast-RCNN would be a part of the back propagation chain</span></p><p class="c8"><span class="c4">Authors showed their results with method (1) in the paper.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c16 c17 c25">Mask RCNN</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2"><span class="c1 c16 c17">Improvements in Mask RCNN :</span></p><p class="c2"><span class="c4">- RoI Pooling layer is replaced with RoI align layers.</span></p><p class="c2"><span class="c4">- Additional semantic segmentation branch is added for predicting both bbox and segmentation at a time. I will refer it as [D]</span></p><p class="c2 c3"><span class="c1 c17 c22 c28"></span></p><p class="c2"><span class="c1 c16 c17">The Algorithm Mask RCNN:</span></p><p class="c2"><span class="c4">The algorithm is very similar to faster rcnn. The major difference here is that they used a more accurate version of the RoI Pooling layer. They also added a segmentation branch to the original network and was able to produce accurate semantic segmentation along with bounding box prediction.</span></p><p class="c2"><span class="c4">&nbsp;</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1">Mask RCNN Loss</span></p><p class="c2"><span class="c6">Loss = L</span><span class="c0">cls</span><span class="c6">&nbsp;+ L</span><span class="c0">bbox </span><span class="c6">+ L</span><span class="c0 c16">seg</span></p><p class="c2"><span class="c6">L</span><span class="c0">cls</span><span class="c6">&nbsp;and L</span><span class="c0">reg</span><span class="c4">&nbsp;are similar to Faster RCNN.</span></p><p class="c2"><span class="c6">L</span><span class="c0">seg = </span><span class="c6">1/N x </span><span class="c5">&Sigma;</span><span class="c5 c12">N</span><span class="c5 c23">i</span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c0 c10">jk </span><span class="c6">-y</span><span class="c6 c12">jk</span><span class="c0">i</span><span class="c6 c12">*</span><span class="c6">log(y</span><span class="c6 c12">jk</span><span class="c0">i</span><span class="c4">) ] , j and k are pixel locations in horizontal and vertical axis.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 819.16px; height: 509.97px;"><img alt="" src="files/rcnn/image8.png" style="width: 819.16px; height: 509.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure : Mask RCNN Algorithm</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 591.70px; height: 303.94px;"><img alt="" src="files/rcnn/image10.png" style="width: 591.70px; height: 303.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Figure : RoI Align Layer</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">RoI Align layer does the same job of RoI pooling but little accurately using a different approach.</span></p><p class="c2"><span class="c4">[1] RoI Scaling : It does not round up the floating point number while scaling down the RoI sizes to match the feature map dimension/</span></p><p class="c2"><span class="c4">[2] Resizing : It uses bilinear interpolation for converting the floating window size to a fixed window input size i.e 5x5 here.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c16 c17 c29">Summary</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">RCNN </span></p><p class="c2"><span class="c4">&nbsp;[A] Selective Search &nbsp;</span></p><p class="c2"><span class="c4">&nbsp;[B] 1 by 1 FE </span></p><p class="c2"><span class="c4">&nbsp;[C] 1 by 1 classification by separate binary SVM for each class</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Fast RCNN </span></p><p class="c2"><span class="c4">&nbsp;[A] Selective Search</span></p><p class="c2"><span class="c4">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c2"><span class="c4">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Faster RCNN</span></p><p class="c2"><span class="c4">&nbsp;[A] RPN + Anchor box</span></p><p class="c2"><span class="c4">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c2"><span class="c4">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Mask RCNN </span></p><p class="c2"><span class="c4">&nbsp;[A] RPN + Anchor Box</span></p><p class="c2"><span class="c4">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Align</span></p><p class="c2"><span class="c4">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c2"><span class="c4">&nbsp;[D] Segmentation Branch</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c13">Symbols and Short forms</span></p><hr><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 792.79px; height: 324.78px;"><img alt="" src="files/rcnn/image4.png" style="width: 792.79px; height: 324.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">Below are some short forms that will be used in the later sections of this chapter : </span></p><p class="c2"><span class="c4">RoI &nbsp; &nbsp;: Region of Interest. Small cropped section of an image.</span></p><p class="c2"><span class="c4">Fmap /feat map : Feature Map. &nbsp;Truncated 3-dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.</span></p><p class="c2"><span class="c4">FE &nbsp; &nbsp; : Feature Extractor. It will represent a deep learning model that extracts features from input data. The term &lsquo;FE&rsquo; will be used synonymously with any deep neural network architecture. e.g VGG16 which exactly does the same.</span></p><p class="c2"><span class="c4">FC &nbsp; &nbsp; : Fully Connected Dense Layer/Layers.</span></p><p class="c2"><span class="c4">bbox &nbsp;: Bounding box. [i.e box coordinates x,y,w,h]</span></p><p class="c2"><span class="c4">GT &nbsp; &nbsp;: Ground Truth</span></p><p class="c2"><span class="c4">CNN : Convolutional Neural Network. ; Conv. : Convolutional.</span></p><p class="c2"><span class="c4">For explainability of the figures I have used consistent symbols while drawing part of the model. &nbsp;The symbols and references are defined at the end.</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c13 c24 c17">Appendix :</span></p><hr><p class="c2 c3"><span class="c1 c18 c17"></span></p><p class="c3 c30"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Selective search &nbsp;</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c4">Quote from Uijlings et al 2013 - &quot;Our grouping procedure now works as follows. We first</span></p><p class="c2"><span class="c4">use (Felzenszwalb and Huttenlocher 2004) to create initial regions. Then we use a greedy algorithm to iteratively group regions together: First the similarities between all neighbour- ing regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours. The process ofgroup- ing the most similar regions is repeated until the whole image becomes a single region&quot;</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.50px; height: 360.45px;"><img alt="" src="files/rcnn/image11.png" style="width: 519.50px; height: 360.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image from the selective search paper by &nbsp;Uijlings et al 2013</span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2"><span class="c6">Relevance</span><span class="c4">&nbsp;between the above explanation and the authors figure in the respective papers. The steps [A],[B], [C] and [D] are shown below for better correlating with the above content, while reading from the paper.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 574.50px; height: 540.29px;"><img alt="" src="files/rcnn/image3.png" style="width: 574.50px; height: 540.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c9 c1">(i) Top : Image from RCNN paper by Girshick et al. 2014. (ii) Bottom : Image from Fast RCNN paper by Girshick et al. 2015</span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 342.50px; height: 349.19px;"><img alt="" src="files/rcnn/image1.png" style="width: 342.50px; height: 349.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 411.50px; height: 223.90px;"><img alt="" src="files/rcnn/image2.png" style="width: 411.50px; height: 223.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2"><span class="c9 c1">&nbsp;Left : Image from Faster RCNN paper Ren et al 2016 , Right : Image from Mask RCNN from He et al </span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2 c3"><span class="c9 c1"></span></p><p class="c2"><span class="c1 c16 c17">References</span></p><p class="c2 c3"><span class="c1 c16 c17"></span></p><p class="c2"><span class="c1 c16 c17">Models :</span></p><p class="c2"><span class="c4">1. ImageNet Classification with Deep Convolutional</span></p><p class="c2"><span class="c6">Neural Networks : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1666002452240647&amp;usg=AOvVaw06qVLceFju-sS17oAmSuTi">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></span><span class="c4">&nbsp; &nbsp;</span></p><p class="c2"><span class="c6">2. Very Deep Convolutional Networks For Large-Scale Image Recognition: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.1556.pdf&amp;sa=D&amp;source=editors&amp;ust=1666002452241032&amp;usg=AOvVaw1QxATBhBZfTh_WC86AKjqS">https://arxiv.org/pdf/1409.1556.pdf</a></span></p><p class="c2"><span class="c6">3. Visualizing and understanding convolutional neural networks :</span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/pdf/1311.2901.pdf&amp;sa=D&amp;source=editors&amp;ust=1666002452241282&amp;usg=AOvVaw2JLo5tUvo8IOYTffmXu_Ra">https://arxiv.org/pdf/1311.2901.pdf</a></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Region Proposal Generation </span></p><p class="c2"><span class="c6">4. Selective search : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s11263-013-0620-5&amp;sa=D&amp;source=editors&amp;ust=1666002452241704&amp;usg=AOvVaw0dwVGX2ea10s6Q3e8svJ0g">https://link.springer.com/article/10.1007/s11263-013-0620-5</a></span></p><p class="c2"><span class="c6">5. Objectness &nbsp; &nbsp; &nbsp; &nbsp; : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/2004.02945&amp;sa=D&amp;source=editors&amp;ust=1666002452241992&amp;usg=AOvVaw1dm5FjRkMlfcxJfnAXLCQn">https://arxiv.org/abs/2004.02945</a></span></p><p class="c2"><span class="c4">Object Detection</span></p><p class="c2"><span class="c6">6. SPP-Net: Deep Absolute Pose Regression with Synthetic Views : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1406.4729&amp;sa=D&amp;source=editors&amp;ust=1666002452242333&amp;usg=AOvVaw376gGZhrfRI_0gDi0LH6uQ">https://arxiv.org/abs/1406.4729</a></span></p><p class="c2"><span class="c6">7. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.6229&amp;sa=D&amp;source=editors&amp;ust=1666002452242595&amp;usg=AOvVaw1NGvWxMYpBYYS7Y_qrzSir">https://arxiv.org/abs/1312.6229</a></span></p><p class="c2"><span class="c6">8. Deformable Part Models are Convolutional Neural Networks : </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.5403&amp;sa=D&amp;source=editors&amp;ust=1666002452242867&amp;usg=AOvVaw26lil9jt8qh0w59EJsF7r8">https://arxiv.org/pdf/1409.5403</a></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c6">9. R-CNN: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1311.2524&amp;sa=D&amp;source=editors&amp;ust=1666002452243151&amp;usg=AOvVaw26O8tN2aZpPsyM8SYWPX_E">https://arxiv.org/abs/1311.2524</a></span></p><p class="c2"><span class="c6">10. Fast R-CNN: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1504.08083&amp;sa=D&amp;source=editors&amp;ust=1666002452243406&amp;usg=AOvVaw145m7BBZM8yNqx1W8DIqBb">https://arxiv.org/abs/1504.08083</a></span></p><p class="c2"><span class="c6">11 Faster R-CNN: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1666002452243690&amp;usg=AOvVaw1iy_ijjkrx6bvwzXpAnNho">https://arxiv.org/abs/1506.01497</a></span></p><p class="c2"><span class="c6">12. Mask R-CNN: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1666002452243949&amp;usg=AOvVaw2NVDfK4djVJt7XoT7FuqVy">https://arxiv.org/abs/1703.06870</a></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Multibox Approaches </span></p><p class="c2"><span class="c6">13. Scalable object detection using deep neural networks : </span><span class="c6 c11"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.2249&amp;sa=D&amp;source=editors&amp;ust=1666002452244365&amp;usg=AOvVaw0hcad1wULneij0_NgW344M">https://arxiv.org/abs/1312.2249</a></span></p><p class="c2"><span class="c6">14. Scalable, high-quality object detection &nbsp;: </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1412.1441&amp;sa=D&amp;source=editors&amp;ust=1666002452244637&amp;usg=AOvVaw14_9XVWmG3HoRGSErZHkwT">https://arxiv.org/abs/1412.1441</a></span></p><p class="c2 c3"><span class="c4"></span></p><p class="c2"><span class="c1 c16 c17">Blogs </span></p><p class="c2"><span class="c4">Special thanks to for easy such a nice explanation of important things -</span></p><p class="c2"><span class="c6">1. </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/&amp;sa=D&amp;source=editors&amp;ust=1666002452245164&amp;usg=AOvVaw2a3G1uFMF1MLiJ84fBGEsp">https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</a></span></p><p class="c2"><span class="c6">2. </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4&amp;sa=D&amp;source=editors&amp;ust=1666002452245486&amp;usg=AOvVaw2v6K36rzFK1ndYkNvh636S">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></span></p><p class="c2"><span class="c6">3. </span><span class="c11 c6"><a class="c7" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272&amp;sa=D&amp;source=editors&amp;ust=1666002452245770&amp;usg=AOvVaw1D9-nqft267waNE_ZXfYSu">https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272</a></span></p><p class="c2 c3"><span class="c9 c1"></span></p></body></html>