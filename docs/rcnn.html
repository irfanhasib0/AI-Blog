<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_qptiuu6mlbhu-0{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-1{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-2{list-style-type:none}.lst-kix_4rhwz7ggv80b-2>li:before{content:"-  "}.lst-kix_4rhwz7ggv80b-3>li:before{content:"-  "}ul.lst-kix_qptiuu6mlbhu-7{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-8{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-3{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-4{list-style-type:none}ul.lst-kix_qptiuu6mlbhu-5{list-style-type:none}.lst-kix_4rhwz7ggv80b-1>li:before{content:"-  "}ul.lst-kix_qptiuu6mlbhu-6{list-style-type:none}.lst-kix_4rhwz7ggv80b-0>li:before{content:"-  "}.lst-kix_b2x7vhnewr2g-1>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-1}.lst-kix_e4jv1uqp760x-6>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-6,decimal) ". "}.lst-kix_e4jv1uqp760x-7>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-7,lower-latin) ". "}ul.lst-kix_4rhwz7ggv80b-2{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-5.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-5 0}ul.lst-kix_4rhwz7ggv80b-1{list-style-type:none}.lst-kix_e4jv1uqp760x-3>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-3,decimal) ". "}ul.lst-kix_4rhwz7ggv80b-0{list-style-type:none}ol.lst-kix_e4jv1uqp760x-4.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-4 0}.lst-kix_e4jv1uqp760x-4>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-4,lower-latin) ". "}.lst-kix_e4jv1uqp760x-5>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-5,lower-roman) ". "}.lst-kix_4rhwz7ggv80b-4>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-0>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-0,decimal) ". "}.lst-kix_4rhwz7ggv80b-5>li:before{content:"-  "}.lst-kix_4rhwz7ggv80b-8>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-2>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-2,lower-roman) ". "}.lst-kix_4rhwz7ggv80b-6>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-1>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-1,lower-latin) ". "}.lst-kix_4rhwz7ggv80b-7>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-8>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-8}.lst-kix_kwwyqny7ndcx-6>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-6.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-6 0}ol.lst-kix_e4jv1uqp760x-3.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-3 0}.lst-kix_kwwyqny7ndcx-3>li:before{content:"-  "}.lst-kix_kwwyqny7ndcx-7>li:before{content:"-  "}.lst-kix_b2x7vhnewr2g-8>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-8}.lst-kix_kwwyqny7ndcx-4>li:before{content:"-  "}.lst-kix_kwwyqny7ndcx-5>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-1{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-2{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-0{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-5{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-6{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-3{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-4{list-style-type:none}ol.lst-kix_eg1j27aju83u-0.start{counter-reset:lst-ctn-kix_eg1j27aju83u-0 0}ol.lst-kix_b2x7vhnewr2g-7{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-8{list-style-type:none}.lst-kix_kwwyqny7ndcx-8>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-8>li:before{content:"" counter(lst-ctn-kix_e4jv1uqp760x-8,lower-roman) ". "}ol.lst-kix_eg1j27aju83u-5.start{counter-reset:lst-ctn-kix_eg1j27aju83u-5 0}ol.lst-kix_b2x7vhnewr2g-0.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-0 0}.lst-kix_e4jv1uqp760x-6>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-6}.lst-kix_eg1j27aju83u-7>li{counter-increment:lst-ctn-kix_eg1j27aju83u-7}ul.lst-kix_npk1gp2e3on-0{list-style-type:none}ul.lst-kix_npk1gp2e3on-1{list-style-type:none}ul.lst-kix_npk1gp2e3on-2{list-style-type:none}.lst-kix_kwwyqny7ndcx-2>li:before{content:"-  "}ul.lst-kix_npk1gp2e3on-7{list-style-type:none}ul.lst-kix_npk1gp2e3on-8{list-style-type:none}.lst-kix_kwwyqny7ndcx-0>li:before{content:"-  "}ul.lst-kix_npk1gp2e3on-3{list-style-type:none}ul.lst-kix_npk1gp2e3on-4{list-style-type:none}.lst-kix_kwwyqny7ndcx-1>li:before{content:"-  "}ul.lst-kix_npk1gp2e3on-5{list-style-type:none}ul.lst-kix_npk1gp2e3on-6{list-style-type:none}ol.lst-kix_eg1j27aju83u-6.start{counter-reset:lst-ctn-kix_eg1j27aju83u-6 0}.lst-kix_ukzohec7w29a-8>li:before{content:"-  "}ol.lst-kix_e4jv1uqp760x-8.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-8 0}.lst-kix_b2x7vhnewr2g-5>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-5}.lst-kix_3hdjefme7cz8-7>li:before{content:"-  "}.lst-kix_ukzohec7w29a-4>li:before{content:"-  "}ol.lst-kix_eg1j27aju83u-0{list-style-type:none}ol.lst-kix_eg1j27aju83u-1{list-style-type:none}ol.lst-kix_eg1j27aju83u-2{list-style-type:none}ol.lst-kix_eg1j27aju83u-3{list-style-type:none}.lst-kix_eg1j27aju83u-0>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-0,decimal) ". "}ol.lst-kix_eg1j27aju83u-4{list-style-type:none}.lst-kix_e4jv1uqp760x-3>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-3}ol.lst-kix_eg1j27aju83u-5{list-style-type:none}ol.lst-kix_eg1j27aju83u-6{list-style-type:none}ol.lst-kix_eg1j27aju83u-7{list-style-type:none}.lst-kix_ukzohec7w29a-6>li:before{content:"-  "}ol.lst-kix_eg1j27aju83u-8{list-style-type:none}ul.lst-kix_sogmjv37c5jb-3{list-style-type:none}ul.lst-kix_sogmjv37c5jb-4{list-style-type:none}ul.lst-kix_sogmjv37c5jb-1{list-style-type:none}ul.lst-kix_sogmjv37c5jb-2{list-style-type:none}.lst-kix_qptiuu6mlbhu-7>li:before{content:"-  "}ul.lst-kix_sogmjv37c5jb-0{list-style-type:none}.lst-kix_eg1j27aju83u-2>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-2,lower-roman) ". "}.lst-kix_eg1j27aju83u-4>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-4,lower-latin) ". "}.lst-kix_eg1j27aju83u-6>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-6,decimal) ". "}.lst-kix_eg1j27aju83u-0>li{counter-increment:lst-ctn-kix_eg1j27aju83u-0}.lst-kix_b2x7vhnewr2g-3>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-3}ul.lst-kix_3hdjefme7cz8-5{list-style-type:none}ul.lst-kix_3hdjefme7cz8-6{list-style-type:none}ul.lst-kix_3hdjefme7cz8-3{list-style-type:none}ul.lst-kix_3hdjefme7cz8-4{list-style-type:none}.lst-kix_eg1j27aju83u-8>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-8,lower-roman) ". "}ul.lst-kix_3hdjefme7cz8-7{list-style-type:none}ul.lst-kix_3hdjefme7cz8-8{list-style-type:none}ul.lst-kix_ukzohec7w29a-2{list-style-type:none}ul.lst-kix_ukzohec7w29a-3{list-style-type:none}ul.lst-kix_ukzohec7w29a-4{list-style-type:none}.lst-kix_eg1j27aju83u-5>li{counter-increment:lst-ctn-kix_eg1j27aju83u-5}ul.lst-kix_ukzohec7w29a-5{list-style-type:none}ul.lst-kix_3hdjefme7cz8-1{list-style-type:none}ul.lst-kix_ukzohec7w29a-6{list-style-type:none}ul.lst-kix_3hdjefme7cz8-2{list-style-type:none}ul.lst-kix_ukzohec7w29a-7{list-style-type:none}ul.lst-kix_ukzohec7w29a-8{list-style-type:none}ul.lst-kix_3hdjefme7cz8-0{list-style-type:none}ol.lst-kix_e4jv1uqp760x-6{list-style-type:none}ol.lst-kix_e4jv1uqp760x-7{list-style-type:none}ol.lst-kix_e4jv1uqp760x-6.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-6 0}ol.lst-kix_e4jv1uqp760x-8{list-style-type:none}ol.lst-kix_e4jv1uqp760x-2{list-style-type:none}.lst-kix_qptiuu6mlbhu-3>li:before{content:"-  "}ol.lst-kix_e4jv1uqp760x-3{list-style-type:none}ol.lst-kix_e4jv1uqp760x-4{list-style-type:none}ol.lst-kix_eg1j27aju83u-8.start{counter-reset:lst-ctn-kix_eg1j27aju83u-8 0}ol.lst-kix_e4jv1uqp760x-5{list-style-type:none}.lst-kix_qptiuu6mlbhu-5>li:before{content:"-  "}ul.lst-kix_sogmjv37c5jb-7{list-style-type:none}ul.lst-kix_sogmjv37c5jb-8{list-style-type:none}ul.lst-kix_sogmjv37c5jb-5{list-style-type:none}ul.lst-kix_sogmjv37c5jb-6{list-style-type:none}.lst-kix_b2x7vhnewr2g-4>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-4}.lst-kix_3hdjefme7cz8-3>li:before{content:"-  "}.lst-kix_npk1gp2e3on-6>li:before{content:"-  "}.lst-kix_3hdjefme7cz8-5>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-4>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-4}ul.lst-kix_kwwyqny7ndcx-1{list-style-type:none}.lst-kix_qptiuu6mlbhu-1>li:before{content:"-  "}ul.lst-kix_kwwyqny7ndcx-2{list-style-type:none}.lst-kix_npk1gp2e3on-4>li:before{content:"-  "}ul.lst-kix_kwwyqny7ndcx-3{list-style-type:none}ul.lst-kix_kwwyqny7ndcx-4{list-style-type:none}ol.lst-kix_e4jv1uqp760x-5.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-5 0}ul.lst-kix_kwwyqny7ndcx-0{list-style-type:none}.lst-kix_npk1gp2e3on-0>li:before{content:"-  "}ul.lst-kix_kwwyqny7ndcx-5{list-style-type:none}ul.lst-kix_kwwyqny7ndcx-6{list-style-type:none}.lst-kix_npk1gp2e3on-2>li:before{content:"-  "}ul.lst-kix_kwwyqny7ndcx-7{list-style-type:none}.lst-kix_3hdjefme7cz8-1>li:before{content:"-  "}ul.lst-kix_kwwyqny7ndcx-8{list-style-type:none}.lst-kix_sogmjv37c5jb-5>li:before{content:"-  "}.lst-kix_sogmjv37c5jb-6>li:before{content:"-  "}.lst-kix_sogmjv37c5jb-3>li:before{content:"-  "}.lst-kix_sogmjv37c5jb-7>li:before{content:"-  "}.lst-kix_eg1j27aju83u-8>li{counter-increment:lst-ctn-kix_eg1j27aju83u-8}.lst-kix_sogmjv37c5jb-1>li:before{content:"-  "}.lst-kix_sogmjv37c5jb-2>li:before{content:"-  "}.lst-kix_b2x7vhnewr2g-0>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-0}.lst-kix_sogmjv37c5jb-0>li:before{content:"-  "}.lst-kix_sogmjv37c5jb-8>li:before{content:"-  "}ol.lst-kix_eg1j27aju83u-2.start{counter-reset:lst-ctn-kix_eg1j27aju83u-2 0}.lst-kix_e4jv1uqp760x-7>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-7}.lst-kix_npk1gp2e3on-8>li:before{content:"-  "}ol.lst-kix_e4jv1uqp760x-1.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-1 0}.lst-kix_sogmjv37c5jb-4>li:before{content:"-  "}.lst-kix_b2x7vhnewr2g-7>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-7,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-5>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-5,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-6>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-6,decimal) ". "}ol.lst-kix_e4jv1uqp760x-7.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-7 0}ol.lst-kix_e4jv1uqp760x-0{list-style-type:none}ol.lst-kix_e4jv1uqp760x-1{list-style-type:none}.lst-kix_b2x7vhnewr2g-4>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-4,lower-latin) ". "}ol.lst-kix_eg1j27aju83u-7.start{counter-reset:lst-ctn-kix_eg1j27aju83u-7 0}.lst-kix_b2x7vhnewr2g-1>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-1,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-2>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-2,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-3>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-3,decimal) ". "}ol.lst-kix_b2x7vhnewr2g-8.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-8 0}.lst-kix_e4jv1uqp760x-5>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-5}.lst-kix_b2x7vhnewr2g-0>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-0,decimal) ". "}ul.lst-kix_ukzohec7w29a-0{list-style-type:none}.lst-kix_eg1j27aju83u-6>li{counter-increment:lst-ctn-kix_eg1j27aju83u-6}ul.lst-kix_ukzohec7w29a-1{list-style-type:none}.lst-kix_b2x7vhnewr2g-2>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-2}.lst-kix_eg1j27aju83u-4>li{counter-increment:lst-ctn-kix_eg1j27aju83u-4}.lst-kix_e4jv1uqp760x-0>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-0}.lst-kix_b2x7vhnewr2g-8>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-8,lower-roman) ". "}ol.lst-kix_b2x7vhnewr2g-7.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-7 0}ol.lst-kix_e4jv1uqp760x-2.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-2 0}.lst-kix_ukzohec7w29a-1>li:before{content:"-  "}.lst-kix_ukzohec7w29a-2>li:before{content:"-  "}.lst-kix_ukzohec7w29a-0>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-2>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-2}.lst-kix_eg1j27aju83u-2>li{counter-increment:lst-ctn-kix_eg1j27aju83u-2}ol.lst-kix_b2x7vhnewr2g-1.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-1 0}.lst-kix_3hdjefme7cz8-8>li:before{content:"-  "}.lst-kix_ukzohec7w29a-3>li:before{content:"-  "}.lst-kix_3hdjefme7cz8-6>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-4.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-4 0}.lst-kix_eg1j27aju83u-1>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-1,lower-latin) ". "}.lst-kix_eg1j27aju83u-1>li{counter-increment:lst-ctn-kix_eg1j27aju83u-1}.lst-kix_ukzohec7w29a-7>li:before{content:"-  "}.lst-kix_ukzohec7w29a-5>li:before{content:"-  "}.lst-kix_qptiuu6mlbhu-8>li:before{content:"-  "}.lst-kix_eg1j27aju83u-3>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-3,decimal) ". "}.lst-kix_b2x7vhnewr2g-6>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-6}ol.lst-kix_eg1j27aju83u-1.start{counter-reset:lst-ctn-kix_eg1j27aju83u-1 0}.lst-kix_eg1j27aju83u-3>li{counter-increment:lst-ctn-kix_eg1j27aju83u-3}ol.lst-kix_e4jv1uqp760x-0.start{counter-reset:lst-ctn-kix_e4jv1uqp760x-0 0}.lst-kix_eg1j27aju83u-5>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-5,lower-roman) ". "}.lst-kix_eg1j27aju83u-7>li:before{content:"" counter(lst-ctn-kix_eg1j27aju83u-7,lower-latin) ". "}ol.lst-kix_eg1j27aju83u-4.start{counter-reset:lst-ctn-kix_eg1j27aju83u-4 0}.lst-kix_qptiuu6mlbhu-2>li:before{content:"-  "}ul.lst-kix_4rhwz7ggv80b-8{list-style-type:none}ul.lst-kix_4rhwz7ggv80b-7{list-style-type:none}ul.lst-kix_4rhwz7ggv80b-6{list-style-type:none}.lst-kix_qptiuu6mlbhu-4>li:before{content:"-  "}ul.lst-kix_4rhwz7ggv80b-5{list-style-type:none}ul.lst-kix_4rhwz7ggv80b-4{list-style-type:none}ul.lst-kix_4rhwz7ggv80b-3{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-2.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-2 0}.lst-kix_qptiuu6mlbhu-6>li:before{content:"-  "}.lst-kix_3hdjefme7cz8-0>li:before{content:"-  "}.lst-kix_3hdjefme7cz8-2>li:before{content:"-  "}.lst-kix_b2x7vhnewr2g-7>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-7}.lst-kix_3hdjefme7cz8-4>li:before{content:"-  "}.lst-kix_npk1gp2e3on-7>li:before{content:"-  "}ol.lst-kix_eg1j27aju83u-3.start{counter-reset:lst-ctn-kix_eg1j27aju83u-3 0}.lst-kix_npk1gp2e3on-5>li:before{content:"-  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_npk1gp2e3on-3>li:before{content:"-  "}.lst-kix_e4jv1uqp760x-1>li{counter-increment:lst-ctn-kix_e4jv1uqp760x-1}.lst-kix_qptiuu6mlbhu-0>li:before{content:"-  "}.lst-kix_npk1gp2e3on-1>li:before{content:"-  "}ol.lst-kix_b2x7vhnewr2g-3.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-3 0}ol{margin:0;padding:0}table td,table th{padding:0}.c8{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:italic}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman";font-style:italic}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c23{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Times New Roman"}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{vertical-align:sub;font-size:12pt;font-family:"Times New Roman";font-weight:400}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c6{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c25{text-decoration:none;vertical-align:baseline;font-style:normal}.c22{color:#000000;text-decoration:none;vertical-align:baseline}.c18{color:#000000;text-decoration:none;font-style:normal}.c4{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c17{color:#333333;text-decoration:none;vertical-align:baseline}.c31{font-weight:700;font-size:15pt;font-family:"Times New Roman"}.c26{color:#666666;text-decoration:none;vertical-align:baseline}.c16{background-color:#ffffff;max-width:648pt;padding:72pt 72pt 72pt 72pt}.c10{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c30{font-weight:700;font-size:16pt;font-family:"Times New Roman"}.c27{font-size:14pt;font-family:"Times New Roman";font-weight:700}.c19{margin-left:72pt;padding-left:0pt}.c9{color:inherit;text-decoration:inherit}.c29{font-weight:400;font-family:"Times New Roman"}.c20{background-color:#ffffff;color:#202124}.c24{padding:0;margin:0}.c11{vertical-align:super}.c12{margin-left:36pt}.c28{padding-left:0pt}.c7{height:11pt}.c15{color:#67ab9f}.c13{font-style:italic}.c21{color:#202124}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16 doc-content"><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c10">&nbsp; &nbsp; &nbsp;</span><span class="c14 c30">&nbsp; Object Detection : The RCNN Family</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span class="c14 c10">&nbsp;</span></p><p class="c3"><span class="c10 c13">Object Detection</span><span class="c1">&nbsp;is the combination of two sub tasks (i) Object Localization (ii) Object Classification. These two steps comprise the simplest intuitive way, firstly we localize the object in the image and then classify it to an object category. When it comes to object detection with deep learning, we need to consider an additional step &lsquo;Feature Extraction&rsquo;. It breaks down the raw data into a high dimensional feature space, which is more convenient for the computer to work with.</span></p><p class="c3"><span class="c10">[A]</span><span class="c4">&nbsp;</span><span class="c10">Object localization :</span><span class="c1">&nbsp;This step takes the rgb image and extracts the N set of (x,y,w,h) bounding box coordinates for potential objects present in it. It can be done in two ways (i) using some specific image processing based algorithm e.g Selective search, (ii) using convolutional neural network e.g Region Proposal Network used in Faster RCNN algorithm.</span></p><p class="c3"><span class="c10">[B]</span><span class="c4">&nbsp;</span><span class="c10">Feature map Extraction :</span><span class="c1">&nbsp;It takes an image array and converts it to a high dimensional feature map array using a convolutional layer based neural network e.g vgg16, resnet 50 etc.</span></p><p class="c3"><span class="c10">[C]</span><span class="c4">&nbsp;</span><span class="c10">Classification :</span><span class="c1">&nbsp;This step takes each of the extracted feature maps and classifies it to an object category using dense layers or some machine learning &nbsp;based classifier algorithm e.g SVM (support vector machine). </span></p><p class="c3"><span class="c1">Note : [A], [B] and [C] are used for denoting these 3 steps in the upcoming sections.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c5">RCNN</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">The RCNN method uses the following modules to perform object detection :</span></p><p class="c3"><span class="c10">Selective Search &nbsp;:</span><span class="c1">&nbsp;It is an algorithm for generating region proposal suggestions. It suggests potential bboxes (bounding boxes) that may contain an object. &#39;Objectness&#39; is another similar algorithm that can be used for region proposal generation. SOTA works now use a simple CNN based architecture called RPN (region proposal network) for this purpose. See the appendix at the bottom for more about selective search. </span></p><p class="c3"><span class="c10">Feature Extractor (FE) : &nbsp;</span><span class="c1">It can be any deep convolutional neural network. The output of the network is truncated from the last dense layer (if a 1d feature vector is necessary) or any of the intermediate convolutional layers (if a 2 dimensional feature map is required). The 2d featuremap also preserves the spatial relationship with the input image due to the sliding nature of convolution operation. &nbsp;For RCNN they don&#39;t require the spatial relation, so use the output of a dense layer as a 1d feature vector. For fast RCNN it will need a 2d feature map instead.</span></p><p class="c3"><span class="c10">Binary SVM :</span><span class="c1">&nbsp;Support vector machine algorithm is used for binary (yes/no) classification every single class. Binary SVM is specially suited for single class classification since it tries to select the hyperplane that separates the two classes with highest margin.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 873.64px; height: 735.33px;"><img alt="" src="files/rcnn/image13.png" style="width: 873.64px; height: 735.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c8">Figure : RCNN (top) and Fast RCNN (bottom)</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c22 c10 c13">RCNN Algorithm :</span></p><p class="c3"><span class="c1">1. [A] Selective Search algorithm is used for generating ~ 2000 sets of bounding box coordinate (x,y,w,h) proposals per image. Using the bbox coordinates found from selective search, image RoIs (Region of Interest) of different sizes are cropped. For detail on it see the appendix at the bottom.</span></p><p class="c3"><span class="c1">2. [A] The SVM classifier can only take fixed sized inputs. So the outputs of the feature extractor should have constant size. It is possible if the input image RoIs to the feature extractor have fixed size. For this reason each RoI is &#39;wrapped&#39; to a fixed size [resizing with bilinear/nearest sampling] before inputting them to the feature extractor model. </span></p><p class="c3"><span class="c1">3. [B] A deep convolutional neural network is used for extracting feature maps from each of the image RoI one by one. The model output is truncated from the bottom and output from a dense layer with 4096 nodes is extracted as a feature map.</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Pre training with ICSLV-2012 images with 200 classes lr 0.01 on CNN architecture by krizhevsky at el. 2012.</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Oversampling positive classes 32 positive sample + 96 neg sample per mini-batch</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Fine tuning with lr 0.001 with wrapped region proposal with N + 1 class. N is no of object class 1 is background</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During fine tuning a region proposal with more than 0.50 IoU with a positive sample was taken as positive.</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During Feature Extraction the last layer is removed and features from an internal dense layer are collected. i.e 1024 or 2048 features</span></p><p class="c3"><span class="c1">4. [C] &nbsp;Separate binary classifiers for each of the N (N=no of categories) are trained to categorize the fmaps to respective object categories.</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; One binary SVM classifier is used for each of the classes.</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Hard negative mining [4] is found to be useful for fast convergence. For detail see right section.</span></p><p class="c3"><span class="c1">5. [A*] Fine tuning of the localization boxes are done by bbox regression using Ridge regression with lambda = 1000. The regressor only learns the x,y offset from the corner of the bbox predicted by selective search. For w,h it is log offset instead. The next section will explain it in detail.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c2"><span class="c10">BBOX Regression : </span></p><p class="c2"><span class="c1">This approach was Inspired from the Deformable part model by Girshick at el. 2010. Let&rsquo;s assume the following -</span></p><ul class="c24 lst-kix_npk1gp2e3on-0 start"><li class="c2 c12 c28 li-bullet-0"><span class="c1">X,y are the coordinates of a bbox in an image and w, h are its width and height respectively</span></li></ul><ul class="c24 lst-kix_qptiuu6mlbhu-0 start"><li class="c2 c12 c28 li-bullet-0"><span class="c4">The initial bbox predictions are P = &nbsp;[P</span><span class="c0">x</span><span class="c4">,P</span><span class="c0">y</span><span class="c4">,P</span><span class="c0">w</span><span class="c4">,P</span><span class="c0">h</span><span class="c1">]</span></li><li class="c2 c12 c28 li-bullet-0"><span class="c4">The initial bbox ground truths are G = [G</span><span class="c0">x</span><span class="c4">,G</span><span class="c0">y</span><span class="c4">,G</span><span class="c0">w</span><span class="c4">,G</span><span class="c0">h</span><span class="c1">]</span></li><li class="c2 c12 c28 li-bullet-0"><span class="c4">d</span><span class="c0">x</span><span class="c4">(P) , d</span><span class="c0">y</span><span class="c4">(P), d</span><span class="c0">w</span><span class="c4">(P), d</span><span class="c0">h</span><span class="c1">(P) is the output of regression i.e the errors/offsets to be corrected on P for matching with G more accurately.</span></li></ul><p class="c2"><span class="c4">For x and y, a &ldquo;scale invariant&rdquo; transformation d</span><span class="c0">x</span><span class="c4">(P) d</span><span class="c0">y</span><span class="c1">(P) is learnt. They are positional offset corrections from an initial less accurate prediction by selective search. The offset (box placement error) value can vary within a wide range based on the size of the bbox. Big boxes can require a large offset while for small boxes it would be the opposite. The regressor output dx,dy are scale invariant offset, so the regressor can learn offset values for a fixed unit box (or unit scale i.e ranging from 0-1) for all the bbox and later need to be multiplied with the respective predicted size (Pw , Ph) of the bbox before adding to the prediction box i.e &nbsp;Gx = Pw * dx(P) + Px. For w and h a log scale transformation is learnt. It makes it capable of learning a wide range of change in w and h direction. Unlike x,y offsets, here it is just the difference of their logarithmic value instead of their actual value (eq [5] below).</span></p><p class="c2"><span class="c4">&nbsp;Most of the SOTA methods use an updated version of the bbox regression method introduced in RCNN. For example yolo-v3 uses &nbsp;G</span><span class="c0">x</span><span class="c4">&nbsp;= Sigmoid(P) +P</span><span class="c0">x </span><span class="c4">. They wrap the d</span><span class="c0">x &nbsp;</span><span class="c1">with a sigmoid function to restrict its output between 0-1. Some methods use a fixed grid system with a potential bbox at every location of the featuremap as an initial bbox then corrects it with regression. We will explain about these in detail in respective sections.</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c10">Hard Negative Mining :</span><span class="c1">&nbsp; It is an approach that oversamples the difficult or hard to learn images more and makes the learning faster. It was possible to achieve good accuracy after just one pass over the data using hard negative mining for the SVM models.</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 742.50px; height: 392.98px;"><img alt="" src="files/rcnn/image10.png" style="width: 742.50px; height: 392.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">As per our today&#39;s practice on SOTA object detection algorithms several things might seem to be redundant -</span></p><p class="c3"><span class="c1">[i] Extracting feature maps for every RoI one by one. [It is solved in Fast RCNN eventually]</span></p><p class="c3"><span class="c1">[ii] Using a separate SVM for each class instead of using the softmax class outputs of the feature extractor network. [Improved in Fast RCNN]</span></p><p class="c3"><span class="c1">[iii] The word &quot;wrapped&quot; image is used in the paper for resizing each Fmap RoI to a fixed size. Nowadays a good practice is using bilinear sampling. Note that - rcnn first resizes (wrapped) each of the image RoI then inputs them to Feature extractor but Fast RCNN will resize the feature maps using RoI pooling layer.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">&nbsp;Using additional SVM classifier instead of the dense layer output from the feature extractor network :</span></p><p class="c3"><span class="c1">Quote from the paper&#39;s appendix B : &quot;We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classi- fier was trained on randomly sampled negative examples rather than on the subset of &ldquo;hard negatives&rdquo; used for SVM training.&quot;</span></p><p class="c3"><span class="c1">Anyway, in Fast RCNN the authors were finally able to do it using a small dense network after the feature extractor, without any SVM classifier.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c17 c27 c13">Fast RCNN</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3 c7"><span class="c10 c13 c17"></span></p><p class="c3"><span class="c14 c10">Improvements in Fast RCNN:</span></p><p class="c3"><span class="c1">- Fast RCNN eliminated one by one RoI input into the feature extractor. There are lots of common regions between RoI&#39;s specially when nearly 2000 RoI generated from one image. This method applies a feature extractor for the whole image just once and then it crops the corresponding RoI for each of the ~ 2000 corresponding regions from the feature map. </span></p><p class="c3"><span class="c1">- Replacing image wrapping with RoI Pooling.</span></p><p class="c3"><span class="c1">- Getting rid of per class svm by using the softmax outputs from dense layers after feature extractor.</span></p><p class="c3 c7"><span class="c10 c15 c25"></span></p><p class="c3"><span class="c10 c15">&nbsp;</span><span class="c14 c10">Fast RCNN Algorithm :</span></p><p class="c3"><span class="c1">1.[A] Selective Search generates ~ 2k RoI proposals per image. </span></p><p class="c3"><span class="c1">2.[B] A CNN based network i.e VGG16 is used for feature extraction.</span></p><p class="c3"><span class="c1">3.[B] The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps RoIs so that it can fit into the dense layer afterwards. (H=7, W=7 for VGG 16) </span></p><p class="c3"><span class="c4">4. [C] The last fully connected layer is replaced with 2 sibling dense layers (i) one &nbsp;for classification (ii) another for bbox regression. For regression it adopted the same fine tuning mechanism (optimizing t</span><span class="c0">x</span><span class="c4">,t</span><span class="c0">x</span><span class="c4">,t</span><span class="c0">x</span><span class="c4">,t</span><span class="c0">x</span><span class="c1">) like RCNN. </span></p><p class="c3 c7"><span class="c1"></span></p><p class="c2"><span class="c22 c10 c13">RoI Pooling Layer</span></p><p class="c2"><span class="c1">RoI Pooling Layer : The &nbsp;operator [*] represents the flooring operation here. e.g [1.13] = 1.00</span></p><p class="c2"><span class="c1">The functionality of RoI Pooling layer can be divided into two parts -</span></p><p class="c2"><span class="c10">[1] Scaling the image RoIs to fmap dimension : </span><span class="c1">Due to multiple polling operations in the consecutive convolutional layers, the feature map array has smaller (e.g 16~32 times smaller) width and height compared to the original image. In the example below the feat map is 16 times smaller than the original image in terms of width and height. The RoI Pooling layer scales down each of the RoI coordinates (generated at the image scale) so that it fits on top of the feature map in the corresponding feature location for that image RoI. Each RoI coordinate needs to be divided by the scale factor (here it is 16) for fitting on the feature map. In this way the 188x188 RoI window coordinates are converted to 11x11 below. The center coordinates (x,y) of the RoI is also divided by the factor like (w,h) which is not shown in the figure for brevity.</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c10">[2] Resizing the Fmap RoI to fit the fixed input of dense layers :</span><span class="c1">&nbsp;The dense layer has fixed input size i.e 25 dense nodes. If a 5x5x1 feature map tensor is flattened (5x5x1=25) only then it will fit the dense input. For 5x5x10 the dense should have 250 input nodes. For each of the RoI Feature maps do a maxpool operation with kernel size ([h/h&#39;&#39;], [w/w&#39;&#39;]) for below example it will be ([11/5],[11/5]) = (2,2). Now an arbitrary feature map of size 11x11 is operated with a maxpool of kernel size 2 for fitting fixed 5x5 input size of the dense layers afterward. Another example can be like - an orbitary Fmap RoI of size 20x20 would require a kernel of 4x4 for fitting it into 5x5 input size</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 725.12px; height: 631.29px;"><img alt="" src="files/rcnn/image2.png" style="width: 725.12px; height: 631.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">&nbsp;Figure : RoI Pooling Layer top 2d view with numerical examples bottom 3d view of the same thing</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c10 c13">&nbsp;</span><span class="c5">Faster RCNN</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span class="c10 c14">Improvements :</span></p><p class="c3"><span class="c1">- The selective search is replaced with a Region Proposal Network that mostly shares weights with the Feature Extractor Network.</span></p><p class="c3"><span class="c1">- The region proposals are further broken down to 3 different scales with 3 different aspect ratios called anchor boxes.</span></p><p class="c3 c7"><span class="c10 c13 c22"></span></p><p class="c3"><span class="c14 c10">The Algorithm:</span></p><p class="c3"><span class="c1">Fater-RCNN can be described as a RPN (Region Proposal Network) and a Fast RCNN. In very short they replaced the Selective search bbox proposal system with RPN network. </span></p><p class="c3"><span class="c10">[A] &nbsp;RPN (Region Proposal Network): </span></p><p class="c3"><span class="c10">Anchor Box : </span><span class="c4">Anchor boxes are prior box estimates at every feature map location. They are determined before the training and fed into the network beforehand. &nbsp;They found 9 anchors from 3 different scales 128x128, 256x256 and 512x512 with 3 aspect ratios 1:1 , 1:2 and 2:1. The prior boxes at every feature map location are further fine tuned by training the RPN model. At every feature map location the prior box coordinate is grid center coordinate for x</span><span class="c0">a</span><span class="c4">&nbsp;and y</span><span class="c0">a</span><span class="c4">; it is common for all the 9 anchors at that location. Nine different w</span><span class="c0">a</span><span class="c4">,h</span><span class="c0">a</span><span class="c4">&nbsp;are found by applying 3 aspect ratio at 3 scales. &nbsp;The RPN model learns how to fit each of the anchor boxes to the object present at that location by predicting offsets t</span><span class="c0">x</span><span class="c4">,t</span><span class="c0">y</span><span class="c4">,t</span><span class="c0">w</span><span class="c4">,t</span><span class="c0">h</span><span class="c1">. At the same time it learns two confidence scores for object and background for each anchor box. &nbsp;More on the anchor boxes will be explained in the anchor box section below.</span></p><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span class="c4">&nbsp;</span><span class="c10">RPN Model : </span><span class="c4">The output of the last shared convolutional layer is passed through a 3x3 convolution layer. For vgg 16 based implementation it should result in an array of shape 40x60x512. Now this array is passed through two sibling 1x1 convolution layers. One layer is for bbox regression and another for class confidence prediction. The bbox regressor has a shape of 40x60x n*k(=36) where n(= 4) &nbsp;represents four [x,y,w,h] &nbsp;coordinates for each anchor and k(=9) represents nine anchor boxes. The confidence predictor output has shape 40x60x n*k(=18) = 40x60x18 where n(=2) for [P</span><span class="c0">obj</span><span class="c4">, P</span><span class="c0">bg</span><span class="c1">], &nbsp;probability of object and background respectively and k=9 as usual. </span></p><p class="c3"><span class="c1">If the output of the RPN has width &lsquo;m&rsquo; and height &lsquo;n&rsquo; the depth would be 2k and 4k for confidence and bbox outputs. A total of m*n*k set of (x,y,w,h) can be extracted from the depthwise 4k data points at each of the m*n locations. Similarly m*n*k set of (object conf,background conf) can be found from 2k data points depthwise. </span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c10">Training RPN :</span><span class="c1">&nbsp;During training they use &ldquo;image-centric&rdquo; sampling. They sample a total 256 bbox from one image while trying to keep the object and background ratio 1:1 if not possible they pad with background. The RPN is trained with binary classification labels. Among the anchor boxes at each location, boxes having maximum IoU with any of the ground truth boxes is taken as a positive sample. Boxes with less than 0.3 IoU with all the GT (Ground Truth) boxes are taken as negative samples (background), others are ignored. </span></p><p class="c3"><span class="c4">They train the RPN model with the following loss function below. Eq1 and Eq2 define t which is the difference between prediction and anchor box. Eq 3 and 4 defines t* which is the difference between anchor box and ground truth. The loss is calculated between t and t* which ultimately minimizes the difference between ground truth and anchor box. The RPN model outputs &lsquo;t&rsquo; from Eq 1 and 2 the value of &nbsp;x, y,w and h can be back calculated from t</span><span class="c0">x</span><span class="c4">,t</span><span class="c0">y</span><span class="c4">,t</span><span class="c0">w</span><span class="c4">,t</span><span class="c0">h</span><span class="c1">&nbsp;predicted from the RPN for prediction.</span></p><p class="c3"><span class="c4">(Notation is consistent with the authors of Faster-RCNN)</span></p><ul class="c24 lst-kix_4rhwz7ggv80b-0 start"><li class="c2 c12 c28 li-bullet-0"><span class="c4">x , x* , x</span><span class="c0">a </span><span class="c1">P prediction, ground truth and anchor respectively.</span></li><li class="c2 c12 c28 li-bullet-0"><span class="c4">p</span><span class="c4 c11">obj</span><span class="c4">,p</span><span class="c4 c11">bg</span><span class="c1">&nbsp;in for object and background confidence.</span></li><li class="c2 c12 c28 li-bullet-0"><span class="c4">&lsquo;t</span><span class="c0">i</span><span class="c1">&rsquo; represents the i(th) bbox (x,y,w,h).</span></li></ul><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c4">t</span><span class="c0">x</span><span class="c4">&nbsp; &nbsp; = (x-x</span><span class="c0">a</span><span class="c4">)/w</span><span class="c0">a</span><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;; t</span><span class="c0">y</span><span class="c4">&nbsp;= (y - y</span><span class="c0">a</span><span class="c4">)/h</span><span class="c0">a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c1">&mdash; (1)</span></p><p class="c2"><span class="c4">t</span><span class="c0">w</span><span class="c4">&nbsp; &nbsp;= log(w/w</span><span class="c0">a</span><span class="c4">) &nbsp; &nbsp; &nbsp; &nbsp;; t</span><span class="c0">h</span><span class="c4">&nbsp;= log(h/h</span><span class="c0">a</span><span class="c1">) &nbsp; &nbsp; &nbsp; &nbsp;&mdash; (2)</span></p><p class="c2"><span class="c4">t</span><span class="c0">x* &nbsp; &nbsp;</span><span class="c4">= (x*-x</span><span class="c0">a</span><span class="c4">)/w</span><span class="c0">a</span><span class="c4">&nbsp;, &nbsp; &nbsp; ; t</span><span class="c0">y</span><span class="c4">&nbsp;= (y* - y</span><span class="c0">a</span><span class="c4">)/h</span><span class="c0">a &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c4">&mdash; (3)</span></p><p class="c2"><span class="c4">t</span><span class="c0">w*</span><span class="c4">&nbsp; = log(w*/w</span><span class="c0">a</span><span class="c4">) &nbsp; &nbsp; ; t</span><span class="c0">h</span><span class="c4">&nbsp;= log(h*/h</span><span class="c0">a</span><span class="c4">) &nbsp; &nbsp; &nbsp;</span><span class="c0">&nbsp;</span><span class="c1">&mdash; (4)</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c4">L</span><span class="c0">&nbsp;</span><span class="c4">&nbsp;= &lambda; x &nbsp;1/</span><span class="c4">N</span><span class="c0">reg</span><span class="c0">&nbsp; </span><span class="c4">x &nbsp;</span><span class="c4 c20">&Sigma;</span><span class="c4 c11 c21">N</span><span class="c0 c21">i</span><span class="c4">&nbsp; &nbsp; [ </span><span class="c4">p</span><span class="c0">i</span><span class="c4 c11">obj</span><span class="c4">&nbsp;x</span><span class="c4">&nbsp;</span><span class="c4">L</span><span class="c0">reg(i)</span><span class="c4">&nbsp;(t</span><span class="c0">i</span><span class="c4">, t</span><span class="c0">i*</span><span class="c4">) + &nbsp;Robust L</span><span class="c0">1</span><span class="c4">&nbsp;(t</span><span class="c0">i</span><span class="c4">, t</span><span class="c0">i*</span><span class="c1">)] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(Robust L1 Smoothing introduced in Fast RCNN)</span></p><p class="c2"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; + &nbsp; &nbsp;1/</span><span class="c4">N</span><span class="c0">cls</span><span class="c4">&nbsp;x </span><span class="c4 c20">&Sigma;</span><span class="c4 c11 c21">N</span><span class="c0 c21">i </span><span class="c4">&nbsp; &nbsp; [ </span><span class="c4 c11">&nbsp;</span><span class="c4">L</span><span class="c0">cls</span><span class="c4">&nbsp;(p</span><span class="c4 c11">obj</span><span class="c0">i</span><span class="c4 c11">*</span><span class="c4">,p</span><span class="c4 c11">bg</span><span class="c0">i</span><span class="c4 c11">*</span><span class="c4">)]</span><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(e.g binary cross entropy loss can be used as L</span><span class="c0">cls</span><span class="c1">)</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c4">here </span><span class="c4">N</span><span class="c0">cls</span><span class="c4">&nbsp;= 256 ; </span><span class="c4">N</span><span class="c0">reg</span><span class="c1">&nbsp;= 2400 ; &nbsp;Lambda = 10 </span></p><p class="c2"><span class="c4">Total Loss = &nbsp;L</span><span class="c0">cls</span><span class="c4">&nbsp;+ </span><span class="c4">L</span><span class="c0 c18">reg</span></p><p class="c2 c7"><span class="c0 c18"></span></p><p class="c3"><span class="c1">They do bbox and confidence prediction at every fmap location respectively. The 1x1 convolution layer can also be interpreted as a dense layer applied at every pixel location and results in an array with the same height and width while having depth of 4k and 2k respectively, k= number of anchor boxes per location. Region Proposal Network proposes possible box coordinates and objectness confidence at each pixel location of the feature map. This is a class agnostic box predictor so it has nothing to do with the category of the object. The output of the RPN module is converted to bbox coordinates at every feature map location .</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 864.30px; height: 762.84px;"><img alt="" src="files/rcnn/image7.png" style="width: 864.30px; height: 762.84px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c8">Figure : Faster RCNN</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c22 c10 c13">Anchor Box / Multi Box / Default Box</span></p><p class="c3"><span class="c10">Translation Invariance : </span><span class="c4">Anchor boxes were first used by erhan at el. , szegedy at el in their respective works. It is referred to as the multibox approach or default box in other works. &nbsp;</span><span class="c29">Initial anchor box methods used different anchors (prior boxes) at different locations of the image resulting in a lot of unique anchors. For example, the multibox method by szegedy at el. was not translation invariant. It used 800 anchor boxes at different locations of the image. They found this 800 anchors by k means clustering o</span><span class="c1">f the [x,y,w,h] of all the Ground truth bboxes. In contrast, faster rcnn does k means clustering for h,w only irrespective of their x,y location in image. These 9 boxes can just fit any arbitrary x,y location in the image instead of 800 different boxes for various x,y locations. They conducted some experiments with different values of k and found k=9 anchor boxes are optimal for achieving the best mAP. &nbsp;This 9 boxes are now capable of producing w*h*9 ~ 2400 x 9 RoI per image at maximum.</span></p><p class="c3"><span class="c1">&nbsp;-&gt; For Multibox the shape of tensor for RoI prediction was 4+1* 800 dimensional &nbsp;fully connected layer</span></p><p class="c3"><span class="c1">&nbsp;-&gt; &nbsp;For Faster RCNN it reduced to 4+2*9 convolutional output layer.</span></p><p class="c3"><span class="c10">Multiple objects at one location :</span><span class="c1">&nbsp;At each pixel location we can get k=9 boxes and respective confidence scores. It will make it possible to detect multiple objects centered at the same image location (having different aspect ratio or scale). </span></p><p class="c3"><span class="c10">Multi scale at low parameters :</span><span class="c1">&nbsp;It makes the model learn multiple scales of objects without using additional branches from the backbone network like SSD or FPN. So it can have multiple scales with minimum possible cost of parameters.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 637.50px; height: 310.45px;"><img alt="" src="files/rcnn/image6.png" style="width: 637.50px; height: 310.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c8">&nbsp; &nbsp;Image from Faster RCNN paper Ren et al 2016</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 488.50px; height: 406.64px;"><img alt="" src="files/rcnn/image8.png" style="width: 488.50px; height: 406.64px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure : &nbsp;Total 9 anchor boxes at one pixel locations</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">The feature extractor and classifier is the same as Fast RCNN.</span></p><p class="c3"><span class="c14 c10">[B] Feature Extractor :</span></p><p class="c3"><span class="c1">&nbsp;- It primarily uses the VGG16 network feature extractor which shares its weights with the RPN (until the RPN specific layers). It used the VGG16 and also ZF [zeiler at el.] pre-trained on ImageNet dataset for demonstrating their algorithm performance.</span></p><p class="c3"><span class="c1">&nbsp;- The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps for every Fmap RoI so that it can fit into the first dense layer (H=7, W=7 for VGG 16) </span></p><p class="c3"><span class="c1">&nbsp;- RoI pooling layer projects the RoI coordinates on top of the feature map array in a max pooling manner, the difference here is it adjusts the pooling window size for keeping the output size fixed for all the fmap. RoIs.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c10">[C] Classifier :</span><span class="c1">&nbsp;Each of the output (fmap RoIs) from the RoI Pooling layer is sent to a small dense network ending with 2 sibling dense layers (i) one for classification (ii) another for bbox regression. The regressor here </span></p><p class="c3"><span class="c1">learns the x,y offset and log offset for w and h from the prior boxes predicted by RPN initially.</span></p><p class="c2 c7"><span class="c1"></span></p><p class="c2"><span class="c14 c10">Back propagation :</span></p><p class="c2"><span class="c10">(1) Alternate training :</span><span class="c1">&nbsp;(Used by the authors)</span></p><p class="c2"><span class="c1">RPN and Fast-RCNN share the same network (the &quot;Shared CNN Layers&quot; part in above figure). Rest of the layers are task specific i.e RPN Layers and Dense Layers at the end. Both of the networks are initialized with imagenet pre-trained weights for either VGG-16 (first 13 CNN layers) or ZF network (first 5 CNN layers) &nbsp;[zeiler at el.]. *Fast-RCNN is a special case of Fast-RCNN, &nbsp;that will be trained with region proposals from RPN predictions instead of selective search. </span></p><p class="c2"><span class="c1">- First the RPN is trained. </span></p><p class="c2"><span class="c1">- Then using the region proposal results of RPN is used as input for training the *Fast-RCNN.</span></p><p class="c2"><span class="c1">- Next RPN is initialized with the Fast-RCNN weights (only the shared part) and trained again for the RPN specific layers at the bottom are trained only keeping the shared layers fixed.</span></p><p class="c2"><span class="c1">- Finally, both the *Fast-RCNN is trained again keeping the common layers fixed.</span></p><p class="c2"><span class="c10">(2) Approximate Joint training :</span><span class="c1">&nbsp;Using the same network for RPN and Fast-RCNN* from the beginning. Training them together. For Fast-RCNN the outputs from RPN during forward pass are taken as input assuming them fixed predefined values. Since the RoI pooling layer is not differentiable that&#39;s why they needed to consider the outputs of RPN fixed values.</span></p><p class="c2"><span class="c10">(3) Non Approximate Joint training :</span><span class="c1">&nbsp;In this method they proposed using a differentiable RoI Pooling layer where so that the outputs taken from RoI pooling and fed back to the Fast-RCNN would be a part of the back propagation chain</span></p><p class="c2"><span class="c1">Authors showed their results with method (1) in the paper.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c31">Mask RCNN</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span class="c14 c10">Improvements in Mask RCNN :</span></p><p class="c3"><span class="c1">- RoI Pooling layer is replaced with RoI align layers.</span></p><p class="c3"><span class="c1">- Additional semantic segmentation branch is added for predicting both bbox and segmentation at a time. I will refer it as [D]</span></p><p class="c3 c7"><span class="c25 c10 c15"></span></p><p class="c3"><span class="c14 c10">The Algorithm Mask RCNN:</span></p><p class="c3"><span class="c1">The algorithm is very similar to faster rcnn. The major difference here is that they used a more accurate version of the RoI Pooling layer. They also added a segmentation branch to the original network and was able to produce accurate semantic segmentation along with bounding box prediction.</span></p><p class="c3"><span class="c1">&nbsp;</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c10">Mask RCNN Loss</span></p><p class="c3"><span class="c4">Loss = L</span><span class="c0">cls</span><span class="c4">&nbsp;+ L</span><span class="c0">bbox </span><span class="c4">+ L</span><span class="c0 c18">seg</span></p><p class="c3"><span class="c4">L</span><span class="c0">cls</span><span class="c4">&nbsp;and L</span><span class="c0">reg</span><span class="c1">&nbsp;are similar to Faster RCNN.</span></p><p class="c3"><span class="c4">L</span><span class="c0">seg = </span><span class="c4">1/N x </span><span class="c4 c20">&Sigma;</span><span class="c4 c20 c11">N</span><span class="c0 c20">i</span><span class="c4">&nbsp;[</span><span class="c4 c20">&Sigma;</span><span class="c0 c21">jk </span><span class="c4">-y</span><span class="c4 c11">jk</span><span class="c0">i</span><span class="c4 c11">*</span><span class="c4">log(y</span><span class="c4 c11">jk</span><span class="c0">i</span><span class="c1">) ] , j and k are pixel locations in horizontal and vertical axes.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 819.16px; height: 509.97px;"><img alt="" src="files/rcnn/image9.png" style="width: 819.16px; height: 509.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure : Mask RCNN Algorithm</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 591.70px; height: 303.94px;"><img alt="" src="files/rcnn/image11.png" style="width: 591.70px; height: 303.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Figure : RoI Align Layer</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">RoI Align layer does the same job of RoI pooling but little accurately using a different approach.</span></p><p class="c3"><span class="c1">[1] RoI Scaling : It does not round up the floating point number while scaling down the RoI sizes to match the feature map dimension/</span></p><p class="c3"><span class="c1">[2] Resizing : It uses bilinear interpolation for converting the floating window size to a fixed window input size i.e 5x5 here.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c27">Summary</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">RCNN </span></p><p class="c3"><span class="c1">&nbsp;[A] Selective Search &nbsp;</span></p><p class="c3"><span class="c1">&nbsp;[B] 1 by 1 FE </span></p><p class="c3"><span class="c1">&nbsp;[C] 1 by 1 classification by separate binary SVM for each class</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Fast RCNN </span></p><p class="c3"><span class="c1">&nbsp;[A] Selective Search</span></p><p class="c3"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c3"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Faster RCNN</span></p><p class="c3"><span class="c1">&nbsp;[A] RPN + Anchor box</span></p><p class="c3"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c3"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Mask RCNN </span></p><p class="c3"><span class="c1">&nbsp;[A] RPN + Anchor Box</span></p><p class="c3"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Align</span></p><p class="c3"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c3"><span class="c1">&nbsp;[D] Segmentation Branch</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c13 c27">Symbols and Short forms</span></p><hr><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 792.79px; height: 324.78px;"><img alt="" src="files/rcnn/image5.png" style="width: 792.79px; height: 324.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">Below are some short forms that will be used in the later sections of this chapter : </span></p><p class="c3"><span class="c1">RoI &nbsp; &nbsp;: Region of Interest. Small cropped section of an image.</span></p><p class="c3"><span class="c1">Fmap /feat map : Feature Map. &nbsp;Truncated 3-dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.</span></p><p class="c3"><span class="c1">FE &nbsp; &nbsp; : Feature Extractor. It will represent a deep learning model that extracts features from input data. The term &lsquo;FE&rsquo; will be used synonymously with any deep neural network architecture. e.g VGG16 which exactly does the same.</span></p><p class="c3"><span class="c1">FC &nbsp; &nbsp; : Fully Connected Dense Layer/Layers.</span></p><p class="c3"><span class="c1">bbox &nbsp;: Bounding box. [i.e box coordinates x,y,w,h]</span></p><p class="c3"><span class="c1">GT &nbsp; &nbsp;: Ground Truth</span></p><p class="c3"><span class="c1">CNN : Convolutional Neural Network. ; Conv. : Convolutional.</span></p><p class="c3"><span class="c1">For explainability of the figures I have used consistent symbols while drawing part of the model. &nbsp;The symbols and references are defined at the end.</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c5">Appendix :</span></p><hr><p class="c3 c7"><span class="c22 c10 c13"></span></p><p class="c7 c32"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Selective search &nbsp;</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c1">Quote from Uijlings et al 2013 - &quot;Our grouping procedure now works as follows. We first</span></p><p class="c3"><span class="c1">use (Felzenszwalb and Huttenlocher 2004) to create initial regions. Then we use a greedy algorithm to iteratively group regions together: First the similarities between all neighbour- ing regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours. The process ofgroup- ing the most similar regions is repeated until the whole image becomes a single region&quot;</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.50px; height: 360.45px;"><img alt="" src="files/rcnn/image12.png" style="width: 519.50px; height: 360.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image from the selective search paper by &nbsp;Uijlings et al 2013</span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3"><span class="c4">Relevance</span><span class="c1">&nbsp;between the above explanation and the authors figure in the respective papers. The steps [A],[B], [C] and [D] are shown below for better correlating with the above content, while reading from the paper.</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 574.50px; height: 540.29px;"><img alt="" src="files/rcnn/image4.png" style="width: 574.50px; height: 540.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c8">(i) Top : Image from RCNN paper by Girshick et al. 2014. (ii) Bottom : Image from Fast RCNN paper by Girshick et al. 2015</span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 342.50px; height: 349.19px;"><img alt="" src="files/rcnn/image1.png" style="width: 342.50px; height: 349.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 411.50px; height: 223.90px;"><img alt="" src="files/rcnn/image3.png" style="width: 411.50px; height: 223.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3"><span class="c8">&nbsp;Left : Image from Faster RCNN paper Ren et al 2016 , Right : Image from Mask RCNN from He et al </span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3 c7"><span class="c8"></span></p><p class="c3"><span class="c27">References :</span></p><hr><p class="c3 c7"><span class="c8"></span></p><p class="c3 c7"><span class="c14 c10"></span></p><p class="c3"><span class="c10">GitHub Repositories:</span></p><ol class="c24 lst-kix_eg1j27aju83u-0 start" start="1"><li class="c3 c19 li-bullet-0"><span class="c6 c4 c13"><a class="c9" href="https://www.google.com/url?q=https://github.com/facebookresearch/Detectron&amp;sa=D&amp;source=editors&amp;ust=1666851627108600&amp;usg=AOvVaw1F74N985wEkApkTSkrzqOa">https://github.com/facebookresearch/Detectron</a></span></li><li class="c3 c19 li-bullet-0"><span class="c6 c4 c13"><a class="c9" href="https://www.google.com/url?q=https://github.com/matterport/Mask_RCNN&amp;sa=D&amp;source=editors&amp;ust=1666851627109162&amp;usg=AOvVaw3wBlCBhalflADR1U21N830">https://github.com/matterport/Mask_RCNN</a></span></li></ol><p class="c3 c7 c12"><span class="c14 c10"></span></p><p class="c3"><span class="c14 c10">Models :</span></p><p class="c3"><span class="c1">1. ImageNet Classification with Deep Convolutional</span></p><p class="c3"><span class="c4">Neural Networks : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1666851627110198&amp;usg=AOvVaw3GtwWCJdSU3kkNZs3_ScSK">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></span><span class="c1">&nbsp; &nbsp;</span></p><p class="c3"><span class="c4">2. Very Deep Convolutional Networks For Large-Scale Image Recognition: </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.1556.pdf&amp;sa=D&amp;source=editors&amp;ust=1666851627110783&amp;usg=AOvVaw0qEsU0hEqLiJC6tO5pdq1X">https://arxiv.org/pdf/1409.1556.pdf</a></span></p><p class="c3"><span class="c4">3. Visualizing and understanding convolutional neural networks :</span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/pdf/1311.2901.pdf&amp;sa=D&amp;source=editors&amp;ust=1666851627111265&amp;usg=AOvVaw3bc1P5jqzOJgWEXqcXB1Ac">https://arxiv.org/pdf/1311.2901.pdf</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Region Proposal Generation </span></p><p class="c3"><span class="c4">4. Selective search : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s11263-013-0620-5&amp;sa=D&amp;source=editors&amp;ust=1666851627111999&amp;usg=AOvVaw3rniLqC4YyCARqLkkRRmkg">https://link.springer.com/article/10.1007/s11263-013-0620-5</a></span></p><p class="c3"><span class="c4">5. Objectness &nbsp; &nbsp; &nbsp; &nbsp; : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/2004.02945&amp;sa=D&amp;source=editors&amp;ust=1666851627112446&amp;usg=AOvVaw2Kt5puV85yo93bzAi74nnU">https://arxiv.org/abs/2004.02945</a></span></p><p class="c3"><span class="c1">Object Detection</span></p><p class="c3"><span class="c4">6. SPP-Net: Deep Absolute Pose Regression with Synthetic Views : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1406.4729&amp;sa=D&amp;source=editors&amp;ust=1666851627112948&amp;usg=AOvVaw2_-yzvDG60U1BSd9fIuXsb">https://arxiv.org/abs/1406.4729</a></span></p><p class="c3"><span class="c4">7. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.6229&amp;sa=D&amp;source=editors&amp;ust=1666851627113364&amp;usg=AOvVaw05CWyONpsrnHXJx-GW9yHs">https://arxiv.org/abs/1312.6229</a></span></p><p class="c3"><span class="c4">8. Deformable Part Models are Convolutional Neural Networks : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.5403&amp;sa=D&amp;source=editors&amp;ust=1666851627113767&amp;usg=AOvVaw3t2nfPJjpx8dQi_ANvdRIK">https://arxiv.org/pdf/1409.5403</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c4">9. R-CNN: </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1311.2524&amp;sa=D&amp;source=editors&amp;ust=1666851627114287&amp;usg=AOvVaw2wD8ba5BpQW7H1swd3VCLO">https://arxiv.org/abs/1311.2524</a></span></p><p class="c3"><span class="c4">10. Fast R-CNN: </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1504.08083&amp;sa=D&amp;source=editors&amp;ust=1666851627114705&amp;usg=AOvVaw3KaHmcyF9_pClcCNT_en5h">https://arxiv.org/abs/1504.08083</a></span></p><p class="c3"><span class="c4">11 Faster R-CNN: </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1666851627115120&amp;usg=AOvVaw17go9Hbol5dYP01hjoA_nH">https://arxiv.org/abs/1506.01497</a></span></p><p class="c3"><span class="c4">12. Mask R-CNN: </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1666851627115525&amp;usg=AOvVaw2biTQIIRDgjAP7clwhqqow">https://arxiv.org/abs/1703.06870</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Multibox Approaches </span></p><p class="c3"><span class="c4">13. Scalable object detection using deep neural networks : </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.2249&amp;sa=D&amp;source=editors&amp;ust=1666851627116154&amp;usg=AOvVaw2XZzr3iAdg7Hw-ZO29MlRl">https://arxiv.org/abs/1312.2249</a></span></p><p class="c3"><span class="c4">14. Scalable, high-quality object detection &nbsp;: </span><span class="c4 c6"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1412.1441&amp;sa=D&amp;source=editors&amp;ust=1666851627116561&amp;usg=AOvVaw2gWUI9ZgIqhag_frHo2jmB">https://arxiv.org/abs/1412.1441</a></span></p><p class="c3 c7"><span class="c1"></span></p><p class="c3"><span class="c14 c10">Blogs </span></p><p class="c3"><span class="c1">Special thanks to for easy such a nice explanation of important things -</span></p><p class="c3"><span class="c4">1. </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/&amp;sa=D&amp;source=editors&amp;ust=1666851627117344&amp;usg=AOvVaw1L5VpEjJdDaTfB5evWujyV">https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</a></span></p><p class="c3"><span class="c4">2. </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46&amp;sa=D&amp;source=editors&amp;ust=1666851627117864&amp;usg=AOvVaw2o5_sfsK8uItRbLPfrUYKl">https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46</a></span></p><p class="c3"><span class="c4">3. </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4&amp;sa=D&amp;source=editors&amp;ust=1666851627118396&amp;usg=AOvVaw37DJgDgT0fcqxhABXMEjz4">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></span></p><p class="c3"><span class="c4">4. </span><span class="c6 c4"><a class="c9" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272&amp;sa=D&amp;source=editors&amp;ust=1666851627118905&amp;usg=AOvVaw3tTsYitSMF1PZDbQicA1Go">https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272</a></span></p><p class="c3 c7"><span class="c8"></span></p></body></html>