<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_b2x7vhnewr2g-6.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-6 0}.lst-kix_b2x7vhnewr2g-8>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-8}.lst-kix_b2x7vhnewr2g-0>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-0}.lst-kix_b2x7vhnewr2g-2>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-2}ol.lst-kix_b2x7vhnewr2g-1.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-1 0}.lst-kix_b2x7vhnewr2g-5>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-5}ol.lst-kix_b2x7vhnewr2g-1{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-2{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-0{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-5{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-4.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-4 0}ol.lst-kix_b2x7vhnewr2g-6{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-3{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-2.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-2 0}ol.lst-kix_b2x7vhnewr2g-4{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-7{list-style-type:none}ol.lst-kix_b2x7vhnewr2g-8{list-style-type:none}.lst-kix_b2x7vhnewr2g-8>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-8,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-1>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-1}.lst-kix_b2x7vhnewr2g-4>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-4}ol.lst-kix_b2x7vhnewr2g-7.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-7 0}.lst-kix_b2x7vhnewr2g-7>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-7,lower-latin) ". "}ol.lst-kix_b2x7vhnewr2g-0.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-0 0}.lst-kix_b2x7vhnewr2g-7>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-7}ol.lst-kix_b2x7vhnewr2g-5.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-5 0}.lst-kix_b2x7vhnewr2g-5>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-5,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-6>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-6,decimal) ". "}.lst-kix_b2x7vhnewr2g-4>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-4,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-1>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-1,lower-latin) ". "}.lst-kix_b2x7vhnewr2g-2>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-2,lower-roman) ". "}.lst-kix_b2x7vhnewr2g-6>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-6}.lst-kix_b2x7vhnewr2g-3>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-3,decimal) ". "}.lst-kix_b2x7vhnewr2g-3>li{counter-increment:lst-ctn-kix_b2x7vhnewr2g-3}ol.lst-kix_b2x7vhnewr2g-8.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-8 0}.lst-kix_b2x7vhnewr2g-0>li:before{content:"" counter(lst-ctn-kix_b2x7vhnewr2g-0,decimal) ". "}ol.lst-kix_b2x7vhnewr2g-3.start{counter-reset:lst-ctn-kix_b2x7vhnewr2g-3 0}ol{margin:0;padding:0}table td,table th{padding:0}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c10{color:#666666;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:sub;font-size:12pt;font-family:"Arial";font-style:normal}.c12{color:#67ab9f;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:italic}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c18{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c5{background-color:#ffffff;font-size:12pt;color:#202124}.c33{text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c24{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c22{background-color:#f8f9fa;font-size:9pt;font-style:italic}.c14{font-size:13.5pt;font-style:italic;font-weight:700}.c35{color:#333333;font-style:normal}.c27{font-style:italic;color:#666666}.c16{color:inherit;text-decoration:inherit}.c34{color:#333333;font-style:italic}.c17{font-size:16pt}.c29{font-size:20pt}.c32{font-size:13.5pt}.c26{font-size:14pt}.c38{color:#333333}.c30{color:#67ab9f}.c36{font-style:italic}.c19{color:#202124}.c3{font-size:10.5pt}.c6{font-size:12pt}.c4{vertical-align:sub}.c28{color:#666666}.c37{font-size:10pt}.c13{vertical-align:super}.c31{background-color:#f8f9fa}.c21{height:11pt}.c25{font-size:11pt}.c20{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c24 doc-content"><p class="c8"><span>&nbsp; </span><span class="c17">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c15 c17">&nbsp; &nbsp; &nbsp; &nbsp;Object Detection : RCNN Family</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c8"><span class="c15 c17">&nbsp;</span></p><p class="c8"><span class="c20 c36">Object Detection</span><span class="c1">&nbsp;consists of two sub tasks (i) Object Localization (ii) Object Classification. For the purpose of implementation, we can further break it down to total three steps -</span></p><p class="c8"><span class="c20">[A]</span><span>&nbsp;</span><span class="c20">Object localization.</span><span class="c1">&nbsp;This step takes the rgb image and extracts it to the N number of x,y,w,h bounding box coordinates for potential objects using a specific algorithm i.e Selective search. </span></p><p class="c8"><span class="c20">[B]</span><span>&nbsp;</span><span class="c20">Feature map Extraction.</span><span class="c1">&nbsp;It takes an image array and converts it to a high dimensional feature map array using a convolutional layer based neural network i.e vgg16, resnet 50 etc.</span></p><p class="c8"><span class="c20">[C]</span><span>&nbsp;</span><span class="c20">Classification.</span><span class="c1">&nbsp;It takes each of the extracted feature maps and classifies it to an object category using dense layers or some machine learning classifier algorithm i.e SVM (support vector machine). Detailed explanation of step A,B and C can be found on the right section.</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c1">Some of the short forms that will be used in the later sections of this chapter : </span></p><p class="c8"><span class="c1">RoI &nbsp; &nbsp;: Region of Interest. Small cropped section of an image.</span></p><p class="c8"><span class="c1">Fmap : Feature Map. &nbsp;Truncated 3 dimensional output of a CNN based network from any layer at the middle. It represents the learnt features upto that layer.</span></p><p class="c8"><span class="c1">FE &nbsp; &nbsp; : Feature Extractor. It will be used synonymously with CNN based network architecture. e.g VGG16 can be denoted as FE sometimes.</span></p><p class="c8"><span class="c1">FC &nbsp; &nbsp; : Fully Connected Dense Layer/Layers.</span></p><p class="c8"><span class="c1">bbox &nbsp;: Bounding box. [i.e box coordinates x,y,w,h]</span></p><p class="c8"><span class="c1">GT &nbsp; &nbsp;: Ground Truth</span></p><p class="c8"><span class="c1">CNN : Convolutional Neural Network. ; Conv. : Convolutional.</span></p><p class="c8"><span class="c1">For explainability of the figures I have used the consistent symbols for drawing part of the model. &nbsp;The symbols and references are defined at the end.</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c11 c26">RCNN</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c9 c3">The RCNN method uses the following modules to perform object detection :</span></p><p class="c8"><span class="c3 c20">Selective Search &nbsp;:</span><span class="c9 c3">&nbsp;It is an algorithm for generating region proposal suggestions. It suggests potential bboxes that may contain an object. &#39;Objectness&#39; is another similar algorithm that can be used for region proposal generation. SOTA works now use a simple CNN based architecture (region proposal network) for this purpose. See the right section for more about selective search. </span></p><p class="c8"><span class="c3 c20">FE : &nbsp;</span><span class="c9 c3">It can be any deep convolutional neural network. The output of the network is truncated from the last dense layer (if a 1d feature vector is necessary) or any of the intermediate convolutional layers (if a 2 dimensional feature map is required). The 2d featuremap also preserves the spatial relationship with the input image due to the sliding nature of convolution operation. &nbsp;For RCNN they don&#39;t require the spatial relation, so use the output of a dense layer as a 1d feature vector. For fast RCNN it will need a 2d feature map instead.</span></p><p class="c8"><span class="c3 c20">Binary SVM :</span><span class="c9 c3">&nbsp;Support vector machine algorithm is used for binary (yes/no) classification every single class. Binary SVM is specially suited for single class classification since it tries to select the hyperplane that separates the two classes with highest margin.</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 850.50px; height: 705.15px;"><img alt="" src="files/rcnn/image9.png" style="width: 850.50px; height: 705.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c10">Figure : RCNN (top) and Fast RCNN (bottom)</span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c1"></span></p><p class="c0"><span class="c3">Most of the SOTA methods use an updated version of it. For example yolo uses &nbsp;G</span><span class="c3 c4">x</span><span class="c3">&nbsp;= Sigmoid(P) +P</span><span class="c3 c4">x </span><span class="c3">they use sigmoid in place of d</span><span class="c3 c4">x</span><span class="c9 c3">&nbsp;for restricting it between 0-1. They use a fixed grid system with a potential bbox at every location of the featuremap as an initial bbox then corrects it with regression. Don&#39;t worry, It will be discussed in detail in respective sections.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c9 c3">This approach was Inspired from the Deformable part model [] 2010.</span></p><p class="c0"><span class="c3">The initial bbox predictions are P = &nbsp;[P</span><span class="c3 c4">x</span><span class="c3">,P</span><span class="c3 c4">y</span><span class="c3">,P</span><span class="c3 c4">w</span><span class="c3">,P</span><span class="c3 c4">h</span><span class="c9 c3">]</span></p><p class="c0"><span class="c3">The initial bbox ground truths are G = [G</span><span class="c3 c4">x</span><span class="c3">,G</span><span class="c3 c4">y</span><span class="c3">,G</span><span class="c3 c4">w</span><span class="c3">,G</span><span class="c3 c4">h</span><span class="c9 c3">]</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c3">For x and y, scale invariant transformation d</span><span class="c3 c4">x</span><span class="c3">(P) d</span><span class="c3 c4">y</span><span class="c3">(P) t is learnt. Its value has no relation with the location of the box itself, rather it is just an offset from a rough initial prediction. If we restrict the value of d from 0-1 i.e applying sigmoid then the value of the offset t</span><span class="c3 c4">x</span><span class="c3">&nbsp;and t</span><span class="c3 c4">y</span><span class="c3">&nbsp;can maximum P</span><span class="c3 c4">x</span><span class="c3">&nbsp;and P</span><span class="c3 c4">y</span><span class="c9 c3">&nbsp;respectively.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c9 c3">In my understanding - &nbsp;A log scale transformation is learnt for w and h so that it can learn a wide range of change in w and h direction (by taking the difference of their log instead of their exact value. eq [3] and [5]) Since in reality w and h can vary with quite high magnitude compared to x and y.</span></p><p class="c0"><span class="c3">Fast RCNN uses the same only difference is t</span><span class="c3 c4">i</span><span class="c3">=d</span><span class="c3 c4">i</span><span class="c9 c3">(x) becomes d(output from FC regression layers)</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c11 c3">BBOX Regression</span></p><p class="c0"><span class="c3">Mostly all the SOTA methods use an updated version of it. For example yolo uses &nbsp;G</span><span class="c3 c4">x</span><span class="c3">&nbsp;= Sigmoid(P) +P</span><span class="c3 c4">x </span><span class="c3">they use sigmoid in place of d</span><span class="c3 c4">x</span><span class="c9 c3">&nbsp;for restricting it between 0-1. They use a fixed grid system with a potential bbox at every location of the featuremap as an initial bbox then corrects it with regression. Don&#39;t worry, It will be discussed in detail in respective sections.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c9 c3">This approach was Inspired from the Deformable part model [] 2010.</span></p><p class="c0"><span class="c3">The initial bbox predictions are P = &nbsp;[P</span><span class="c3 c4">x</span><span class="c3">,P</span><span class="c3 c4">y</span><span class="c3">,P</span><span class="c3 c4">w</span><span class="c3">,P</span><span class="c3 c4">h</span><span class="c9 c3">]</span></p><p class="c0"><span class="c3">The initial bbox ground truths are G = [G</span><span class="c3 c4">x</span><span class="c3">,G</span><span class="c3 c4">y</span><span class="c3">,G</span><span class="c3 c4">w</span><span class="c3">,G</span><span class="c3 c4">h</span><span class="c9 c3">]</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c3">For x and y, a scale invariant transformation d</span><span class="c3 c4">x</span><span class="c3">(P) d</span><span class="c3 c4">y</span><span class="c3">(P) t is learnt. Its value has no relation with the location of the box itself, rather it is just an offset from a rough initial prediction. If we restrict the value of d from 0-1 i.e applying sigmoid then the value of the offset t</span><span class="c3 c4">x</span><span class="c3">&nbsp;and t</span><span class="c3 c4">y</span><span class="c3">&nbsp;can maximum P</span><span class="c3 c4">x</span><span class="c3">&nbsp;and P</span><span class="c3 c4">y</span><span class="c9 c3">&nbsp;respectively.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c9 c3">In my understanding - &nbsp;A log scale transformation is learnt for w and h so that it can learn a wide range of change in w and h direction (by taking the difference of their log instead of their exact value. eq [3] and [5]) Since in reality w and h can vary with quite high magnitude compared to x and y.</span></p><p class="c0"><span class="c3">Fast RCNN uses the same only difference is t</span><span class="c3 c4">i</span><span class="c3">=d</span><span class="c3 c4">i</span><span class="c9 c3">(x) becomes d(output from FC regression layers)</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c3 c20">Hard Negative Mining :</span><span class="c9 c3">&nbsp; It is an approach that oversamples the difficult samples more and thus tries to make it learn faster. It was possible to achieve good accuracy after just one pass over the data using hard negative mining for the SVM models.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 869.44px; height: 459.77px;"><img alt="" src="files/rcnn/image6.png" style="width: 869.44px; height: 459.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c11 c3">RCNN Algorithm :</span></p><p class="c8"><span class="c9 c3">1. [A] Selective Search algorithm is used for generating ~ 2k sets of bounding box coordinate (x,y,w,h) proposals per image. </span></p><p class="c8"><span class="c9 c3">2. [A] Using the bbox coordinates found from selective search, image RoIs of different size are cropped. Each RoI is &#39;wrapped&#39; to a fixed size [resizing with bilinear/nearest sampling]. Fixed sized RoI is required for the classifier to take input later from the FE.</span></p><p class="c8"><span class="c9 c3">3. [B] A deep convolutional neural network is used for extracting feature maps from each of the image RoI one by one. The model output is truncated from the bottom and output from a dense layer with 4096 nodes is extracted as a feature map.</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Pre training with ICSLV-2012 images with 200 classes lr 0.01 on CNN architecture by krizhevsky at el. 2012.</span></p><p class="c8"><span class="c9 c3 c31">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Oversampling positive classes 32 positive sample + 96 neg sample per mini-batch</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Fine tuning with lr 0.001 with wrapped region proposal with N + 1 class. N is no of object class 1 is background</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During fine tuning a region proposal with more than 0.50 IoU with a positive sample was taken as positive.</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; During Feature Extraction the last layer is removed and features from an internal dense layer are collected. i.e 1024 or 2048 features</span></p><p class="c8"><span class="c9 c3">4. [C] &nbsp;Separate binary classifiers for each of the N (N=no of categories) are trained to categorize the fmaps to respective object categories.</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; One binary SVM classifier is used for each of the classes.</span></p><p class="c8"><span class="c9 c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-&gt; Hard negative mining [4] is found to be useful for fast convergence. For detail see right section.</span></p><p class="c8"><span class="c9 c3">5. [*] Fine tuning of the localization boxes are done by bbox regression using Ridge regression with lambda = 1000 [for detail see the section at right]. The regressor only learns the x,y offset from the corner of the bbox predicted by selective search. For w,h it is log offset instead. </span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c9 c3">As per our today&#39;s practice on SOTA object detection algorithms several things might seem redundant -</span></p><p class="c8"><span class="c9 c3">[i] Extracting feature maps for every RoI one by one. [It is solved in Fast RCNN eventually]</span></p><p class="c8"><span class="c9 c3">[ii] Using a separate SVM for each class instead of using the softmax class outputs of the feature extractor network. [Improved in Fast RCNN]</span></p><p class="c8"><span class="c9 c3">[iii] The word &quot;wrapped&quot; image is used in the paper for resizing each Fmap RoI to a fixed size. Nowadays a good practice is using bilinear sampling. Note that - rcnn first resizes (wrapped) each of the image RoI then inputs them to Feature extractor but Fast RCNN will resize the feature maps using RoI pooling layer.</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c9 c3">&nbsp;Using additional SVM classifier instead of the dense layer output from the feature extractor network :</span></p><p class="c8"><span class="c9 c3">Quote from the paper&#39;s appendix B : &quot;We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classi- fier was trained on randomly sampled negative examples rather than on the subset of &ldquo;hard negatives&rdquo; used for SVM training.&quot;</span></p><p class="c8"><span class="c9 c3">Anyway, in Fast RCNN the authors were finally able to do it using a small dense network after the feature extractor, without any SVM classifier.</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c33 c26 c20 c34">Fast RCNN</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c33 c14 c38"></span></p><p class="c8"><span class="c15 c3">Improvements in Fast RCNN:</span></p><p class="c8"><span class="c9 c3">- Fast RCNN eliminated one by one RoI input into the feature extractor. There are lots of common regions between RoI&#39;s specially when nearly 2000 RoI generated from one image. This method applies a feature extractor for the whole image just once and then it crops the corresponding RoI for each of the ~ 2000 corresponding regions from the feature map. </span></p><p class="c8"><span class="c9 c3">- Replacing image wrapping with RoI Pooling.</span></p><p class="c8"><span class="c9 c3">- Getting rid of per class svm by using the softmax outputs from dense layers after feature extractor.</span></p><p class="c7"><span class="c3 c12"></span></p><p class="c8"><span class="c3 c20 c30">&nbsp;</span><span class="c15 c3">Fast RCNN Algorithm :</span></p><p class="c8"><span class="c9 c3">1.[A] Selective Search generates ~ 2k RoI proposals per image. </span></p><p class="c8"><span class="c9 c3">2.[B] A CNN based network i.e VGG16 is used for feature extraction.</span></p><p class="c8"><span class="c9 c3">3.[B] The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps RoIs so that it can fit into the dense layer afterwards. (H=7, W=7 for VGG 16) </span></p><p class="c8"><span class="c3">4. [C] The last fully connected layer is replaced with 2 sibling dense layers (i) one &nbsp;for classification (ii) another for bbox regression. For regression it adopted the same fine tuning mechanism (optimizing t</span><span class="c3 c4">x</span><span class="c3">,t</span><span class="c3 c4">x</span><span class="c3">,t</span><span class="c3 c4">x</span><span class="c3">,t</span><span class="c3 c4">x</span><span class="c9 c3">) like RCNN. </span></p><p class="c7"><span class="c9 c3"></span></p><p class="c0"><span class="c11 c3">RoI Pooling Layer</span></p><p class="c0"><span class="c9 c3">RoI Pooling Layer : The &nbsp;operator [*] represents the flooring operation here. e.g [1.13] = 1.00</span></p><p class="c0"><span class="c9 c3">The functionality of RoI Pooling layer can be divided into two parts -</span></p><p class="c0"><span class="c3 c20">[1] Scaling the image RoIs to fmap dimension : </span><span class="c9 c3">Due to multiple polling operations in the consecutive convolutional layers, the feature map array has smaller (e.g 16~32 times smaller) width and height compared to the original image. In the example below the feat map is 16 times smaller than the original image in terms of width and height. The RoI Pooling layer scales down each of the RoI coordinates (generated at the image scale) so that it fits on top of the feature map in the corresponding feature location for that image RoI. Each RoI coordinates need to be divided by the scale factor (here it is 16) for fitting on the feature map. In this way the 188x188 RoI window coordinates are converted to 11x11 below. The center coordinates (x,y) of the RoI is also divided by the factor like (w,h) which is not shown in the figure for brevity.</span></p><p class="c0 c21"><span class="c9 c3"></span></p><p class="c0"><span class="c3 c20">[2] Resizing the Fmap RoI to fit the fixed input of dense layers :</span><span class="c9 c3">&nbsp;The dense layer has fixed input size i.e 25 dense nodes. If a 5x5x1 feature map tensor is flattened (5x5x1=25) only then it will fit the dense input. For 5x5x10 the dense should have 250 input nodes. For each of the RoI Fmaps do a maxpool operation with kernel size ([h/h&#39;&#39;], [w/w&#39;&#39;]) for below example it will be ([11/5],[11/5]) = (2,2). Now an arbitrary feature map of size 11x11 is operated with a maxpool of kernel size 2 for fitting fixed 5x5 input size of the dense layers afterward. Another example can be like - an orbitary Fmap RoI of size 20x20 would require a kernel of 4x4 for fitting it into 5x5 input size</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 701.50px; height: 576.46px;"><img alt="" src="files/rcnn/image12.png" style="width: 701.50px; height: 576.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c20 c27">&nbsp;</span><span class="c10">Figure : RoI Pooling Layer top 2d view with numerical examples bottom 3d view of the same thing</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c14">&nbsp;</span><span class="c11 c26">Faster RCNN</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c15 c3"></span></p><p class="c8"><span class="c15 c3">Improvements :</span></p><p class="c8"><span class="c9 c3">- The selective search is replaced with a Region Proposal Network that mostly shares weights with the Feature Extractor Network.</span></p><p class="c8"><span class="c9 c3">- The region proposals are further broken down to 3 different scales with 3 different aspect ratios called anchor boxes.</span></p><p class="c7"><span class="c11 c3"></span></p><p class="c8"><span class="c11 c3">The Algorithm:</span></p><p class="c8"><span class="c15 c3">[A] RPN (Region Proposal Network): </span></p><p class="c8"><span class="c9 c3">&nbsp;- Region Proposal Network proposes possible box coordinates and objectness confidence at each pixel location of the feature map. This is a class agnostic box predictor so it has nothing to do with the category of the object. The output of the RPN module is converted to bbox coordinates at every feature map location and trained with the bbox regressor. </span></p><p class="c8"><span class="c3">&nbsp; - The output of the last shared convolutional layer is passed through a n</span><span class="c3 c20">x</span><span class="c9 c3">n convolution layer (e,g 3x3) then forwarded to - 2 sibling 1x1 convolution layers for bbox and confidence prediction at every fmap location. The 1x1 convolution layer can also be interpreted as a dense layer applied at every pixel locations and results in an array with same height and width while having depth of 4k and 2k respectively, k= number of anchor box per location. &nbsp;</span></p><p class="c8"><span class="c9 c3">&nbsp;- At each pixel location we can get k=9 boxes and respective confidence scores. It will make it possible to detect two objects centered at the same image location (having different aspect ratio or scale). If the output of the RPN has width m and height n the depth would be 2k and 4k for confidence and bbox outputs. So a total of m*n*k set of (x,y,w,h) can be extracted from the depthwise 4k data points at each of the m*n locations. Similarly m*n*k set of (object conf,background conf) can be found from 2k data points depthwise. For more on it see &#39;Anchor box&#39; section on right.</span></p><p class="c8"><span class="c9 c3">&nbsp;- The RPN is trained with binary classification labels. Among the potential k boxes at every feat. map location, boxes having IoU more than 0.7 (if multiple is found then the highest one is taken only) it is taken as a positive sample. Boxes with less than 0.3 IoU with all the GT boxes are taken as negative samples (background), others are ignored. </span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c15 c3">[B] Feature Extractor :</span></p><p class="c8"><span class="c9 c3">&nbsp;- It used the CNN network feature extractor which shares its weights with the RPN (until the RPN specific layers). It used the VGG16 and also ZF [zeiler at el.] pre-trained on ImageNet dataset for demonstrating their algorithm performance.</span></p><p class="c8"><span class="c9 c3">&nbsp;- The last max pool layer is replaced with a RoI Pooling layer for extracting fixed sized feature maps for every Fmap RoI so that it can fit into the first dense layer (H=7, W=7 for VGG 16) </span></p><p class="c8"><span class="c9 c3">&nbsp;- RoI pooling layer projects the RoI coordinates on top of the feature map array in a max pooling manner, the difference here is it adjusts the pooling window size for keeping the output size fixed for all the fmap. RoIs.</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c3 c20">[C] Classifier :</span><span class="c9 c3">&nbsp;Each of the output (fmap RoIs) from the RoI Pooling layer is sent to a small dense network ending with 2 sibling dense layers (i) one for classification (ii) another for bbox regression. The regressor here </span></p><p class="c8"><span class="c9 c3">learns the x,y offset and log offset for w and h from the prior boxes predicted by RPN initially.</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 907.77px; height: 801.39px;"><img alt="" src="files/rcnn/image13.png" style="width: 907.77px; height: 801.39px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c10">Figure : Faster RCNN</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c6 c11">Anchor Box / Multi Box / Default Box</span></p><p class="c7"><span class="c33 c3 c20 c35"></span></p><p class="c8"><span class="c33 c35 c3 c20">Scale invariance</span></p><p class="c8"><span class="c3">Gw = P</span><span class="c3 c4">w</span><span class="c3">&nbsp;exp(d</span><span class="c3 c4">w</span><span class="c9 c3">(FC outputs)) --- [1] </span></p><p class="c8"><span class="c3">Gh = P</span><span class="c3 c4">h</span><span class="c3">&nbsp;exp(d</span><span class="c3 c4">h</span><span class="c9 c3">(FC outputs)) &nbsp;---- [2]</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c9 c3">Anchor boxes are prior box estimates at every feature map location. They are determined before the training and fed into the network beforehand. &nbsp;The prior boxes at every image location are further fine tuned by putting them on eq. 1 and 2 before predicting the final bbox. So the model basically learns how to fit each of the anchor boxes to the object present at that location. At the same time it learns two confidence scores for object and background for each anchor box. That&#39;s why the 2k scores are associated with each anchor box.</span></p><p class="c8"><span class="c9 c3">Anchor boxes were first used by [10][11] sometimes referred to as the Multibox approach (in contrast to one box at one image location) . </span></p><p class="c8"><span class="c3 c31">Initial anchor box methods used different anchors (prior boxes) at different locations of the image resulting in a lot of unique anchors. That&#39;s why the</span><span class="c9 c3">&nbsp;Multibox method [11] was not translation invariant. It used 800 anchor boxes at different locations of the image. The found this 800 anchors by k means clustering of the [x,y,w,h] of all the Ground truth bboxes. While faster rcnn does k means clustering for h,w only (defining the scale and aspect ratio on the object regardless of the x,y location in image). So these 9 boxes can just fit anywhere in the image. We don&#39;t need 800 prior boxes any more. They did some experiments with different values of k and found k=9 anchor boxes are optimal for achieving the best mAP. They used these same 9 boxes at every locations of the image, capable of producing (w*h=~2400) ~2400 x 9 RoI per image at maximum.</span></p><p class="c8"><span class="c9 c3">&nbsp;-&gt; For Multibox the shape of tensor for RoI prediction was 4+1* 800 dimensional &nbsp;fully connected layer</span></p><p class="c8"><span class="c9 c3">&nbsp;-&gt; &nbsp;For Faster RCNN it reduced to 4+2*9 convolutional output layer.</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 637.50px; height: 310.45px;"><img alt="" src="files/rcnn/image5.png" style="width: 637.50px; height: 310.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span class="c20 c22">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c22 c20 c28">Image from Faster RCNN paper Ren et al 2016</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 488.50px; height: 406.64px;"><img alt="" src="files/rcnn/image7.png" style="width: 488.50px; height: 406.64px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c27 c20">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c10">Figure : &nbsp;Total 9 anchor boxes at one pixel locations</span></p><p class="c7"><span class="c9 c3"></span></p><p class="c7"><span class="c9 c3"></span></p><p class="c7"><span class="c1"></span></p><p class="c0"><span class="c15 c6">RPN Loss :</span></p><p class="c0 c21"><span class="c9 c6"></span></p><p class="c0"><span class="c6">x, x</span><span class="c6 c4">a</span><span class="c6">, x</span><span class="c6 c13">*</span><span class="c9 c6">&nbsp;are prediction , anchor box, ground truth.</span></p><p class="c0"><span class="c6">t</span><span class="c6 c4">i </span><span class="c6">&nbsp;: x,y,w,h of &nbsp;i</span><span class="c6 c13">th</span><span class="c6">&nbsp;bbox , p</span><span class="c6 c4">i</span><span class="c6">&nbsp;and p</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c9 c6">&nbsp;are prediction and GT confidence.</span></p><p class="c0"><span class="c6">p</span><span class="c13 c37">obj</span><span class="c6">,p</span><span class="c6 c13">bg</span><span class="c9 c6">&nbsp;in for object and background confidence.</span></p><p class="c0"><span class="c6">L</span><span class="c29 c4">reg</span><span class="c6">(t</span><span class="c29 c4">x</span><span class="c6">&nbsp;, t</span><span class="c4 c29">x</span><span class="c29 c13">*</span><span class="c6">&nbsp;) &nbsp;= (x-x*)/w &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;L</span><span class="c6 c4">reg</span><span class="c6">(t</span><span class="c6 c4">y</span><span class="c6">&nbsp;, t</span><span class="c6 c4">y</span><span class="c6 c13">*</span><span class="c6">&nbsp;) = (y - y</span><span class="c6 c13">*</span><span class="c6">)/h</span><span class="c6 c4">a</span><span class="c9 c6">&nbsp;;</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">reg</span><span class="c6">(t</span><span class="c6 c4">w</span><span class="c6">&nbsp;, t</span><span class="c6 c4">w</span><span class="c6 c13">*</span><span class="c6">&nbsp;) = log(w / w</span><span class="c6 c13">*</span><span class="c6">)/w</span><span class="c6 c4">a</span><span class="c6">&nbsp; &nbsp;; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;L</span><span class="c6 c4">reg</span><span class="c6">(t</span><span class="c6 c4">h</span><span class="c6">&nbsp;, t</span><span class="c6 c4">h</span><span class="c6 c13">*</span><span class="c6">&nbsp;) &nbsp;= log(h / h</span><span class="c6 c13">*</span><span class="c6">)/h</span><span class="c2">a</span></p><p class="c0"><span class="c9 c6">&nbsp;</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">i</span><span class="c6">&nbsp;= &nbsp;L</span><span class="c6 c4">cls</span><span class="c6">&nbsp;+ L</span><span class="c2">bbox</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">cls</span><span class="c6">&nbsp; = 1/N</span><span class="c6 c4">cls</span><span class="c6">&nbsp;x </span><span class="c5">&Sigma;</span><span class="c19 c6 c13">N</span><span class="c19 c6 c4">i </span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c6 c13 c19">M</span><span class="c19 c6 c4">j </span><span class="c6">-p</span><span class="c6 c13">obj</span><span class="c6 c4">i</span><span class="c6 c13">* </span><span class="c6">log(p</span><span class="c6 c13">obj</span><span class="c6 c4">i</span><span class="c6">) - p</span><span class="c6 c13">bg</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">log(p</span><span class="c6 c13">bg</span><span class="c6 c4">i</span><span class="c9 c6">) ]</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">reg</span><span class="c6">&nbsp;= &lambda; x 1/N</span><span class="c6 c4">reg &nbsp;</span><span class="c6">x </span><span class="c5">&Sigma;</span><span class="c19 c6 c13">N</span><span class="c19 c6 c4">i</span><span class="c6">&nbsp;p</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">&nbsp;x Lreg(t</span><span class="c6 c4">i</span><span class="c6">, t</span><span class="c6 c4">i</span><span class="c6">* &nbsp;) + Robust L</span><span class="c6 c4">1</span><span class="c9 c6">&nbsp;</span></p><p class="c0"><span class="c6">here N</span><span class="c6 c4">cls</span><span class="c6">&nbsp;= 256 ; N</span><span class="c6 c4">reg</span><span class="c9 c6">&nbsp;= 2400 ; &nbsp;Lambda = 10 </span></p><p class="c0 c21"><span class="c9 c6"></span></p><p class="c0"><span class="c6 c20">Fast-RCNN</span><span class="c6 c13 c20">*</span><span class="c15 c6">&nbsp; &nbsp;Loss :</span></p><p class="c0 c21"><span class="c9 c6"></span></p><p class="c0"><span class="c9 c6">Here the regression loss is mostly the same except for the fact that it represents the RPN predicted coordinates here (instead of the GT). So here the loss function learns the offset from RPN predicted coordinates.</span></p><p class="c0"><span class="c6">Loss = L</span><span class="c6 c4">cls</span><span class="c6">&nbsp;+ L</span><span class="c2">bbox</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">bbox</span><span class="c6">&nbsp;= 1/N x </span><span class="c5">&Sigma;</span><span class="c5 c13">N</span><span class="c5 c4">i</span><span class="c6">&nbsp;P</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">&nbsp;x (t </span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">- t</span><span class="c6 c4">i</span><span class="c6">&nbsp;) ^2 ; t</span><span class="c6 c4">i </span><span class="c6">&nbsp;: x,y,w,h of &nbsp;i</span><span class="c6 c13">th</span><span class="c9 c6">&nbsp;bbox + Robust L1</span></p><p class="c0"><span class="c6">L</span><span class="c6 c4">cls</span><span class="c6">&nbsp;= 1/N x </span><span class="c5">&Sigma;</span><span class="c5 c13">N</span><span class="c5 c4">i</span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c19 c6 c13">M</span><span class="c19 c6 c4">j </span><span class="c6">-p</span><span class="c6 c13">j</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">log(p</span><span class="c6 c13">j</span><span class="c6 c4">i</span><span class="c9 c6">) ]</span></p><p class="c0"><span class="c6">i : i</span><span class="c6 c13">th</span><span class="c6 c9">&nbsp;data sample in mini batch</span></p><p class="c0"><span class="c6">j : j</span><span class="c6 c13">th</span><span class="c9 c6">&nbsp;object category</span></p><p class="c0 c21"><span class="c9 c6"></span></p><p class="c0 c21"><span class="c9 c6"></span></p><p class="c0"><span class="c3 c15">Back propagation :</span></p><p class="c0"><span class="c3 c20">(1) Alternate training :</span><span class="c9 c3">&nbsp;(Used by the authors)</span></p><p class="c0"><span class="c9 c3">RPN and Fast-RCNN share the same network (the &quot;Shared CNN Layers&quot; part in above figure). Rest of the layers are task specific i.e RPN Layers and Dense Layers at the end. Both of the networks are initialized with imagenet pre-trained weights for either VGG-16 (first 13 CNN layers) or ZF network (first 5 CNN layers) &nbsp;[zeiler at el.]. *Fast-RCNN is a special case of Fast-RCNN, &nbsp;that will be trained with region proposals from RPN predictions instead of selective search. </span></p><p class="c0"><span class="c9 c3">- First the RPN is trained. </span></p><p class="c0"><span class="c9 c3">- Then using the region proposal results of RPN is used as input for training the *Fast-RCNN.</span></p><p class="c0"><span class="c9 c3">- Next RPN is initialized with the Fast-RCNN weights (only the shared part) and trained again for the RPN specific layers at the bottom are trained only keeping the shared layers fixed.</span></p><p class="c0"><span class="c9 c3">- Finally, both the *Fast-RCNN is trained again keeping the common layers fixed.</span></p><p class="c0"><span class="c3 c20">(2) Approximate Joint training :</span><span class="c9 c3">&nbsp;Using the same network for RPN and Fast-RCNN* from the beginning. Training them together. For Fast-RCNN the outputs from RPN during forward pass are taken as input assuming them fixed predefined values. Since the RoI pooling layer is not differentiable that&#39;s why they needed to consider the outputs of RPN fixed values.</span></p><p class="c0"><span class="c3 c20">(3) Non Approximate Joint training :</span><span class="c9 c3">&nbsp;In this method they proposed using a differentiable RoI Pooling layer where so that the outputs taken from RoI pooling and fed back to the Fast-RCNN would be a part of the back propagation chain</span></p><p class="c0"><span class="c9 c3">Authors showed their results with method (1) in the paper.</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c32">Mask RCNN</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c15 c32"></span></p><p class="c8"><span class="c15 c3">Improvements in Mask RCNN :</span></p><p class="c8"><span class="c9 c3">- RoI Pooling layer is replaced with RoI align layers.</span></p><p class="c8"><span class="c9 c3">- Additional semantic segmentation branch is added for predicting both bbox and segmentation at a time. I will refer it as [D]</span></p><p class="c7"><span class="c12 c3"></span></p><p class="c8"><span class="c15 c3">The Algorithm Mask RCNN:</span></p><p class="c8"><span class="c9 c3">The algorithm is very similar to faster rcnn other than the two updates mentioned above.</span></p><p class="c8"><span class="c9 c3">&nbsp;</span></p><p class="c7"><span class="c9 c6"></span></p><p class="c8"><span class="c15 c6">Mask RCNN Loss</span></p><p class="c7"><span class="c9 c6"></span></p><p class="c8"><span class="c6">Loss = L</span><span class="c6 c4">cls</span><span class="c6">&nbsp;+ L</span><span class="c6 c4">bbox </span><span class="c6">+ L</span><span class="c2">seg</span></p><p class="c8"><span class="c6">L</span><span class="c6 c4">cls</span><span class="c6">&nbsp;and L</span><span class="c6 c4">reg</span><span class="c9 c6">&nbsp;are similar to Faster RCNN.</span></p><p class="c8"><span class="c6">L</span><span class="c6 c4">seg = </span><span class="c6">1/N x </span><span class="c5">&Sigma;</span><span class="c5 c13">N</span><span class="c5 c4">i</span><span class="c6">&nbsp;[</span><span class="c5">&Sigma;</span><span class="c19 c6 c4">jk </span><span class="c6">-y</span><span class="c6 c13">jk</span><span class="c6 c4">i</span><span class="c6 c13">*</span><span class="c6">log(y</span><span class="c6 c13">jk</span><span class="c6 c4">i</span><span class="c9 c6">) ] , j and k are pixel locations in horizontal and vertical axis.</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 819.16px; height: 509.97px;"><img alt="" src="files/rcnn/image8.png" style="width: 819.16px; height: 509.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c27 c20">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c10">Figure : Mask RCNN Algorithm</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 591.70px; height: 303.94px;"><img alt="" src="files/rcnn/image10.png" style="width: 591.70px; height: 303.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c27 c20">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c10">Figure : RoI Align Layer</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c9 c3">RoI Align layer does the same job of RoI pooling but little accurately using a different approach.</span></p><p class="c8"><span class="c9 c3">[1] RoI Scaling : It does not round up the floating point number while scaling down the RoI sizes to match the feature map dimension/</span></p><p class="c8"><span class="c9 c3">[2] Resizing : It uses bilinear interpolation for converting the floating window size to a fixed window input size i.e 5x5 here.</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c17">Summary</span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">RCNN </span></p><p class="c8"><span class="c1">&nbsp;[A] Selective Search &nbsp;</span></p><p class="c8"><span class="c1">&nbsp;[B] 1 by 1 FE </span></p><p class="c8"><span class="c1">&nbsp;[C] 1 by 1 classification by separate binary SVM for each class</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Fast RCNN </span></p><p class="c8"><span class="c1">&nbsp;[A] Selective Search</span></p><p class="c8"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c8"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Faster RCNN</span></p><p class="c8"><span class="c1">&nbsp;[A] RPN + Anchor box</span></p><p class="c8"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Pooling</span></p><p class="c8"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Mask RCNN </span></p><p class="c8"><span class="c1">&nbsp;[A] RPN + Anchor Box</span></p><p class="c8"><span class="c1">&nbsp;[B] Single pass Feature Extractor + 1x1 RoI Align</span></p><p class="c8"><span class="c1">&nbsp;[C] 1x1 fmap through dense layer and softmax </span></p><p class="c8"><span class="c1">&nbsp;[D] Segmentation Branch</span></p><p class="c7"><span class="c1"></span></p><hr><p class="c7"><span class="c15 c17"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 792.79px; height: 324.78px;"><img alt="" src="files/rcnn/image4.png" style="width: 792.79px; height: 324.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c11 c17">Appendix :</span></p><hr><p class="c7"><span class="c11 c17"></span></p><p class="c21 c23"><span class="c1"></span></p><p class="c8"><span class="c15 c26">Selective search &nbsp;</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c1">Quote from Uijlings et al 2013 - &quot;Our grouping procedure now works as follows. We first</span></p><p class="c8"><span class="c1">use (Felzenszwalb and Huttenlocher 2004) to create initial regions. Then we use a greedy algorithm to iteratively group regions together: First the similarities between all neighbour- ing regions are calculated. The two most similar regions are grouped together, and new similarities are calculated between the resulting region and its neighbours. The process ofgroup- ing the most similar regions is repeated until the whole image becomes a single region&quot;</span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.50px; height: 360.45px;"><img alt="" src="files/rcnn/image11.png" style="width: 519.50px; height: 360.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c10">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Image from the selective search paper by &nbsp;Uijlings et al 2013</span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c10"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 574.50px; height: 540.29px;"><img alt="" src="files/rcnn/image3.png" style="width: 574.50px; height: 540.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c10">(i) Top : Image from RCNN paper by Girshick et al. 2014. (ii) Bottom : Image from Fast RCNN paper by Girshick et al. 2015</span></p><p class="c7"><span class="c1"></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 433.50px; height: 443.06px;"><img alt="" src="files/rcnn/image1.png" style="width: 433.50px; height: 443.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c10">Image from Faster RCNN paper Ren et al 2016</span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c10"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 535.50px; height: 295.30px;"><img alt="" src="files/rcnn/image2.png" style="width: 535.50px; height: 295.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c10"></span></p><p class="c8"><span class="c10">Image from Mask RCNN from He et al</span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c10"></span></p><p class="c7"><span class="c10"></span></p><p class="c8"><span class="c15 c17">References</span></p><p class="c7"><span class="c15 c17"></span></p><p class="c8"><span class="c15 c25">Models :</span></p><p class="c8"><span class="c1">1. ImageNet Classification with Deep Convolutional</span></p><p class="c8"><span>Neural Networks : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1665939918989800&amp;usg=AOvVaw1ql4nDgcjxYBwX7NXe-x2R">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></span><span class="c1">&nbsp; &nbsp;</span></p><p class="c8"><span>2. Very Deep Convolutional Networks For Large-Scale Image Recognition: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.1556.pdf&amp;sa=D&amp;source=editors&amp;ust=1665939918990178&amp;usg=AOvVaw2cb9E47-cBszyolmP8ZyPA">https://arxiv.org/pdf/1409.1556.pdf</a></span></p><p class="c8"><span>3. Visualizing and understanding convolutional neural networks :</span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/pdf/1311.2901.pdf&amp;sa=D&amp;source=editors&amp;ust=1665939918990415&amp;usg=AOvVaw02ZzkBhMdhr9u2c88LYznm">https://arxiv.org/pdf/1311.2901.pdf</a></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Region Proposal Generation </span></p><p class="c8"><span>4. Selective search : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s11263-013-0620-5&amp;sa=D&amp;source=editors&amp;ust=1665939918990855&amp;usg=AOvVaw2eiBSK6bwTO63Y7KLVc65i">https://link.springer.com/article/10.1007/s11263-013-0620-5</a></span></p><p class="c8"><span>5. Objectness &nbsp; &nbsp; &nbsp; &nbsp; : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/2004.02945&amp;sa=D&amp;source=editors&amp;ust=1665939918991165&amp;usg=AOvVaw1HIMHAicSKmZcFZBd1fHOr">https://arxiv.org/abs/2004.02945</a></span></p><p class="c8"><span class="c1">Object Detection</span></p><p class="c8"><span>6. SPP-Net: Deep Absolute Pose Regression with Synthetic Views : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1406.4729&amp;sa=D&amp;source=editors&amp;ust=1665939918991535&amp;usg=AOvVaw28xsFXshIBYHLyE2KzhyH3">https://arxiv.org/abs/1406.4729</a></span></p><p class="c8"><span>7. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.6229&amp;sa=D&amp;source=editors&amp;ust=1665939918991783&amp;usg=AOvVaw0eozmSrDzOU-EdChx2LWne">https://arxiv.org/abs/1312.6229</a></span></p><p class="c8"><span>8. Deformable Part Models are Convolutional Neural Networks : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/pdf/1409.5403&amp;sa=D&amp;source=editors&amp;ust=1665939918992075&amp;usg=AOvVaw02R_hEmGmNDm2xOY5R1YsT">https://arxiv.org/pdf/1409.5403</a></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span>9. R-CNN: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1311.2524&amp;sa=D&amp;source=editors&amp;ust=1665939918992365&amp;usg=AOvVaw2zcvpLb2zeRTNfyfTxQz5X">https://arxiv.org/abs/1311.2524</a></span></p><p class="c8"><span>10. Fast R-CNN: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1504.08083&amp;sa=D&amp;source=editors&amp;ust=1665939918992594&amp;usg=AOvVaw05eykCC3ECdx98BV2gyZ3r">https://arxiv.org/abs/1504.08083</a></span></p><p class="c8"><span>11 Faster R-CNN: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;source=editors&amp;ust=1665939918992838&amp;usg=AOvVaw0AdC9xAK5MKmlF1FCfbpkU">https://arxiv.org/abs/1506.01497</a></span></p><p class="c8"><span>12. Mask R-CNN: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1703.06870&amp;sa=D&amp;source=editors&amp;ust=1665939918993070&amp;usg=AOvVaw2G135AWkcur_e4HO5GHo25">https://arxiv.org/abs/1703.06870</a></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Multibox Approaches </span></p><p class="c8"><span>13. Scalable object detection using deep neural networks : </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1312.2249&amp;sa=D&amp;source=editors&amp;ust=1665939918993396&amp;usg=AOvVaw0nfGzm19Om0czUuwwjgonW">https://arxiv.org/abs/1312.2249</a></span></p><p class="c8"><span>14. Scalable, high-quality object detection &nbsp;: </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://arxiv.org/abs/1412.1441&amp;sa=D&amp;source=editors&amp;ust=1665939918993652&amp;usg=AOvVaw2XwRYkV1OZl02OOOk3OobQ">https://arxiv.org/abs/1412.1441</a></span></p><p class="c7"><span class="c1"></span></p><p class="c8"><span class="c15 c25">Blogs </span></p><p class="c8"><span class="c1">Special thanks to for easy such a nice explanation of important things -</span></p><p class="c8"><span>1. </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/&amp;sa=D&amp;source=editors&amp;ust=1665939918994092&amp;usg=AOvVaw1M71zo2Isi7m3ufoeIthwQ">https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</a></span></p><p class="c8"><span>2. </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4&amp;sa=D&amp;source=editors&amp;ust=1665939918994394&amp;usg=AOvVaw07RQf1pDT1SBF8hs-FQR7L">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></span></p><p class="c8"><span>3. </span><span class="c18"><a class="c16" href="https://www.google.com/url?q=https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272&amp;sa=D&amp;source=editors&amp;ust=1665939918994746&amp;usg=AOvVaw1NpzBCNxsPnx8yTFw7StEK">https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272</a></span></p><p class="c7"><span class="c10"></span></p></body></html>